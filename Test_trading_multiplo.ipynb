{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a85a9176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from multiprocessing import Pool, cpu_count, Manager, Value\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization, LSTM, Dropout, Dense, Conv1D, Flatten, GRU, Attention, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import random\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.subplots as sp\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "_features_scala_prezzo_tutte = [\n",
    "    \"Close\",\n",
    "    \"EMA_5\", \n",
    "    \"EMA_20\", \n",
    "    \"EMA_50\",\n",
    "    \"EMA_100\",\n",
    "    \"Open\",  \n",
    "    \"High\",\n",
    "    \"Low\",\n",
    "    \"PSAR\",\n",
    "    \"SUPERT\"\n",
    "]\n",
    "\n",
    "_features_da_scalare_singolarmente_tutte = [\n",
    "    \"Volume\",\n",
    "    \"ATR\",\n",
    "    \"PSARaf\",\n",
    "    \"ADX\",\n",
    "    \"OBV\"\n",
    "]\n",
    "\n",
    "_features_oscillatori_tutte = [\n",
    "    \"MACDh\",    \n",
    "    \"MACD\",\n",
    "    \"MACDs\",\n",
    "    \"AROONOSC\",\n",
    "    \"TRIX\",\n",
    "    \"TRIXs\",\n",
    "    \"DM_OSC\",\n",
    "    \"TSI\",\n",
    "    \"TSIs\",\n",
    "    \"ROC_10\",\n",
    "    \"KVO\",\n",
    "    \"KVOs\",\n",
    "    \"VI_OSC\"\n",
    "]\n",
    "\n",
    "_features_no_scala_tutte = [\n",
    "    \"SUPERTd\",  \n",
    "    \"PSARr\",\n",
    "    \"CMF\",\n",
    "    \"VHF\",\n",
    "    \"VTX_OSC\"\n",
    "]\n",
    "\n",
    "_features_candele_tutte = [\n",
    "    \"CDL_2CROWS\", \"CDL_3BLACKCROWS\", \"CDL_3INSIDE\", \"CDL_3LINESTRIKE\", \"CDL_3OUTSIDE\", \"CDL_3STARSINSOUTH\", \"CDL_3WHITESOLDIERS\", \"CDL_ABANDONEDBABY\", \"CDL_ADVANCEBLOCK\", \"CDL_BELTHOLD\", \"CDL_BREAKAWAY\", \"CDL_CLOSINGMARUBOZU\", \"CDL_CONCEALBABYSWALL\", \"CDL_COUNTERATTACK\", \"CDL_DARKCLOUDCOVER\", \"CDL_DOJI_10_0.1\", \"CDL_DOJISTAR\", \"CDL_DRAGONFLYDOJI\", \"CDL_ENGULFING\", \"CDL_EVENINGDOJISTAR\", \"CDL_EVENINGSTAR\", \"CDL_GAPSIDESIDEWHITE\", \"CDL_GRAVESTONEDOJI\", \"CDL_HAMMER\", \"CDL_HANGINGMAN\", \"CDL_HARAMI\", \"CDL_HARAMICROSS\", \"CDL_HIGHWAVE\", \"CDL_HIKKAKE\", \"CDL_HIKKAKEMOD\", \"CDL_HOMINGPIGEON\", \"CDL_IDENTICAL3CROWS\", \"CDL_INNECK\", \"CDL_INSIDE\", \"CDL_INVERTEDHAMMER\", \"CDL_KICKING\", \"CDL_KICKINGBYLENGTH\", \"CDL_LADDERBOTTOM\", \"CDL_LONGLEGGEDDOJI\", \"CDL_LONGLINE\", \"CDL_MARUBOZU\", \"CDL_MATCHINGLOW\", \"CDL_MATHOLD\", \"CDL_MORNINGDOJISTAR\", \"CDL_MORNINGSTAR\", \"CDL_ONNECK\", \"CDL_PIERCING\", \"CDL_RICKSHAWMAN\", \"CDL_RISEFALL3METHODS\", \"CDL_SEPARATINGLINES\", \"CDL_SHOOTINGSTAR\", \"CDL_SHORTLINE\", \"CDL_SPINNINGTOP\", \"CDL_STALLEDPATTERN\", \"CDL_STICKSANDWICH\", \"CDL_TAKURI\", \"CDL_TASUKIGAP\", \"CDL_THRUSTING\", \"CDL_TRISTAR\", \"CDL_UNIQUE3RIVER\", \"CDL_UPSIDEGAP2CROWS\", \"CDL_XSIDEGAP3METHODS\",\n",
    "]\n",
    "\n",
    "def inizializza_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"nessuna GPU\")\n",
    "    \n",
    "def pct_change(valore_iniziale, valore_finale):\n",
    "    try:\n",
    "        return ((valore_finale - valore_iniziale) / valore_iniziale) * 100\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def analizza_ticker(nome_simbolo, start, end, progress=True, dropna_iniziali=False, dropna_finali=False):\n",
    "    start_str = start.strftime('%Y-%m-%d')\n",
    "    end_str = end.strftime('%Y-%m-%d')\n",
    "    df = yf.download(nome_simbolo, start=start_str, end=end_str, progress=progress)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = crea_indicatori(df)\n",
    "    if dropna_iniziali:\n",
    "        idx = df[df.notna().all(axis=1) == True].index[0]\n",
    "        df = df[idx:]\n",
    "    if dropna_finali:\n",
    "        idx = df[df.notna().all(axis=1) == True].index[-1]\n",
    "        df = df[:idx]\n",
    "    return df\n",
    "\n",
    "def dropna_iniziali(df):\n",
    "    idx = df[df.notna().all(axis=1) == True].index[0]\n",
    "    df = df[idx:]\n",
    "    return df\n",
    "\n",
    "def dropna_finali(df):\n",
    "    idx = df[df.notna().all(axis=1) == True].index[-1]\n",
    "    df = df[:idx]\n",
    "    return df\n",
    "\n",
    "def imposta_target(df):\n",
    "    def __calcolo_drawdown_gain(df, periodo):\n",
    "        df[f\"Max_High_Futuro_{periodo}d\"] = df[\"High\"].shift(-periodo).rolling(periodo).max()\n",
    "        df[f\"Min_Low_Futuro_{periodo}d\"] = df[\"Low\"].shift(-periodo).rolling(periodo).min()\n",
    "        df[f\"Drawdown_{periodo}d\"] = df[\"Open\"] - df[f\"Min_Low_Futuro_{periodo}d\"]\n",
    "        df[f\"Drawdown_{periodo}d\"] = df[f\"Drawdown_{periodo}d\"].where(df[f\"Drawdown_{periodo}d\"] > 0, 0)\n",
    "        df[f\"Perc_Max_High_Futuro_{periodo}d\"] = ((df[f\"Max_High_Futuro_{periodo}d\"] - df[\"Open\"]) / df[\"Open\"]) * 100\n",
    "        df[f\"Perc_Drawdown_{periodo}d\"] = ((df[f\"Drawdown_{periodo}d\"]) / df[\"Open\"]) * 100 \n",
    "        df[f\"Perc_Drawdown_{periodo}d\"] = df[f\"Perc_Drawdown_{periodo}d\"].where(df[f\"Perc_Drawdown_{periodo}d\"] > 0, 0)\n",
    "        df.drop(columns=[f\"Max_High_Futuro_{periodo}d\", f\"Min_Low_Futuro_{periodo}d\", f\"Drawdown_{periodo}d\"], axis=1, inplace=True)\n",
    "        return df\n",
    "    def __trova_massimi_minimi(df, periodo):\n",
    "        mezzo_periodo = periodo // 2\n",
    "\n",
    "        massimi_passati = df['Close'].shift(1).rolling(mezzo_periodo).max()\n",
    "        massimi_futuri = df['Close'][::-1].shift(1).rolling(mezzo_periodo).max()[::-1]\n",
    "        idx_massimi = (df[\"Close\"] >= massimi_passati) & (df[\"Close\"] >= massimi_futuri)\n",
    "        df.loc[idx_massimi, \"MaxMinRel\"] = periodo\n",
    "\n",
    "        minimi_passati = df['Close'].shift(1).rolling(mezzo_periodo).min()\n",
    "        minimi_futuri = df['Close'][::-1].shift(1).rolling(mezzo_periodo).min()[::-1]\n",
    "        idx_minimi = (df[\"Close\"] <= minimi_passati) & (df[\"Close\"] <= minimi_futuri)\n",
    "        df.loc[idx_minimi, \"MaxMinRel\"] = -periodo\n",
    "            \n",
    "        return df\n",
    "    # df = __calcolo_drawdown_gain(df, 20)\n",
    "    # df = __calcolo_drawdown_gain(df, 50)\n",
    "    # df = __calcolo_drawdown_gain(df, 100)\n",
    "    # df[\"max_gain\"] = df[[\"Perc_Max_High_Futuro_20d\", \"Perc_Max_High_Futuro_50d\", \"Perc_Max_High_Futuro_100d\"]].max(axis=1)\n",
    "    # df[\"max_drawdown\"] = df[[\"Perc_Drawdown_20d\", \"Perc_Drawdown_50d\", \"Perc_Drawdown_100d\"]].min(axis=1)\n",
    "\n",
    "    df['EMA_20_5d'] = df['EMA_20'].shift(-5)\n",
    "    df['EMA_20_10d'] = df['EMA_20'].shift(-10)\n",
    "    df['EMA_20_15d'] = df['EMA_20'].shift(-15)\n",
    "    df['EMA_20_20d'] = df['EMA_20'].shift(-20)\n",
    "    \n",
    "    df['EMA_50_5d'] = df['EMA_50'].shift(-5)\n",
    "    df['EMA_50_10d'] = df['EMA_50'].shift(-10)\n",
    "    df['EMA_50_15d'] = df['EMA_50'].shift(-15)\n",
    "    df['EMA_50_20d'] = df['EMA_50'].shift(-20)\n",
    "    \n",
    "    df['Close_5d'] = df['Close'].shift(-5)\n",
    "    df['Close_10d'] = df['Close'].shift(-10)\n",
    "    df['Close_15d'] = df['Close'].shift(-15)\n",
    "    df['Close_20d'] = df['Close'].shift(-20)\n",
    "    \n",
    "    df['EMA_5_5d'] = df['EMA_5'].shift(-5)\n",
    "    df['EMA_5_10d'] = df['EMA_5'].shift(-10)\n",
    "    df['EMA_5_15d'] = df['EMA_5'].shift(-15)\n",
    "    df['EMA_5_20d'] = df['EMA_5'].shift(-20)\n",
    "    #df['Close_1d'] = df['Close'].shift(-1)\n",
    "    #df['perc_EMA_5_20d'] = ((df['EMA_5_20d'] - df['EMA_5']) / df['EMA_5']) * 100\n",
    "    #df['perc_Close_20d'] = ((df['Close_20d'] - df['Close']) / df['Close']) * 100\n",
    "    #df['incrocio_verde_gialla'] = (ta.cross(df['EMA_20'], df['EMA_50'], above=True)).astype(\"int8\")\n",
    "    #df[\"incrocio_passato_verde_gialla_10d\"] = df[\"incrocio_verde_gialla\"].rolling(10).sum()\n",
    "    df['Max_Close_20d'] = df['Close'].shift(-20).rolling(window=20, min_periods=1).max()\n",
    "    df['pct_change_20d'] = df.apply(lambda row: pct_change(row['Close'], row['Max_Close_20d']), axis=1)\n",
    "    df.drop(columns=[\"Max_Close_20d\"], inplace=True, axis=1)\n",
    "\n",
    "    # df[\"MaxMinRel\"] = 0\n",
    "    # df = __trova_massimi_minimi(df, 20)   \n",
    "    # df = __trova_massimi_minimi(df, 50)   \n",
    "    # df = __trova_massimi_minimi(df, 100)         \n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    df['Target_ingresso'] = (\n",
    "        (df['pct_change_20d'] > 20)\n",
    "    )\n",
    "    df['Target_uscita'] = (\n",
    "        (df['EMA_5'] > df['EMA_20']) & (df['EMA_5_5d'] < df['EMA_20_5d']) & (df['EMA_5_10d'] < df['EMA_20_10d'])\n",
    "    )    \n",
    "    return df\n",
    "    \n",
    "def grafico(df):\n",
    "    close = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['Close'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, .9)'),\n",
    "        name = 'Close'\n",
    "    )\n",
    "\n",
    "    close2 = go.Scatter( # serve solo per il fill del supertrend\n",
    "        x = df.index,\n",
    "        y = df['Close'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, 0)'),\n",
    "        showlegend=False,\n",
    "        name = 'Close2'\n",
    "    )\n",
    "\n",
    "    # min5 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -5].index,\n",
    "    #     y = df[df['MaxMinRel'] == -5]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 5,\n",
    "    #         color = 'rgba(255, 0, 0, .9)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel5'\n",
    "    # )\n",
    "  \n",
    "    # max5 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 5].index,\n",
    "    #     y = df[df['MaxMinRel'] == 5]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 5,\n",
    "    #         color = 'rgba(50, 205, 50, .9)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel5'\n",
    "    # )\n",
    "\n",
    "    # min10 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -10].index,\n",
    "    #     y = df[df['MaxMinRel'] == -10]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 15,\n",
    "    #         color = 'rgba(255, 0, 0, .4)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel10'\n",
    "    # )\n",
    "  \n",
    "    # max10 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 10].index,\n",
    "    #     y = df[df['MaxMinRel'] == 10]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 15,\n",
    "    #         color = 'rgba(50, 205, 50, .4)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel10'\n",
    "    # )\n",
    "\n",
    "    # min20 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -20].index,\n",
    "    #     y = df[df['MaxMinRel'] == -20]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 25,\n",
    "    #         color = 'rgba(255, 0, 0, .2)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel20'\n",
    "    # )\n",
    "  \n",
    "    # max20 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 20].index,\n",
    "    #     y = df[df['MaxMinRel'] == 20]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 25,\n",
    "    #         color = 'rgba(50, 205, 50, .2)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel20'\n",
    "    # )\n",
    "\n",
    "    # min60 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -60].index,\n",
    "    #     y = df[df['MaxMinRel'] == -60]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 35,\n",
    "    #         color = 'rgba(255, 0, 0, .1)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel60'\n",
    "    # )\n",
    "  \n",
    "    # max60 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 60].index,\n",
    "    #     y = df[df['MaxMinRel'] == 60]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 35,\n",
    "    #         color = 'rgba(50, 205, 50, .1)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel60'\n",
    "    # )\n",
    "\n",
    "    ema5 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_5'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='blue'),\n",
    "        name = 'EMA5'\n",
    "    )\n",
    "\n",
    "    ema20 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_20'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='limegreen'),\n",
    "        name = 'EMA20'\n",
    "    )\n",
    "\n",
    "    ema50 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_50'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='orange'),\n",
    "        name = 'EMA50'\n",
    "    )\n",
    "    \n",
    "    ema100 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_100'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='red'),\n",
    "        name = 'EMA100'\n",
    "    )\n",
    "    \n",
    "    psar = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['PSAR'], \n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 2, \n",
    "            color = 'rgba(0, 0, 0, .8)',  \n",
    "        ),\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SAR Parabolico'\n",
    "    )\n",
    "    \n",
    "    stup = np.where(df['SUPERTd'] == 1, df['SUPERT'], np.nan)\n",
    "    stdown = np.where(df['SUPERTd'] == -1, df['SUPERT'], np.nan)\n",
    "    stupfill = np.where(df['SUPERTd'] == 1, df['SUPERT'], df['Close'])\n",
    "    stdownfill = np.where(df['SUPERTd'] == -1, df['SUPERT'], df['Close'])\n",
    "    \n",
    "    strendup = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stup, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='limegreen',\n",
    "            width=1\n",
    "        ),\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        connectgaps = False,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "        \n",
    "    strendupfill = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stupfill, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0,0,0,0)',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        fill='tonexty',   \n",
    "        fillcolor='rgba(50, 205, 50, 0.1)', \n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "        \n",
    "    strenddown = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stdown, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='red',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "    \n",
    "    strenddownfill = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stdownfill, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0,0,0,0)',\n",
    "            width=1\n",
    "        ),\n",
    "        fill='tonexty',   \n",
    "        fillcolor='rgba(255, 0, 0, 0.1)', \n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "    \n",
    "    target_in = go.Scatter(\n",
    "        x = df[df['Target_ingresso'] == 1].index,\n",
    "        y = df[df['Target_ingresso'] == 1]['Close'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(0, 200, 0, .9)'\n",
    "        ),\n",
    "        name = 'target_in'\n",
    "    )\n",
    "    \n",
    "    target_out = go.Scatter(\n",
    "        x = df[df['Target_uscita'] == 1].index,\n",
    "        y = df[df['Target_uscita'] == 1]['Close'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(220, 0, 0, .9)'\n",
    "        ),\n",
    "        name = 'target_out'\n",
    "    )\n",
    "        \n",
    "    layout = dict(xaxis = dict(autorange=True),\n",
    "                  yaxis = dict(title = 'Close', autorange=True),\n",
    "                  autosize = True,\n",
    "                  margin = go.layout.Margin(\n",
    "                      l=0,  # Sinistra\n",
    "                      r=0,  # Destra\n",
    "                      b=0,  # Basso\n",
    "                      t=50,  # Alto\n",
    "                      pad=0  # Padding\n",
    "                  ),\n",
    "                  legend = dict(traceorder = 'normal', bordercolor = 'black')\n",
    "    )\n",
    "        \n",
    "    fig = sp.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    fig.update_layout(layout)\n",
    "    \n",
    "    # RIGA 1\n",
    "\n",
    "    fig.add_trace(close, row=1, col=1)\n",
    "    fig.add_trace(strendupfill, row=1, col=1)\n",
    "    fig.add_trace(strendup, row=1, col=1)\n",
    "    fig.add_trace(close2, row=1, col=1)\n",
    "    fig.add_trace(strenddownfill, row=1, col=1)\n",
    "    fig.add_trace(strenddown, row=1, col=1)\n",
    "    #fig.add_trace(min5, row=1, col=1); fig.add_trace(max5, row=1, col=1)\n",
    "    #fig.add_trace(min10, row=1, col=1); fig.add_trace(max10, row=1, col=1)\n",
    "    #fig.add_trace(min20, row=1, col=1); fig.add_trace(max20, row=1, col=1)\n",
    "    #fig.add_trace(min60, row=1, col=1); fig.add_trace(max60, row=1, col=1)\n",
    "    fig.add_trace(target_in, row=1, col=1); fig.add_trace(target_out, row=1, col=1)\n",
    "    fig.add_trace(ema5, row=1, col=1); fig.add_trace(ema20, row=1, col=1); fig.add_trace(ema50, row=1, col=1); fig.add_trace(ema100, row=1, col=1)\n",
    "    fig.add_trace(psar, row=1, col=1)\n",
    "    \n",
    "    pyo.plot(fig, filename=\"grafico_target.html\", auto_open=True)\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def crea_indicatori(df):\n",
    "    def __rinomina_colonne(df):\n",
    "        df = df.rename(columns={\n",
    "            'PSARaf_0.02_0.2': 'PSARaf',\n",
    "            'PSARr_0.02_0.2': 'PSARr',\n",
    "            'MACD_20_50_9': 'MACD',\n",
    "            'MACDh_20_50_9': 'MACDh',\n",
    "            'MACDs_20_50_9': 'MACDs',\n",
    "            'TSI_13_25_13': 'TSI',\n",
    "            'TSIs_13_25_13': 'TSIs',\n",
    "            'SUPERT_20_3.0': 'SUPERT',\n",
    "            'SUPERTd_20_3.0': 'SUPERTd',\n",
    "            'ADX_20': 'ADX',\n",
    "            'DMP_20': 'DMP',\n",
    "            'DMN_20': 'DMN',\n",
    "            'CMF_10': 'CMF',\n",
    "            'TRIX_18_9': 'TRIX',\n",
    "            'TRIXs_18_9': 'TRIXs',\n",
    "            'KVO_34_55_13': 'KVO',\n",
    "            'KVOs_34_55_13': 'KVOs',\n",
    "            'DCL_20_20': 'DCL',\n",
    "            'DCM_20_20': 'DCM',\n",
    "            'DCU_20_20': 'DCU',\n",
    "            'VTXP_20': 'VTXP',\n",
    "            'VTXM_20': 'VTXM',\n",
    "            'AROOND_20': 'AROOND',\n",
    "            'AROONU_20': 'AROONU',\n",
    "            'AROONOSC_20': 'AROONOSC',\n",
    "            'NVI_1': 'NVI',\n",
    "            'PVI_1': 'PVI',\n",
    "            'VHF_20': 'VHF',\n",
    "            'ATRr_14': 'ATR'\n",
    "        })\n",
    "        return df\n",
    "    \n",
    "    psar = ta.psar(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], af0=0.02, af=0.02, max_af=0.2)\n",
    "    psar[\"PSAR\"] = psar[\"PSARl_0.02_0.2\"].combine_first(psar[\"PSARs_0.02_0.2\"])\n",
    "    psar.drop([\"PSARl_0.02_0.2\", \"PSARs_0.02_0.2\"], axis=1, inplace=True)\n",
    "    macd = ta.macd(close=df[\"Close\"], fast=20, slow=50, signal=9)\n",
    "    tsi = ta.tsi(close=df[\"Close\"], fast=13, slow=25)\n",
    "    supertrend = ta.supertrend(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], length=20, multiplier=3)\n",
    "    supertrend.drop([\"SUPERTl_20_3.0\", \"SUPERTs_20_3.0\"], axis=1, inplace=True)\n",
    "    ema5 = ta.ema(close=df[\"Close\"], length=5)\n",
    "    ema20 = ta.ema(close=df[\"Close\"], length=20)\n",
    "    ema50 = ta.ema(close=df[\"Close\"], length=50)\n",
    "    ema100 = ta.ema(close=df[\"Close\"], length=100)\n",
    "    adx = ta.adx(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], length=20)\n",
    "    roc = ta.roc(close=df[\"Close\"], length=10)\n",
    "    cmf = ta.cmf(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], volume=df['Volume'], length=10)\n",
    "    trix = ta.trix(close=df['Close'], length=18)\n",
    "    klinger = ta.kvo(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], volume=df['Volume'], short=34, long=55)\n",
    "    vi = ta.vortex(high=df['High'], low=df['Low'], close=df['Close'], length=20)\n",
    "    aroon = ta.aroon(high=df['High'], low=df['Low'], close=df['Close'], length=20)\n",
    "    nvi = ta.nvi(close=df['Close'], volume=df['Volume'])\n",
    "    pvi = ta.pvi(close=df['Close'], volume=df['Volume'])\n",
    "    vhf = ta.vhf(close=df['Close'], length=20)\n",
    "    atr = ta.atr(high=df['High'], low=df['Low'], close=df['Close'])\n",
    "    obv = ta.obv(close=df[\"Close\"], volume=df[\"Volume\"])\n",
    "    #candele = ta.cdl_pattern(open_=df[\"Open\"], high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"])\n",
    "\n",
    "    df = pd.concat([df, ema5, ema20, ema50, ema100, psar, macd, tsi, supertrend, adx, trix, vi, aroon, nvi, pvi, atr, cmf, roc, klinger, vhf, obv], axis=1)\n",
    "\n",
    "    df = __rinomina_colonne(df)\n",
    "    \n",
    "    #df['HLC3'] = ((df['High'] + df['Low'] + df['Close']) / 3)\n",
    "    df[\"DM_OSC\"] = (df[\"DMP\"] - df[\"DMN\"])\n",
    "    df[\"VTX_OSC\"] = (df[\"VTXP\"] - df[\"VTXM\"])\n",
    "    df[\"VI_OSC\"] = (df[\"PVI\"] - df[\"NVI\"])\n",
    "    \n",
    "    df.drop(columns=[\"DMP\", \"DMN\", \"VTXP\", \"VTXM\", \"PVI\", \"NVI\", \"AROOND\", \"AROONU\"], inplace=True, axis=1)\n",
    "    \n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def _scarica(nome_simbolo, scarica_prima, scarica_dopo, data_inizio, data_fine, append):\n",
    "    try:\n",
    "        if append == True:\n",
    "            ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', \"ticker\")\n",
    "            ti_min = ticker.index.min()\n",
    "            ti_max = ticker.index.max()\n",
    "            if scarica_prima:\n",
    "                inizio = data_inizio - pd.Timedelta(days=365)\n",
    "                fine = ti_min - pd.Timedelta(days=1)\n",
    "                df_inizio = analizza_ticker(nome_simbolo, start=inizio, end=fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "                ticker = pd.concat([df_inizio, ticker], axis=0, ignore_index=False)\n",
    "            if scarica_dopo:\n",
    "                inizio = ti_max - pd.Timedelta(days=365) \n",
    "                fine = data_fine\n",
    "                df_fine = analizza_ticker(nome_simbolo, start=inizio, end=fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "                ticker = ticker[ticker.index < df_fine.index.min()]\n",
    "                ticker = pd.concat([ticker, df_fine], axis=0, ignore_index=False)\n",
    "        else:\n",
    "            ticker = analizza_ticker(nome_simbolo, start=data_inizio, end=data_fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "        return nome_simbolo, ticker, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_simbolo, None, str(e)\n",
    "\n",
    "def _callback_tickers(result, totale_processati, tot_tickers):\n",
    "    nome_simbolo, ticker, error = result\n",
    "    if error == \"\":\n",
    "        ticker.to_hdf(f'tickers/{nome_simbolo}.h5', key='ticker', mode='w')\n",
    "    else:\n",
    "        print(f\"Errore su funzione di callback per {nome_simbolo}: {error}\")\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "    print(f\"{totale_processati.value}/{tot_tickers}) Scaricato ticker {nome_simbolo}\")   \n",
    "    \n",
    "def _carica_screener(nome_simbolo, lista_scr, lista_scr_out, prob, nome_dataset):\n",
    "    try:\n",
    "        df = pd.read_hdf(f'screeners/{nome_simbolo}.h5', nome_dataset)\n",
    "        df.index.set_names(['Data'], inplace=True)\n",
    "        df['Ticker'] = nome_simbolo\n",
    "        df.set_index('Ticker', append=True, inplace=True)\n",
    "        df = df.loc[df['Previsione'] > prob]\n",
    "        if nome_dataset == 'screener':\n",
    "            lista_scr.append(df)\n",
    "        else:\n",
    "            lista_scr_out.append(df)\n",
    "        return nome_simbolo, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_simbolo, str(e)\n",
    "\n",
    "def _carica_screener_callback(result, totale_processati, tot_tickers):\n",
    "    nome_simbolo, error = result\n",
    "    if error != \"\":\n",
    "        print(f\"Errore su funzione di callback per {nome_simbolo}: {error}\")\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "    print(f\"{totale_processati.value}/{tot_tickers}) Caricato su screener ticker {nome_simbolo}\")   \n",
    "\n",
    "def to_XY(dati_ticker, timesteps, giorni_previsione, features, targets, bilanciamento=0):\n",
    "    dati_ticker = imposta_target(dati_ticker)\n",
    "    features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in features]\n",
    "    features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in features]\n",
    "    features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in features]\n",
    "    features_no_scala = [ft for ft in _features_no_scala_tutte if ft in features]\n",
    "    features_candele = [ft for ft in _features_candele_tutte if ft in features]\n",
    "\n",
    "    scalers_prezzo = []\n",
    "    scaler_meno_piu = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_standard = MinMaxScaler()\n",
    "\n",
    "    new_dates = pd.bdate_range(start=dati_ticker.index[-1] + pd.Timedelta(days=1), periods=giorni_previsione)\n",
    "    df_new = pd.DataFrame(index=new_dates)\n",
    "    dati_ticker = pd.concat([dati_ticker, df_new])\n",
    "\n",
    "    ft_prezzo = dati_ticker[features_scala_prezzo]\n",
    "    ft_standard = dati_ticker[features_da_scalare_singolarmente]\n",
    "    ft_meno_piu = dati_ticker[features_oscillatori]\n",
    "    ft_no_scala = dati_ticker[features_no_scala]\n",
    "    ft_candele = dati_ticker[features_candele] \n",
    "\n",
    "    _targets = dati_ticker[targets]\n",
    "\n",
    "    i_tot = len(dati_ticker) - giorni_previsione\n",
    "\n",
    "    tot_elementi = i_tot - (timesteps-1)    \n",
    "    \n",
    "    X_prezzo = X_standard = X_meno_piu = X_no_scala = X_candele = None\n",
    "    if len(features_scala_prezzo) > 0:\n",
    "        tot_col_prezzo_x = len(ft_prezzo.columns)\n",
    "        X_prezzo = np.zeros((tot_elementi, timesteps, tot_col_prezzo_x))\n",
    "    if len(features_da_scalare_singolarmente) > 0:\n",
    "        tot_col_standard_x = len(ft_standard.columns)\n",
    "        X_standard = np.zeros((tot_elementi, timesteps, tot_col_standard_x))\n",
    "    if len(features_oscillatori) > 0:\n",
    "        tot_col_meno_piu_x = len(ft_meno_piu.columns)\n",
    "        X_meno_piu = np.zeros((tot_elementi, timesteps, tot_col_meno_piu_x))\n",
    "    if len(features_no_scala) > 0:\n",
    "        tot_col_no_scala_x = len(ft_no_scala.columns)\n",
    "        X_no_scala = np.zeros((tot_elementi, timesteps, tot_col_no_scala_x))\n",
    "    if len(features_candele) > 0:\n",
    "        tot_col_candele_x = len(ft_candele.columns)\n",
    "        X_candele = np.zeros((tot_elementi, timesteps, tot_col_candele_x))\n",
    "    if len(targets) > 0:\n",
    "        #tot_col_targets_y = len(targets.columns)\n",
    "        #Y = np.zeros((tot_elementi, giorni_previsione, tot_col_targets_y)) # togliere se classificazione binaria\n",
    "        Y = np.zeros(tot_elementi) #solo per classificazione binaria\n",
    "    \n",
    "    for i in range(timesteps - 1, i_tot):\n",
    "        if len(features_scala_prezzo) > 0:\n",
    "            arr_x = np.array(ft_prezzo.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_res = arr_x.reshape(-1, 1)\n",
    "            scaler_prezzo = MinMaxScaler()\n",
    "            scaler_prezzo.fit(arr_res)\n",
    "            arr_sc = scaler_prezzo.transform(arr_res).reshape(timesteps, tot_col_prezzo_x)\n",
    "            X_prezzo[i - (timesteps - 1)] = arr_sc\n",
    "            scalers_prezzo.append(scaler_prezzo)\n",
    "\n",
    "        if len(features_da_scalare_singolarmente) > 0:\n",
    "            arr_x = np.array(ft_standard.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_standard.fit_transform(arr_x)   \n",
    "            X_standard[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(features_oscillatori) > 0:\n",
    "            arr_x = np.array(ft_meno_piu.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_meno_piu.fit_transform(arr_x)   \n",
    "            X_meno_piu[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(features_no_scala) > 0:\n",
    "            arr_x = np.array(ft_no_scala.iloc[i - (timesteps - 1):i + 1])\n",
    "            X_no_scala[i - (timesteps - 1)] = arr_x\n",
    "\n",
    "        if len(features_candele) > 0:\n",
    "            arr_x = np.array(ft_candele.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_meno_piu.fit_transform(arr_x) \n",
    "            X_candele[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(targets) > 0:\n",
    "            # arr_y = np.array(targets.iloc[i + 1:i + 1 + giorni_previsione]) # togliere in caso di classificazione binaria\n",
    "            # arr_res = arr_y.reshape(-1, 1) # togliere in caso di classificazione binaria\n",
    "            # arr_sc = scaler_prezzo.transform(arr_res).reshape(giorni_previsione, tot_col_targets_y) # togliere in caso di classificazione binaria\n",
    "            # Y[i - (timesteps - 1)] = arr_sc  # togliere in caso di classificazione binaria\n",
    "            Y[i - (timesteps - 1)] = np.array(_targets.iloc[i]) #solo per classificazione binaria\n",
    "\n",
    "    X_list = [x for x in [X_prezzo, X_standard, X_meno_piu, X_no_scala, X_candele] if x is not None and x.size > 0]\n",
    "    X = np.concatenate(X_list, axis=2) if X_list else np.array([])\n",
    "    idx = dati_ticker.index[timesteps - 1:i_tot]\n",
    "\n",
    "    if bilanciamento > 0:\n",
    "        #rus = RandomUnderSampler(sampling_strategy=bilanciamento)\n",
    "        smote = SMOTE(sampling_strategy=bilanciamento)\n",
    "        dim1 = X.shape[1]\n",
    "        dim2 = X.shape[2]\n",
    "        X_flat = X.reshape(-1, dim1 * dim2)\n",
    "        # Applica l'undersampling\n",
    "        X_flat_resampled, Y = smote.fit_resample(X_flat, Y)\n",
    "\n",
    "        # Ridimensiona X tornando alla forma originale\n",
    "        X = X_flat_resampled.reshape(-1, dim1, dim2)\n",
    "\n",
    "        # Ottieni gli indici originali dopo l'undersampling\n",
    "        #idx_resampled = rus.sample_indices_\n",
    "\n",
    "        # Usa idx_resampled per ottenere gli indici originali\n",
    "        #idx = idx[idx_resampled]\n",
    "\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    return idx, X, Y, scalers_prezzo\n",
    "\n",
    "def concatena(array_list, hdf5_file, dataset_name):\n",
    "    \"\"\"\n",
    "    Concatena una lista di array NumPy a un dataset in un file HDF5, creando il file se non esiste.\n",
    "    \n",
    "    :param array_list: Lista di array NumPy da aggiungere.\n",
    "    :param hdf5_file: Percorso del file HDF5.\n",
    "    :param dataset_name: Nome del dataset all'interno del file HDF5.\n",
    "    \"\"\"\n",
    "    # Apri o crea il file HDF5\n",
    "    with h5py.File(hdf5_file, 'a') as h5f:  # 'a' apre il file in modalità read/write e lo crea se non esiste\n",
    "        # Controlla se il dataset esiste già\n",
    "        if dataset_name in h5f:\n",
    "            # Il dataset esiste, leggi la sua lunghezza e aggiungi gli array\n",
    "            dset = h5f[dataset_name]\n",
    "        else:\n",
    "            # Il dataset non esiste, quindi dobbiamo crearlo\n",
    "            # Usiamo la forma del primo array per definire la forma del dataset\n",
    "            initial_shape = (0,) + array_list[0].shape[1:]\n",
    "            maxshape = (None,) + array_list[0].shape[1:]\n",
    "            \n",
    "            # Crea il dataset con shape iniziale e maxshape\n",
    "            dset = h5f.create_dataset(dataset_name, shape=initial_shape, maxshape=maxshape, chunks=True)\n",
    "            \n",
    "        # Itera su tutti gli array nella lista\n",
    "        for array in array_list:\n",
    "            # Calcola la nuova lunghezza del dataset\n",
    "            new_len = dset.shape[0] + array.shape[0]\n",
    "            # Ridimensiona il dataset per accogliere i nuovi dati\n",
    "            dset.resize(new_len, axis=0)\n",
    "            # Aggiungi il nuovo array alla fine del dataset\n",
    "            dset[-array.shape[0]:] = array\n",
    "\n",
    "class Posizione:\n",
    "    def __init__(self, simbolo, dati_ticker, data, prezzo, n_azioni, stop_loss=None, take_profit=None):\n",
    "        self.simbolo = simbolo\n",
    "        self._ticker = dati_ticker.copy()\n",
    "        self.data_apertura = data\n",
    "        self.data_corrente = data\n",
    "        self.data_minimo = data\n",
    "        self.data_massimo = data\n",
    "        self.data_chiusura = None\n",
    "        self.prezzo_apertura = prezzo\n",
    "        self.prezzo_precedente = self.prezzo_apertura\n",
    "        self.prezzo_corrente = self.prezzo_apertura\n",
    "        self.prezzo_minimo = self.prezzo_apertura\n",
    "        self.prezzo_massimo = self.prezzo_apertura\n",
    "        self.prezzo_chiusura = None\n",
    "        self.ticker = self._ticker.iloc[0]\n",
    "        self.pct_change = 0\n",
    "        self.pct_min = 0\n",
    "        self.pct_max = 0\n",
    "        self.n_azioni = n_azioni\n",
    "        self.giorni_apertura = 1\n",
    "        self.stato = 'APERTURA'\n",
    "        \n",
    "        if stop_loss is None:\n",
    "            self.stop_loss = 0\n",
    "        else:\n",
    "            self.stop_loss = prezzo * (1 - stop_loss)\n",
    "            \n",
    "        if take_profit is None:\n",
    "            self.take_profit = np.inf\n",
    "        else:\n",
    "            self.take_profit = prezzo * (1 + take_profit)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.simbolo}'\n",
    "\n",
    "    def step(self):\n",
    "        self.data_corrente += timedelta(days=1)\n",
    "        if self.data_corrente in self.ticker.index:\n",
    "            self.ticker = self._ticker.loc[self.data_corrente].iloc[0]\n",
    "            low = self.ticker['Low']\n",
    "            high = self.ticker['High']\n",
    "            open = self.ticker['Open']\n",
    "            close = self.ticker['Close']\n",
    "            ema5 = self.ticker['EMA_5']\n",
    "            ema20 = self.ticker['EMA_20']\n",
    "            if low < self.stop_loss:\n",
    "                self.chiudi(self.data_corrente, self.stop_loss)\n",
    "                self.stato = 'CHIUSURA su STOP LOSS'\n",
    "            elif high > self.take_profit:\n",
    "                self.chiudi(self.data_corrente, self.take_profit)\n",
    "                self.stato = 'CHIUSURA su TAKE PROFIT'\n",
    "            elif ema5 < ema20:\n",
    "                self.chiudi(self.data_corrente, close)\n",
    "            else:\n",
    "                self.pct_change = pct_change(self.prezzo_precedente, close)\n",
    "                self.prezzo_precedente = close\n",
    "                self.prezzo_corrente = close\n",
    "                self.stato = 'in corso'\n",
    "                if high > self.prezzo_massimo:\n",
    "                    self.prezzo_massimo = high\n",
    "                    self.data_massimo = self.data_corrente\n",
    "                    self.pct_max = pct_change(self.prezzo_apertura, high)\n",
    "                if low < self.prezzo_minimo:\n",
    "                    self.prezzo_minimo = low  \n",
    "                    self.data_minimo = self.data_corrente  \n",
    "                    self.pct_min = pct_change(self.prezzo_apertura, low)\n",
    "                self.giorni_apertura += 1\n",
    "\n",
    "    def chiudi(self, data, prezzo):\n",
    "        self.prezzo_corrente = prezzo\n",
    "        self.prezzo_chiusura = prezzo\n",
    "        self.stato = 'CHIUSURA'\n",
    "        self.data_chiusura = data\n",
    "\n",
    "class Borsa:\n",
    "    def __init__(self, n_simboli_contemporanei=10, bilancio_iniziale=1000, probabilità_per_acquisto=0.5, giorni_max_posizione=20, stop_loss=None, take_profit=None, data_inizio=pd.Timestamp(year=2005, month=1, day=1), data_fine=pd.Timestamp.now().normalize()):\n",
    "        self.N_SIMBOLI = n_simboli_contemporanei\n",
    "        self.DATA_INIZIO = pd.to_datetime(data_inizio)\n",
    "        self.DATA_FINE = pd.to_datetime(data_fine)\n",
    "        self.BILANCIO_INIZIALE = bilancio_iniziale\n",
    "        self.PROBABILITA_PER_ACQUISTO = probabilità_per_acquisto\n",
    "        self.SL = stop_loss\n",
    "        self.TP = take_profit\n",
    "        self.GIORNI_POS = giorni_max_posizione\n",
    "\n",
    "        self.modello_ingresso = Modello()\n",
    "        self.modello_ingresso.carica(progetto='mod_1_in')\n",
    "        self.timesteps = self.modello_ingresso.timesteps\n",
    "        self.giorni_previsione = self.modello_ingresso.giorni_previsione\n",
    "        self.features = self.modello_ingresso.features\n",
    "        self.targets = self.modello_ingresso.targets\n",
    "        \n",
    "        self.modello_uscita = Modello()\n",
    "        self.modello_uscita.carica(progetto='mod_1_out')\n",
    "        self.features_out = self.modello_uscita.features\n",
    "        self.targets_out = self.modello_uscita.targets        \n",
    "\n",
    "        self.lista_tickers = pd.read_parquet(\"lista_ticker.parquet\")\n",
    "        self.tot_tickers = len(self.lista_tickers)\n",
    "        self.screener = pd.DataFrame()\n",
    "        self.screener_out = pd.DataFrame()\n",
    "\n",
    "    def aggiorna_dati(self):\n",
    "        try:\n",
    "            if os.path.exists(f'_indice.json'):\n",
    "                with open(f'_indice.json', 'r') as jsonfile:\n",
    "                    indice = json.load(jsonfile)\n",
    "                prima_data = pd.to_datetime(indice['prima_data'])\n",
    "                ultima_data = pd.to_datetime(indice['ultima_data'])\n",
    "                \n",
    "                if (self.DATA_INIZIO < prima_data):\n",
    "                    scarica_prima = True\n",
    "                else:\n",
    "                    scarica_prima = False\n",
    "\n",
    "                if (self.DATA_FINE > ultima_data):\n",
    "                    scarica_dopo = True\n",
    "                else:\n",
    "                    scarica_dopo = False\n",
    "\n",
    "                if scarica_prima or scarica_dopo:\n",
    "                    print('Caricamento nuovi dati ticker')\n",
    "                    self.scarica_tickers(scarica_prima, scarica_dopo, self.DATA_INIZIO, self.DATA_FINE, append=True)\n",
    "                    print('Aggiornamento lista tickers')  \n",
    "                    self.aggiorna_lista_tickers()      \n",
    "                    print('Aggiornamento screener')\n",
    "                    self.avvia_screener(append=True)    \n",
    "                else:\n",
    "                    print('Caricamento screener esistenti')\n",
    "                    self.screener = pd.read_hdf('screeners/_screener.h5', 'screener')\n",
    "                    self.screener_out = pd.read_hdf('screeners/_screener.h5', 'screener_out')\n",
    "            else:\n",
    "                print('Scarico totale dati ticker')\n",
    "                self.scarica_tickers(scarica_prima=True, scarica_dopo=True, data_inizio=self.DATA_INIZIO, data_fine=self.DATA_FINE, append=False)\n",
    "                print('Creazione nuovi screener')\n",
    "                self.avvia_screener(append=False)  \n",
    "                print('Aggiornamento lista tickers')  \n",
    "                self.aggiorna_lista_tickers()              \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        \n",
    "    def scarica_tickers(self, scarica_prima=True, scarica_dopo=True, data_inizio=pd.Timestamp(year=2005, month=1, day=1), data_fine=pd.Timestamp.now().normalize(), append=False) -> None: \n",
    "        totale_processati = Value('i', 0)\n",
    "        with Pool(cpu_count()) as p:\n",
    "            callback_with_args = partial(_callback_tickers, totale_processati=totale_processati, tot_tickers=self.tot_tickers)\n",
    "            for i in range(0, self.tot_tickers):\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                param = (nome_simbolo, scarica_prima, scarica_dopo, data_inizio, data_fine, append)\n",
    "                p.apply_async(_scarica, args=param, callback=callback_with_args)\n",
    "            p.close()\n",
    "            p.join()     \n",
    "\n",
    "        indice = {\n",
    "            'prima_data': data_inizio.strftime('%Y-%m-%d'),\n",
    "            'ultima_data': data_fine.strftime('%Y-%m-%d')\n",
    "        }\n",
    "        with open(f'_indice.json', 'w') as jsonfile:\n",
    "            json.dump(indice, jsonfile, indent=4)    \n",
    "\n",
    "    def aggiorna_lista_tickers(self):\n",
    "        cartella_tickers = 'tickers'\n",
    "        cartella_screeners = 'screeners'\n",
    "        files_tickers = os.listdir(cartella_tickers)\n",
    "\n",
    "        tickers_da_rimuovere = []\n",
    "\n",
    "        for file in files_tickers:\n",
    "            percorso_file = os.path.join(cartella_tickers, file)\n",
    "\n",
    "            # Controlla il numero di righe nel file HDF5\n",
    "            try:\n",
    "                df_temp = pd.read_hdf(percorso_file)\n",
    "                if len(df_temp) < 100:\n",
    "                    # Elimina il file da tickers\n",
    "                    os.remove(percorso_file)\n",
    "\n",
    "                    # Elimina il corrispondente file in screeners\n",
    "                    percorso_file_screener = os.path.join(cartella_screeners, file)\n",
    "                    if os.path.exists(percorso_file_screener):\n",
    "                        os.remove(percorso_file_screener)\n",
    "\n",
    "                    # Aggiungi il nome del ticker alla lista dei tickers da rimuovere\n",
    "                    nome_ticker = os.path.splitext(file)[0]\n",
    "                    tickers_da_rimuovere.append(nome_ticker)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nella lettura del file {file}: {e}\")\n",
    "\n",
    "        # Rimuovi i tickers dalla lista\n",
    "        self.lista_tickers = self.lista_tickers[~self.lista_tickers['Ticker'].isin(tickers_da_rimuovere)]\n",
    "\n",
    "        # Aggiorna la lista dei tickers nel DataFrame\n",
    "        lista_files_aggiornata = os.listdir(cartella_tickers)\n",
    "        lista_files_aggiornata = [os.path.splitext(file)[0] for file in lista_files_aggiornata]\n",
    "        self.lista_tickers = self.lista_tickers[self.lista_tickers['Ticker'].isin(lista_files_aggiornata)]\n",
    "\n",
    "        # Salva il DataFrame aggiornato\n",
    "        self.lista_tickers.to_parquet('lista_ticker.parquet')\n",
    "       \n",
    "    def avvia_screener(self, append=False, inizia_da=0) -> None:\n",
    "        tot_tickers = len(self.lista_tickers)\n",
    "\n",
    "        for i in range(inizia_da, tot_tickers):\n",
    "            try:\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                print(\"\\033[42m\" + f'{i+1}/{tot_tickers}) Calcolo screeners per {nome_simbolo}' + \"\\033[0m\")\n",
    "                ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "                if append:\n",
    "                    scr = pd.read_hdf(f'screeners/{nome_simbolo}.h5', 'screener')\n",
    "                    scr_out = pd.read_hdf(f'screeners/{nome_simbolo}.h5', 'screener_out')\n",
    "                    inizio = scr.index.max() - pd.Timedelta(days=365)\n",
    "                    ticker_analisi = ticker.loc[ticker.index >= inizio].copy()\n",
    "                    if scr.index.max() == ticker.index.max():\n",
    "                        scr = scr.drop(scr.index[-1]).copy()\n",
    "                        scr_out = scr_out.drop(scr_out.index[-1]).copy()\n",
    "                    idx, X, Y, _ = to_XY(dati_ticker=ticker_analisi, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "                    idx_out, X_out, Y_out, _ = to_XY(dati_ticker=ticker_analisi, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features_out, targets=self.targets_out, bilanciamento=0)\n",
    "                    print(f'Aggiornamento previsione {nome_simbolo}')\n",
    "                    pred = self.modello_ingresso.model.predict(X)\n",
    "                    pred_out = self.modello_uscita.model.predict(X_out)\n",
    "                    scr_temp = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "                    scr_temp = scr_temp.loc[scr_temp.index > scr.index.max()].copy()\n",
    "                    scr = pd.concat([scr, scr_temp], axis=0, ignore_index=False)\n",
    "                    scr_temp = pd.DataFrame({'Previsione': pred_out.flatten().round(2), 'Reale': Y_out.flatten()}, index=idx_out)\n",
    "                    scr_temp = scr_temp.loc[scr_temp.index > scr_out.index.max()].copy()\n",
    "                    scr_out = pd.concat([scr_out, scr_temp], axis=0, ignore_index=False)\n",
    "                    print(f\"Aggiornamento file di screener {nome_simbolo}.h5\")\n",
    "                    scr.to_hdf(f'screeners/{nome_simbolo}.h5', key='screener', mode='w')\n",
    "                    scr_out.to_hdf(f'screeners/{nome_simbolo}.h5', key='screener_out', mode='a')\n",
    "                else:\n",
    "                    idx, X, Y, _ = to_XY(dati_ticker=ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "                    idx_out, X_out, Y_out, _ = to_XY(dati_ticker=ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features_out, targets=self.targets_out, bilanciamento=0)\n",
    "                    print(f'Previsione {nome_simbolo}')\n",
    "                    pred = self.modello_ingresso.model.predict(X)\n",
    "                    scr = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "                    scr = scr.loc[scr.index >= self.DATA_INIZIO]\n",
    "                    pred_out = self.modello_uscita.model.predict(X_out)\n",
    "                    scr_out = pd.DataFrame({'Previsione': pred_out.flatten().round(2), 'Reale': Y_out.flatten()}, index=idx_out)\n",
    "                    scr_out = scr_out.loc[scr_out.index >= self.DATA_INIZIO]\n",
    "                    print(f\"Salvataggio file di screener {nome_simbolo}.h5\")\n",
    "                    scr.to_hdf(f'screeners/{nome_simbolo}.h5', key='screener', mode='w')\n",
    "                    scr_out.to_hdf(f'screeners/{nome_simbolo}.h5', key='screener_out', mode='a')\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        self.carica_screener('screener')\n",
    "        self.carica_screener('screener_out')\n",
    "        self.screener.to_hdf('screeners/_screener.h5', key='screener', mode='w')\n",
    "        self.screener_out.to_hdf('screeners/_screener.h5', key='screener_out', mode='a')\n",
    "\n",
    "    def carica_screener(self, nome_dataset):\n",
    "        manager = Manager()\n",
    "        lista_scr = manager.list()\n",
    "        lista_scr_out = manager.list()\n",
    "        totale_processati = Value('i', 0)\n",
    "        with Pool(cpu_count()) as p:\n",
    "            callback_with_args = partial(_carica_screener_callback, totale_processati=totale_processati, tot_tickers=self.tot_tickers)\n",
    "            for i in range(0, self.tot_tickers):\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                param = (nome_simbolo, lista_scr, lista_scr_out, self.PROBABILITA_PER_ACQUISTO, nome_dataset)\n",
    "                p.apply_async(_carica_screener, args=param, callback=callback_with_args)\n",
    "            p.close()\n",
    "            p.join()  \n",
    "        if nome_dataset == 'screener':\n",
    "            self.screener = pd.concat(lista_scr, axis=0, ignore_index=False)\n",
    "            self.screener = self.screener.sort_values(by=['Data', 'Previsione'], ascending=[True, False])\n",
    "        else:\n",
    "            self.screener_out = pd.concat(lista_scr_out, axis=0, ignore_index=False)\n",
    "            self.screener_out = self.screener_out.sort_values(by=['Data', 'Previsione'], ascending=[True, False])\n",
    "\n",
    "    def ultimo_screener(self) -> (pd.Timestamp, pd.DataFrame):\n",
    "        # Ottieni l'ultimo valore dell'indice di livello 0\n",
    "        ultimo_indice = self.screener.index.get_level_values(0)[-1]\n",
    "        ultimo_indice_out = self.screener_out.index.get_level_values(0)[-1]\n",
    "        # Usa .loc per selezionare tutte le righe con quell'indice di livello 0\n",
    "        return ultimo_indice.date(), self.screener.loc[ultimo_indice], ultimo_indice_out.date(), self.screener_out.loc[ultimo_indice_out]\n",
    "\n",
    "    def compra(self, simbolo, prezzo_unitario_acquisto, budget_per_simbolo):\n",
    "        n_azioni_acquisto = budget_per_simbolo // prezzo_unitario_acquisto\n",
    "        prezzo_azioni = prezzo_unitario_acquisto * n_azioni_acquisto\n",
    "        commissione = self.applica_commissione(prezzo_azioni)\n",
    "        prezzo_totale_acquisto = prezzo_azioni + commissione \n",
    "        if (prezzo_totale_acquisto > budget_per_simbolo) or (n_azioni_acquisto == 0):\n",
    "            print(f'{simbolo}: Prezzo tot. > budget per simbolo ({prezzo_totale_acquisto} > {budget_per_simbolo})')\n",
    "            return None\n",
    "        else:\n",
    "            posizione = {\n",
    "                'simbolo': simbolo,\n",
    "                'prezzo_unitario': prezzo_unitario_acquisto,\n",
    "                'n_azioni': n_azioni_acquisto,\n",
    "                'commissione': commissione, \n",
    "                'prezzo_totale': prezzo_totale_acquisto,\n",
    "                'stato': 'ACQUISTO'\n",
    "            }\n",
    "            print(posizione.__str__())\n",
    "            pos_df = pd.DataFrame([posizione])\n",
    "            return pos_df\n",
    "\n",
    "    def simulazione_trading(self):\n",
    "        print('Inizio simulazione trading')\n",
    "        screener = self.screener.loc[(self.screener.index.get_level_values(0) >= self.DATA_INIZIO, slice(None)), :]\n",
    "        data_corrente = screener.index.get_level_values(0)[0]\n",
    "        screener_corrente = screener.loc[data_corrente]\n",
    "        budget_per_simbolo = self.BILANCIO_INIZIALE / self.N_SIMBOLI\n",
    "        posizioni_aperte = pd.DataFrame(columns=['simbolo', 'prezzo_unitario', 'n_azioni', 'commissione', 'prezzo_totale', 'stato'])       \n",
    "        \n",
    "        i_scr = 0\n",
    "        # Acquista dieci posizioni \n",
    "        while (len(posizioni_aperte) < 10) and (i_scr < len(screener_corrente)):\n",
    "            simbolo = screener_corrente.index[i_scr]\n",
    "            ticker = pd.read_hdf(f'tickers/{simbolo}.h5', 'ticker')    \n",
    "            prezzi_del_giorno = ticker.loc[data_corrente]\n",
    "            prezzo_unitario_acquisto = prezzi_del_giorno['Open']\n",
    "            posizione = self.compra(simbolo, prezzo_unitario_acquisto, budget_per_simbolo) \n",
    "            if posizione is not None:\n",
    "                posizioni_aperte = pd.concat([posizioni_aperte, posizione], ignore_index=True)\n",
    "            i_scr += 1\n",
    "        return posizioni_aperte    \n",
    "         \n",
    "    def applica_commissione(self, importo_transazione, broker='FINECO'):\n",
    "        if broker == 'FINECO':\n",
    "            tot = importo_transazione * 0.0019\n",
    "            if tot < 2.95:\n",
    "                return 2.95\n",
    "            elif tot > 19:\n",
    "                return 19\n",
    "            else:\n",
    "                return tot \n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    # Funzione obiettivo per l'ottimizzazione\n",
    "    def funzione_obiettivo(self, N_SIMBOLI, GIORNI_POS, SL, TP):\n",
    "        # Imposta i parametri\n",
    "        self.N_SIMBOLI = N_SIMBOLI\n",
    "        self.GIORNI_POS = GIORNI_POS\n",
    "        self.SL = SL\n",
    "        self.TP = TP\n",
    "        print('PARAMETRI:')\n",
    "        print(f'N.simboli = {N_SIMBOLI}')\n",
    "        print(f'Giorni pos. = {GIORNI_POS}')\n",
    "        print(f'SL = {SL}')\n",
    "        print(f'TP = {TP}')\n",
    "        # Esegui il trading\n",
    "        self.avvia_trading()\n",
    "\n",
    "        # Restituisce il bilancio finale in modo negativo per la minimizzazione\n",
    "        return -self._bilancio\n",
    "\n",
    "class Modello:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _crea_modello(self):      \n",
    "        # Input layer\n",
    "        input_layer = Input(shape=(self.timesteps, self.n_features))\n",
    "\n",
    "        # Convolutional layer\n",
    "        conv1 = Conv1D(filters=128, kernel_size=5, activation='relu')(input_layer)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "\n",
    "        # Continuation of the model\n",
    "        lstm2 = GRU(50)(conv1)\n",
    "        lstm2 = Dropout(0.5)(lstm2)\n",
    "\n",
    "        dense2 = Dense(80, activation='relu', kernel_regularizer=regularizers.l2(0.02))(lstm2)\n",
    "        dense2 = Dropout(0.5)(dense2)\n",
    "\n",
    "        batch_norm1 = BatchNormalization()(dense2)\n",
    "\n",
    "        dense3 = Dense(40, activation='relu', kernel_regularizer=regularizers.l2(0.02))(batch_norm1)\n",
    "        dense3 = Dropout(0.5)(dense3)\n",
    "\n",
    "        batch_norm2 = BatchNormalization()(dense3)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(1, activation='sigmoid')(batch_norm2)\n",
    "\n",
    "        adam = Adam(learning_rate=self.learning_rate)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC(curve='PR')])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def crea(self, \n",
    "             progetto='default', \n",
    "             timesteps=120, \n",
    "             giorni_previsione=1, \n",
    "             features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"], \n",
    "             targets=[\"Target_ingresso\"], \n",
    "             n_ticker_batch=400, \n",
    "             bilanciamento=1, \n",
    "             epochs=100,\n",
    "             batch_size=2052, \n",
    "             soglia=0.5, \n",
    "             class_weights={0: 3, 1: 1},\n",
    "             learning_rate=0.001, \n",
    "             train_test_split=0.2\n",
    "            ):\n",
    "        self.progetto = progetto\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.timesteps = timesteps # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "        self.giorni_previsione = giorni_previsione  # giorni futuri di cui effettuare la previsione\n",
    "        self.features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in self.features]\n",
    "        self.features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in self.features]\n",
    "        self.features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in self.features]\n",
    "        self.features_no_scala = [ft for ft in _features_no_scala_tutte if ft in self.features]\n",
    "        self.features_candele = [ft for ft in _features_candele_tutte if ft in self.features]\n",
    "        self.targets = targets\n",
    "        self.n_ticker_batch = n_ticker_batch\n",
    "        self.bilanciamento = bilanciamento\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.soglia = soglia\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_test_split = train_test_split\n",
    "        self.class_weights = class_weights\n",
    "        self.n_features = len(self.features) \n",
    "        self.n_targets = len(self.targets) \n",
    "        self.model = self._crea_modello() \n",
    "        self.model_history = None\n",
    "\n",
    "    def carica(self, progetto='default'):\n",
    "        self.progetto = progetto\n",
    "        percorso_file = f'{self.progetto}/impostazioni.json'\n",
    "        try:\n",
    "            with open(percorso_file, \"r\") as file:\n",
    "                impostazioni = json.load(file) if os.path.getsize(percorso_file) > 0 else {}\n",
    "                self.timesteps = impostazioni.get(\"timesteps\", 120) # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "                self.giorni_previsione = impostazioni.get(\"giorni_previsione\", 1)  # giorni futuri di cui effettuare la previsione\n",
    "                self.features = impostazioni.get(\"features\", [\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"])\n",
    "                self.features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in self.features]\n",
    "                self.features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in self.features]\n",
    "                self.features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in self.features]\n",
    "                self.features_no_scala = [ft for ft in _features_no_scala_tutte if ft in self.features]\n",
    "                self.features_candele = [ft for ft in _features_candele_tutte if ft in self.features]                \n",
    "                self.targets = impostazioni.get(\"targets\", [\"Target_ingresso\"])\n",
    "                self.n_features = len(self.features)\n",
    "                self.n_targets = len(self.targets) \n",
    "                self.n_ticker_batch = impostazioni.get(\"n_ticker_batch\", 400)\n",
    "                self.bilanciamento = impostazioni.get(\"bilanciamento\", 1)\n",
    "                self.batch_size = impostazioni.get(\"batch_size\", 2052)\n",
    "                self.epochs = impostazioni.get(\"epochs\", 100)\n",
    "                self.soglia = impostazioni.get(\"soglia\", 0.5)\n",
    "                self.learning_rate = impostazioni.get(\"learnig_rate\", 0.001)\n",
    "                self.train_test_split = impostazioni.get(\"train_test_split\", 0.2)\n",
    "                self.class_weights = impostazioni.get(\"class_weights\", {0: 3, 1: 1})\n",
    "                self.n_features = len(self.features) \n",
    "                self.n_targets = len(self.targets) \n",
    "                self.model = load_model(f\"{self.progetto}/model.h5\")  \n",
    "                self.model_history = pd.read_hdf(f'{progetto}/model_history.h5', 'history')      \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del file: {e}\")\n",
    "\n",
    "    def _salva_impostazioni(self):\n",
    "        impostazioni = {\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"giorni_previsione\": self.giorni_previsione,\n",
    "            \"features\": self.features,\n",
    "            \"targets\": self.targets,\n",
    "            \"n_ticker_batch\": self.n_ticker_batch,\n",
    "            \"bilanciamento\": self.bilanciamento,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs, \n",
    "            \"soglia\": self.soglia,\n",
    "            \"class_weights\": self.class_weights\n",
    "        }\n",
    "        with open(f'{self.progetto}/impostazioni.json', \"w\") as file:\n",
    "            json.dump(impostazioni, file, indent=4)\n",
    "            \n",
    "        modello = json.loads(self.model.to_json())\n",
    "        with open(f'{self.progetto}/struttura.json', \"w\") as file:\n",
    "            json.dump(modello, file, indent=4)\n",
    "\n",
    "    def salva(self):\n",
    "        os.makedirs(self.progetto, exist_ok=True)\n",
    "        self._salva_impostazioni()\n",
    "        self.model.save(f'{self.progetto}/model.h5')\n",
    "        self.model_history.to_hdf(f'{self.progetto}/model_history.h5', key='history', mode='w')\n",
    "\n",
    "    def genera_XY(self, lista_files, nome_file=''):\n",
    "        perc_file = f'XY/XY_{nome_file}.h5'\n",
    "        if not os.path.exists(perc_file):\n",
    "            manager = Manager()\n",
    "            listaX = manager.list()\n",
    "            listaY = manager.list()\n",
    "            totale_processati = Value('i', 1)  \n",
    "            tot_files = len(lista_files)\n",
    "            with Pool(cpu_count()) as p:\n",
    "                for file_name in lista_files:\n",
    "                    param = (file_name, self.timesteps, self.giorni_previsione, self.features, self.targets, self.bilanciamento)\n",
    "                    p.apply_async(_process_ticker, args=param, callback=lambda result: _callback_XY(result, listaX, listaY, totale_processati, tot_files, hdf5_file=perc_file))\n",
    "\n",
    "                p.close()\n",
    "                p.join()\n",
    "\n",
    "            if len(listaX) > 0:\n",
    "                print(\"\\033[43m\" + 'Salvataggio finale su file X' + \"\\033[0m\")\n",
    "                concatena(listaX, perc_file, dataset_name='X')\n",
    "                print(\"\\033[43m\" + 'Salvataggio finale su file Y' + \"\\033[0m\")\n",
    "                concatena(listaY, perc_file, dataset_name='Y')\n",
    "                del listaX[:]\n",
    "                del listaY[:]\n",
    "\n",
    "    def addestra(self):\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001, verbose=1)\n",
    "        #model_checkpoint = ModelCheckpoint(f'{self.progetto}/model.h5', monitor='val_precision', save_best_only=True, verbose=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "        list_of_files = os.listdir('tickers')\n",
    "        random.shuffle(list_of_files)\n",
    "        train_files = list_of_files[:self.n_ticker_batch]\n",
    "        val_files = list_of_files[self.n_ticker_batch:int(self.n_ticker_batch*(1+self.train_test_split))]\n",
    "        \n",
    "        print(\"\\033[41m\" + 'Preparazione dati di train' + \"\\033[0m\")\n",
    "        self.genera_XY(train_files, 'train')\n",
    "        print(\"\\033[41m\" + 'Preparazione dati di validazione' + \"\\033[0m\")\n",
    "        self.genera_XY(val_files, 'val')\n",
    "        train_generator = DataGenerator('XY/XY_train.h5', self.batch_size)\n",
    "        val_generator = DataGenerator('XY/XY_val.h5', self.batch_size)    \n",
    "           \n",
    "        history = self.model.fit(train_generator, epochs=self.epochs, validation_data=val_generator, callbacks=callbacks, class_weight=self.class_weights, steps_per_epoch=len(train_generator), validation_steps=len(val_generator))\n",
    "        self.model_history = pd.DataFrame(history.history)\n",
    "        self.salva()\n",
    "        self.grafico_loss(salva_su_file=True)\n",
    "        self.grafico_precision(salva_su_file=True)\n",
    "        df = self.test()\n",
    "        return df\n",
    "        \n",
    "    def previsione_singola(self, nome_simbolo):\n",
    "        print(f'Caricamento dati ticker {nome_simbolo}')\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "        print('Generazione X e Y')\n",
    "        idx, X, Y, _ = to_XY(dati_ticker=ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "        print(f'Previsione')\n",
    "        pred = self.modello_ingresso.model.predict(X)\n",
    "        scr = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "        scr = scr[scr.index >= self.DATA_INIZIO]\n",
    "        print(f\"Salvataggio file {nome_simbolo}.h5\")\n",
    "    \n",
    "    def grafico_loss(self, salva_su_file=False):\n",
    "        num_epochs = self.model_history.shape[0]\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['loss'], label=\"Training\")\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['val_loss'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title('Loss')\n",
    "        plt.tight_layout()\n",
    "        if salva_su_file:\n",
    "            plt.savefig(f'{self.progetto}/grafico_loss.png')\n",
    "        plt.show()        \n",
    "\n",
    "    def grafico_precision(self, salva_su_file=False):\n",
    "        num_epochs = self.model_history.shape[0]\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['precision'], label=\"Training\")\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['val_precision'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title('Precision')\n",
    "        plt.tight_layout()\n",
    "        if salva_su_file:\n",
    "            plt.savefig(f'{self.progetto}/grafico_precision.png')\n",
    "        plt.show()        \n",
    "\n",
    "    def test(self, nome_simbolo='BTG'):\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "        ticker = dropna_iniziali(ticker)\n",
    "        ticker = dropna_finali(ticker)\n",
    "        idx, X, Y, _ = to_XY(ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "        print(f'X.shape: {X.shape}')\n",
    "        print(f'Y.shape: {Y.shape}')\n",
    "        print(f'ticker.shape: {ticker.shape}')\n",
    "        pred = self.model.predict(X, batch_size=self.batch_size, verbose=1, use_multiprocessing=True)\n",
    "        pred_binary = (pred > self.soglia).astype(int)\n",
    "        \n",
    "        result = self.model.evaluate(X, Y, batch_size=self.batch_size, verbose=1, use_multiprocessing=True, return_dict=True)\n",
    "        print(result)\n",
    "        \n",
    "        # Visualizza come heatmap\n",
    "        matrice = confusion_matrix(Y, pred_binary)\n",
    "        sns.heatmap(matrice, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Previsti')\n",
    "        plt.ylabel('Reali')\n",
    "        plt.savefig(f'{self.progetto}/confusion_matrix.png')\n",
    "        plt.show()\n",
    "        print(f'idx: {idx.shape}')\n",
    "        print(f'pred: {pred.shape}')\n",
    "        print(f'real: {Y.shape}')\n",
    "        df = pd.DataFrame({'Prev': pred.flatten().round(2), 'Real': Y.flatten()}, index=idx)\n",
    "        return df       \n",
    "\n",
    "def _process_ticker(file_name, timesteps, giorni_previsione, features, targets, bilanciamento):\n",
    "    try:\n",
    "        ticker = pd.read_hdf(f'tickers/{file_name}', 'ticker')\n",
    "        ticker = dropna_iniziali(ticker)\n",
    "        ticker = dropna_finali(ticker)\n",
    "        _, X, Y, _ = to_XY(ticker, timesteps, giorni_previsione, features, targets, bilanciamento)\n",
    "        return file_name, X, Y, \"\"\n",
    "    except Exception as e:\n",
    "        return file_name, np.array([]), np.array([]), str(e)\n",
    "\n",
    "def _callback_XY(result, listaX, listaY, totale_processati, tot_files, hdf5_file):\n",
    "    nome_simbolo, X, Y, err = result\n",
    "    if err == \"\":\n",
    "        if X.shape[0] > 0 and Y.shape[0] > 0:  # Verifica se X e Y sono non vuoti\n",
    "            print(f'X.shape:{X.shape}')\n",
    "            print(f'Y.shape:{Y.shape}')\n",
    "            listaX.append(X)\n",
    "            listaY.append(Y)\n",
    "            print(\"\\033[42m\" + f\"{totale_processati.value}/{tot_files}) Completato ticker {nome_simbolo}\" + \"\\033[0m\")\n",
    "        else:\n",
    "            print(\"\\033[43m\" + f\"Ticker {nome_simbolo} ignorato a causa di dati mancanti o errati.\" + \"\\033[0m\")\n",
    "    else:\n",
    "        print(err)\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "        if len(listaX) >= 100:\n",
    "            print(\"\\033[43m\" + 'Salvataggio su file X' + \"\\033[0m\")\n",
    "            concatena(listaX, hdf5_file, dataset_name='X')\n",
    "            print(\"\\033[43m\" + 'Salvataggio su file Y' + \"\\033[0m\")\n",
    "            concatena(listaY, hdf5_file, dataset_name='Y')\n",
    "            del listaX[:]\n",
    "            del listaY[:]\n",
    "     \n",
    "def modifica_target():\n",
    "    totale_processati = Value('i', 1)\n",
    "    list_of_files = os.listdir('tickers')\n",
    "    tot_tickers = len(list_of_files)\n",
    "    with Pool(cpu_count()) as p:\n",
    "        callback_with_args = partial(_callback_modifica_target, totale_processati=totale_processati, tot_tickers=tot_tickers)\n",
    "        for i in range(0, tot_tickers):\n",
    "            nome_file = list_of_files[i]\n",
    "            param = (nome_file,)\n",
    "            p.apply_async(_modifica_target, args=param, callback=callback_with_args)\n",
    "        p.close()\n",
    "        p.join()          \n",
    "   \n",
    "def _modifica_target(nome_file):\n",
    "    try:\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_file}', 'ticker')\n",
    "        if 'Target_ingresso' in ticker.columns:\n",
    "            ticker.drop(['Target_ingresso'], axis=1, inplace=True)\n",
    "        if 'Target_uscita' in ticker.columns:\n",
    "            ticker.drop(['Target_uscita'], axis=1, inplace=True)\n",
    "        ticker = imposta_target(ticker)\n",
    "        return nome_file, ticker, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_file, ticker, str(e)\n",
    "\n",
    "def _callback_modifica_target(result, totale_processati, tot_tickers):\n",
    "    nome_file, ticker, err = result\n",
    "    if err == \"\":\n",
    "        print(f\"{totale_processati.value}/{tot_tickers}) Modificato target {nome_file}\")\n",
    "        ticker.to_hdf(f'tickers/{nome_file}', key='ticker', mode='w')\n",
    "    else:\n",
    "        print(err)\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "        \n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, file_path, batch_size):\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        # Apriamo il file in modalità lettura e salviamo i riferimenti ai dataset\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.X = self.file['X']\n",
    "        self.Y = self.file['Y']\n",
    "        self.num_samples = self.X.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calcola gli indici per il batch corrente\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "\n",
    "        # Legge solo i dati necessari per il batch corrente\n",
    "        batch_x = self.X[start:end]\n",
    "        batch_y = self.Y[start:end]\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Eventuali azioni alla fine di ogni epoca, se necessario\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        # Assicurati di chiudere il file quando il generatore viene distrutto\n",
    "        self.file.close()\n",
    "\n",
    "def ottimizzazione_parametri():\n",
    "    PROBABILITA_PER_ACQUISTO = 0.5\n",
    "    BILANCIO_INIZIALE = 1000\n",
    "    GIORNI_MAX_POSIZIONE = 40\n",
    "    N_SIMBOLI = 10\n",
    "    DATA_INIZIO = pd.Timestamp(2013, 1, 1)\n",
    "    \n",
    "    inizializza_gpu()\n",
    "\n",
    "    borsa = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, PROBABILITA_PER_ACQUISTO, GIORNI_MAX_POSIZIONE, data_inizio=DATA_INIZIO)\n",
    "    borsa.aggiorna_dati()\n",
    "    \n",
    "    valori_N_SIMBOLI = list(range(1, 11))  # Valori da 1 a 10\n",
    "    valori_GIORNI_POS = list(range(5, 61, 5))  # Valori da 5 a 60 con step di 5\n",
    "    valori_SL = list(np.arange(0.01, 0.1, 0.01))\n",
    "    valori_TP = list(np.arange(0.1, 1, 0.05))\n",
    "\n",
    "    space  = [\n",
    "        Categorical(valori_N_SIMBOLI, name='N_SIMBOLI'),  \n",
    "        Categorical(valori_GIORNI_POS, name='GIORNI_POS'),  \n",
    "        Categorical(valori_SL, name='SL'),  \n",
    "        Categorical(valori_TP, name='TP')\n",
    "    ]\n",
    "\n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        return borsa.funzione_obiettivo(**params)\n",
    "\n",
    "    risultato = gp_minimize(objective, space, n_calls=50, random_state=0)\n",
    "\n",
    "    print(f\"Migliori parametri: {risultato.x}\")\n",
    "\n",
    "    x_iters = risultato.x_iters  # Lista di parametri testati\n",
    "    fun_values = risultato.func_vals  # Lista di valori della funzione obiettivo\n",
    "\n",
    "    df_results = pd.DataFrame(x_iters, columns=['N_SIMBOLI', 'GIORNI_POS', 'SL', 'TP'])\n",
    "    df_results['Valore_Funzione'] = fun_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e1218b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PROBABILITA_PER_ACQUISTO = 0.5\n",
    "BILANCIO_INIZIALE = 1000\n",
    "GIORNI_MAX_POSIZIONE = 40\n",
    "N_SIMBOLI = 10\n",
    "DATA_INIZIO = pd.Timestamp(2013, 1, 1)\n",
    "\n",
    "inizializza_gpu()\n",
    "\n",
    "borsa = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, PROBABILITA_PER_ACQUISTO, GIORNI_MAX_POSIZIONE, data_inizio=DATA_INIZIO)\n",
    "borsa.aggiorna_dati()\n",
    "data, scr, data_out, scr_out = borsa.ultimo_screener()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d772bd",
   "metadata": {},
   "source": [
    "print(data)\n",
    "scr.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe12701",
   "metadata": {},
   "source": [
    "print(data_out)\n",
    "scr_out.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dd30936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Caricamento screener esistenti\n",
      "Inizio simulazione trading\n",
      "{'simbolo': 'DHT', 'prezzo_unitario': 4.179999828338623, 'n_azioni': 23.0, 'commissione': 2.95, 'prezzo_totale': 99.08999605178833, 'stato': 'ACQUISTO'}\n",
      "CSIQ: Prezzo tot. > budget per simbolo (100.95 > 100.0)\n",
      "TIT: Prezzo tot. > budget per simbolo (102.61000366210938 > 100.0)\n",
      "PHN: Prezzo tot. > budget per simbolo (100.90999908447266 > 100.0)\n",
      "{'simbolo': 'BRE', 'prezzo_unitario': 9.300600051879883, 'n_azioni': 10.0, 'commissione': 2.95, 'prezzo_totale': 95.95600051879883, 'stato': 'ACQUISTO'}\n",
      "{'simbolo': 'TLK', 'prezzo_unitario': 18.5049991607666, 'n_azioni': 5.0, 'commissione': 2.95, 'prezzo_totale': 95.47499580383301, 'stato': 'ACQUISTO'}\n",
      "FCNCA: Prezzo tot. > budget per simbolo (2.95 > 100.0)\n",
      "{'simbolo': 'WFG', 'prezzo_unitario': 36.8849983215332, 'n_azioni': 2.0, 'commissione': 2.95, 'prezzo_totale': 76.71999664306641, 'stato': 'ACQUISTO'}\n",
      "HIMX: Prezzo tot. > budget per simbolo (102.17000312805176 > 100.0)\n",
      "CZNC: Prezzo tot. > budget per simbolo (100.39999885559082 > 100.0)\n",
      "{'simbolo': 'PFC', 'prezzo_unitario': 9.210000038146973, 'n_azioni': 10.0, 'commissione': 2.95, 'prezzo_totale': 95.05000038146973, 'stato': 'ACQUISTO'}\n",
      "{'simbolo': 'SBCF', 'prezzo_unitario': 8.0, 'n_azioni': 12.0, 'commissione': 2.95, 'prezzo_totale': 98.95, 'stato': 'ACQUISTO'}\n",
      "{'simbolo': 'FHI', 'prezzo_unitario': 20.700000762939453, 'n_azioni': 4.0, 'commissione': 2.95, 'prezzo_totale': 85.75000305175782, 'stato': 'ACQUISTO'}\n",
      "{'simbolo': 'CASH', 'prezzo_unitario': 7.7733330726623535, 'n_azioni': 12.0, 'commissione': 2.95, 'prezzo_totale': 96.22999687194825, 'stato': 'ACQUISTO'}\n",
      "{'simbolo': 'CALM', 'prezzo_unitario': 20.2450008392334, 'n_azioni': 4.0, 'commissione': 2.95, 'prezzo_totale': 83.9300033569336, 'stato': 'ACQUISTO'}\n",
      "FMX: Prezzo tot. > budget per simbolo (2.95 > 100.0)\n",
      "DQ: Prezzo tot. > budget per simbolo (102.1500014781952 > 100.0)\n",
      "PBR: Prezzo tot. > budget per simbolo (102.20000190734864 > 100.0)\n",
      "{'simbolo': 'X', 'prezzo_unitario': 25.18000030517578, 'n_azioni': 3.0, 'commissione': 2.95, 'prezzo_totale': 78.49000091552735, 'stato': 'ACQUISTO'}\n"
     ]
    }
   ],
   "source": [
    "TP = 0.20\n",
    "SL = 0.07\n",
    "BILANCIO_INIZIALE = 1000\n",
    "N_SIMBOLI = 10\n",
    "DATA_INIZIO = pd.Timestamp(2013, 1, 1)\n",
    "\n",
    "inizializza_gpu()\n",
    "\n",
    "borsa = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, data_inizio=DATA_INIZIO)#, stop_loss=SL, take_profit=TP)\n",
    "borsa.aggiorna_dati()\n",
    "pos = borsa.simulazione_trading()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "158ef6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>simbolo</th>\n",
       "      <th>prezzo_unitario</th>\n",
       "      <th>n_azioni</th>\n",
       "      <th>commissione</th>\n",
       "      <th>prezzo_totale</th>\n",
       "      <th>stato</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DHT</td>\n",
       "      <td>4.180000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>99.089996</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRE</td>\n",
       "      <td>9.300600</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>95.956001</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TLK</td>\n",
       "      <td>18.504999</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>95.474996</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WFG</td>\n",
       "      <td>36.884998</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>76.719997</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PFC</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>95.050000</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SBCF</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>98.950000</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FHI</td>\n",
       "      <td>20.700001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>85.750003</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CASH</td>\n",
       "      <td>7.773333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>96.229997</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CALM</td>\n",
       "      <td>20.245001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>83.930003</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>X</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>78.490001</td>\n",
       "      <td>ACQUISTO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  simbolo  prezzo_unitario  n_azioni  commissione  prezzo_totale     stato\n",
       "0     DHT         4.180000      23.0         2.95      99.089996  ACQUISTO\n",
       "1     BRE         9.300600      10.0         2.95      95.956001  ACQUISTO\n",
       "2     TLK        18.504999       5.0         2.95      95.474996  ACQUISTO\n",
       "3     WFG        36.884998       2.0         2.95      76.719997  ACQUISTO\n",
       "4     PFC         9.210000      10.0         2.95      95.050000  ACQUISTO\n",
       "5    SBCF         8.000000      12.0         2.95      98.950000  ACQUISTO\n",
       "6     FHI        20.700001       4.0         2.95      85.750003  ACQUISTO\n",
       "7    CASH         7.773333      12.0         2.95      96.229997  ACQUISTO\n",
       "8    CALM        20.245001       4.0         2.95      83.930003  ACQUISTO\n",
       "9       X        25.180000       3.0         2.95      78.490001  ACQUISTO"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119865eb",
   "metadata": {},
   "source": [
    "borsa._posizioni[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "df = pd.read_hdf('tickers/SOHU.h5', 'ticker')\n",
    "grafico(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mod = Modello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mod.carica(progetto='mod_1_in')\n",
    "mod.class_weights = {0: 1, 1: 1}\n",
    "df = mod.addestra()\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "mod.crea(\n",
    "    'mod_1_in', \n",
    "    class_weights={0: 1, 1: 1}, \n",
    "    bilanciamento=1, \n",
    "    timesteps=240, \n",
    "    batch_size=8000, \n",
    "    learning_rate=0.01, \n",
    "    n_ticker_batch=300, \n",
    "    train_test_split=0.2,\n",
    "    features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\"]#, \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"]\n",
    ")\n",
    "df = mod.addestra()\n",
    "df\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "mod.crea(\n",
    "    'mod_1_out', \n",
    "    class_weights={0: 1, 1: 1}, \n",
    "    bilanciamento=1, \n",
    "    timesteps=240, \n",
    "    batch_size=4000, \n",
    "    learning_rate=0.01, \n",
    "    n_ticker_batch=300, \n",
    "    train_test_split=0.2,\n",
    "    features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\"],#, \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"]\n",
    "    targets=['Target_uscita']\n",
    ")\n",
    "df = mod.addestra()\n",
    "df\n",
    "mod.salva()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
