{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85a9176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from multiprocessing import Pool, cpu_count, Manager, Value\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization, LSTM, Dropout, Dense, Conv1D, Flatten, GRU, Attention, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas_ta as ta\n",
    "import random\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.subplots as sp\n",
    "\n",
    "_features_scala_prezzo_tutte = [\n",
    "    \"Close\",\n",
    "    \"EMA_5\", \n",
    "    \"EMA_20\", \n",
    "    \"EMA_50\",\n",
    "    \"EMA_100\",\n",
    "    \"Open\",  \n",
    "    \"High\",\n",
    "    \"Low\",\n",
    "    \"PSAR\",\n",
    "    \"SUPERT\", \n",
    "]\n",
    "\n",
    "_features_da_scalare_singolarmente_tutte = [\n",
    "    \"Volume\",\n",
    "    \"ATR\",\n",
    "    \"PSARaf\",\n",
    "    \"ADX\",\n",
    "    \"OBV\"\n",
    "]\n",
    "\n",
    "_features_oscillatori_tutte = [\n",
    "    \"MACDh\",    \n",
    "    \"MACD\",\n",
    "    \"MACDs\",\n",
    "    \"AROONOSC\",\n",
    "    \"TRIX\",\n",
    "    \"TRIXs\",\n",
    "    \"DM_OSC\",\n",
    "    \"TSI\",\n",
    "    \"TSIs\",\n",
    "    \"ROC_10\",\n",
    "    \"KVO\",\n",
    "    \"KVOs\",\n",
    "    \"VI_OSC\"\n",
    "]\n",
    "\n",
    "_features_no_scala_tutte = [\n",
    "    \"SUPERTd\",  \n",
    "    \"PSARr\",\n",
    "    \"CMF\",\n",
    "    \"VHF\",\n",
    "    \"VTX_OSC\"\n",
    "]\n",
    "\n",
    "_features_candele_tutte = [\n",
    "    \"CDL_2CROWS\", \"CDL_3BLACKCROWS\", \"CDL_3INSIDE\", \"CDL_3LINESTRIKE\", \"CDL_3OUTSIDE\", \"CDL_3STARSINSOUTH\", \"CDL_3WHITESOLDIERS\", \"CDL_ABANDONEDBABY\", \"CDL_ADVANCEBLOCK\", \"CDL_BELTHOLD\", \"CDL_BREAKAWAY\", \"CDL_CLOSINGMARUBOZU\", \"CDL_CONCEALBABYSWALL\", \"CDL_COUNTERATTACK\", \"CDL_DARKCLOUDCOVER\", \"CDL_DOJI_10_0.1\", \"CDL_DOJISTAR\", \"CDL_DRAGONFLYDOJI\", \"CDL_ENGULFING\", \"CDL_EVENINGDOJISTAR\", \"CDL_EVENINGSTAR\", \"CDL_GAPSIDESIDEWHITE\", \"CDL_GRAVESTONEDOJI\", \"CDL_HAMMER\", \"CDL_HANGINGMAN\", \"CDL_HARAMI\", \"CDL_HARAMICROSS\", \"CDL_HIGHWAVE\", \"CDL_HIKKAKE\", \"CDL_HIKKAKEMOD\", \"CDL_HOMINGPIGEON\", \"CDL_IDENTICAL3CROWS\", \"CDL_INNECK\", \"CDL_INSIDE\", \"CDL_INVERTEDHAMMER\", \"CDL_KICKING\", \"CDL_KICKINGBYLENGTH\", \"CDL_LADDERBOTTOM\", \"CDL_LONGLEGGEDDOJI\", \"CDL_LONGLINE\", \"CDL_MARUBOZU\", \"CDL_MATCHINGLOW\", \"CDL_MATHOLD\", \"CDL_MORNINGDOJISTAR\", \"CDL_MORNINGSTAR\", \"CDL_ONNECK\", \"CDL_PIERCING\", \"CDL_RICKSHAWMAN\", \"CDL_RISEFALL3METHODS\", \"CDL_SEPARATINGLINES\", \"CDL_SHOOTINGSTAR\", \"CDL_SHORTLINE\", \"CDL_SPINNINGTOP\", \"CDL_STALLEDPATTERN\", \"CDL_STICKSANDWICH\", \"CDL_TAKURI\", \"CDL_TASUKIGAP\", \"CDL_THRUSTING\", \"CDL_TRISTAR\", \"CDL_UNIQUE3RIVER\", \"CDL_UPSIDEGAP2CROWS\", \"CDL_XSIDEGAP3METHODS\",\n",
    "]\n",
    "\n",
    "def inizializza_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"nessuna GPU\")\n",
    "    \n",
    "def pct_change(valore_iniziale, valore_finale):\n",
    "    try:\n",
    "        return ((valore_finale - valore_iniziale) / valore_iniziale) * 100\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def analizza_ticker(nome_simbolo, start, end, progress=True, dropna_iniziali=False, dropna_finali=False):\n",
    "    start_str = start.strftime('%Y-%m-%d')\n",
    "    end_str = end.strftime('%Y-%m-%d')\n",
    "    df = yf.download(nome_simbolo, start=start_str, end=end_str, progress=progress)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = crea_indicatori(df)\n",
    "    if dropna_iniziali:\n",
    "        idx = df[df.notna().all(axis=1) == True].index[0]\n",
    "        df = df[idx:]\n",
    "    if dropna_finali:\n",
    "        idx = df[df.notna().all(axis=1) == True].index[-1]\n",
    "        df = df[:idx]\n",
    "    df = imposta_target(df)\n",
    "    return df\n",
    "\n",
    "def dropna_iniziali(df):\n",
    "    idx = df[df.notna().all(axis=1) == True].index[0]\n",
    "    df = df[idx:]\n",
    "    return df\n",
    "\n",
    "def dropna_finali(df):\n",
    "    idx = df[df.notna().all(axis=1) == True].index[-1]\n",
    "    df = df[:idx]\n",
    "    return df\n",
    "\n",
    "def imposta_target(df):\n",
    "    df['pct_change_5d'] = df.apply(lambda row: pct_change(row['EMA_5'], row['EMA_5_5d']), axis=1)    \n",
    "    df['pct_change_10d'] = df.apply(lambda row: pct_change(row['EMA_5'], row['EMA_5_10d']), axis=1)\n",
    "    \n",
    "    df['Target_ingresso'] = (\n",
    "        (df['pct_change_10d'] > 5) & (df['Close_10d'] > df['EMA_5_10d'])\n",
    "    )\n",
    "    df['Target_uscita'] = (\n",
    "        (df['pct_change_5d'] < -5) & (df['Close_5d'] < df['EMA_5_5d'])\n",
    "    )    \n",
    "    return df\n",
    "    \n",
    "def grafico(df):\n",
    "    close = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['Close'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, .9)'),\n",
    "        name = 'Close'\n",
    "    )\n",
    "\n",
    "    close2 = go.Scatter( # serve solo per il fill del supertrend\n",
    "        x = df.index,\n",
    "        y = df['Close'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, 0)'),\n",
    "        showlegend=False,\n",
    "        name = 'Close2'\n",
    "    )\n",
    "\n",
    "    # min5 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -5].index,\n",
    "    #     y = df[df['MaxMinRel'] == -5]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 5,\n",
    "    #         color = 'rgba(255, 0, 0, .9)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel5'\n",
    "    # )\n",
    "  \n",
    "    # max5 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 5].index,\n",
    "    #     y = df[df['MaxMinRel'] == 5]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 5,\n",
    "    #         color = 'rgba(50, 205, 50, .9)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel5'\n",
    "    # )\n",
    "\n",
    "    # min10 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -10].index,\n",
    "    #     y = df[df['MaxMinRel'] == -10]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 15,\n",
    "    #         color = 'rgba(255, 0, 0, .4)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel10'\n",
    "    # )\n",
    "  \n",
    "    # max10 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 10].index,\n",
    "    #     y = df[df['MaxMinRel'] == 10]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 15,\n",
    "    #         color = 'rgba(50, 205, 50, .4)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel10'\n",
    "    # )\n",
    "\n",
    "    # min20 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -20].index,\n",
    "    #     y = df[df['MaxMinRel'] == -20]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 25,\n",
    "    #         color = 'rgba(255, 0, 0, .2)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel20'\n",
    "    # )\n",
    "  \n",
    "    # max20 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 20].index,\n",
    "    #     y = df[df['MaxMinRel'] == 20]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 25,\n",
    "    #         color = 'rgba(50, 205, 50, .2)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel20'\n",
    "    # )\n",
    "\n",
    "    # min60 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -60].index,\n",
    "    #     y = df[df['MaxMinRel'] == -60]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 35,\n",
    "    #         color = 'rgba(255, 0, 0, .1)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel60'\n",
    "    # )\n",
    "  \n",
    "    # max60 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 60].index,\n",
    "    #     y = df[df['MaxMinRel'] == 60]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 35,\n",
    "    #         color = 'rgba(50, 205, 50, .1)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel60'\n",
    "    # )\n",
    "\n",
    "    ema5 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_5'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='blue'),\n",
    "        name = 'EMA5'\n",
    "    )\n",
    "\n",
    "    ema20 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_20'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='limegreen'),\n",
    "        name = 'EMA20'\n",
    "    )\n",
    "\n",
    "    ema50 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_50'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='orange'),\n",
    "        name = 'EMA50'\n",
    "    )\n",
    "    \n",
    "    ema100 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_100'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='red'),\n",
    "        name = 'EMA100'\n",
    "    )\n",
    "    \n",
    "    psar = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['PSAR'], \n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 2, \n",
    "            color = 'rgba(0, 0, 0, .8)',  \n",
    "        ),\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SAR Parabolico'\n",
    "    )\n",
    "    \n",
    "    stup = np.where(df['SUPERTd'] == 1, df['SUPERT'], np.nan)\n",
    "    stdown = np.where(df['SUPERTd'] == -1, df['SUPERT'], np.nan)\n",
    "    stupfill = np.where(df['SUPERTd'] == 1, df['SUPERT'], df['Close'])\n",
    "    stdownfill = np.where(df['SUPERTd'] == -1, df['SUPERT'], df['Close'])\n",
    "    \n",
    "    strendup = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stup, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='limegreen',\n",
    "            width=1\n",
    "        ),\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        connectgaps = False,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "        \n",
    "    strendupfill = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stupfill, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0,0,0,0)',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        fill='tonexty',   \n",
    "        fillcolor='rgba(50, 205, 50, 0.1)', \n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "        \n",
    "    strenddown = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stdown, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='red',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "    \n",
    "    strenddownfill = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stdownfill, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0,0,0,0)',\n",
    "            width=1\n",
    "        ),\n",
    "        fill='tonexty',   \n",
    "        fillcolor='rgba(255, 0, 0, 0.1)', \n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "    \n",
    "    target_in = go.Scatter(\n",
    "        x = df[df['Target_ingresso'] == 1].index,\n",
    "        y = df[df['Target_ingresso'] == 1]['Close'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(0, 200, 0, .9)'\n",
    "        ),\n",
    "        name = 'target_in'\n",
    "    )\n",
    "    \n",
    "    target_out = go.Scatter(\n",
    "        x = df[df['Target_uscita'] == 1].index,\n",
    "        y = df[df['Target_uscita'] == 1]['Close'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(220, 0, 0, .9)'\n",
    "        ),\n",
    "        name = 'target_out'\n",
    "    )\n",
    "        \n",
    "    layout = dict(xaxis = dict(autorange=True),\n",
    "                  yaxis = dict(title = 'Close', autorange=True),\n",
    "                  autosize = True,\n",
    "                  margin = go.layout.Margin(\n",
    "                      l=0,  # Sinistra\n",
    "                      r=0,  # Destra\n",
    "                      b=0,  # Basso\n",
    "                      t=50,  # Alto\n",
    "                      pad=0  # Padding\n",
    "                  ),\n",
    "                  legend = dict(traceorder = 'normal', bordercolor = 'black')\n",
    "    )\n",
    "        \n",
    "    fig = sp.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    fig.update_layout(layout)\n",
    "    \n",
    "    # RIGA 1\n",
    "\n",
    "    fig.add_trace(close, row=1, col=1)\n",
    "    fig.add_trace(strendupfill, row=1, col=1)\n",
    "    fig.add_trace(strendup, row=1, col=1)\n",
    "    fig.add_trace(close2, row=1, col=1)\n",
    "    fig.add_trace(strenddownfill, row=1, col=1)\n",
    "    fig.add_trace(strenddown, row=1, col=1)\n",
    "#    fig.add_trace(min5, row=1, col=1); fig.add_trace(max5, row=1, col=1)\n",
    "#    fig.add_trace(min10, row=1, col=1); fig.add_trace(max10, row=1, col=1)\n",
    "#    fig.add_trace(min20, row=1, col=1); fig.add_trace(max20, row=1, col=1)\n",
    "#    fig.add_trace(min60, row=1, col=1); fig.add_trace(max60, row=1, col=1)\n",
    "    fig.add_trace(target_in, row=1, col=1); fig.add_trace(target_out, row=1, col=1)\n",
    "    fig.add_trace(ema5, row=1, col=1); fig.add_trace(ema20, row=1, col=1); fig.add_trace(ema50, row=1, col=1); fig.add_trace(ema100, row=1, col=1)\n",
    "    fig.add_trace(psar, row=1, col=1)\n",
    "    \n",
    "    pyo.plot(fig, filename=\"grafico_target.html\", auto_open=True)\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def crea_indicatori(df):\n",
    "    def __trova_massimi_minimi(df, periodo):\n",
    "        mezzo_periodo = periodo // 2\n",
    "\n",
    "        massimi_passati = df['Close'].shift(1).rolling(mezzo_periodo).max()\n",
    "        massimi_futuri = df['Close'][::-1].shift(1).rolling(mezzo_periodo).max()[::-1]\n",
    "        idx_massimi = (df[\"Close\"] >= massimi_passati) & (df[\"Close\"] >= massimi_futuri)\n",
    "        df.loc[idx_massimi, \"MaxMinRel\"] = periodo\n",
    "\n",
    "        minimi_passati = df['Close'].shift(1).rolling(mezzo_periodo).min()\n",
    "        minimi_futuri = df['Close'][::-1].shift(1).rolling(mezzo_periodo).min()[::-1]\n",
    "        idx_minimi = (df[\"Close\"] <= minimi_passati) & (df[\"Close\"] <= minimi_futuri)\n",
    "        df.loc[idx_minimi, \"MaxMinRel\"] = -periodo\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def __rinomina_colonne(df):\n",
    "        df = df.rename(columns={\n",
    "            'PSARaf_0.02_0.2': 'PSARaf',\n",
    "            'PSARr_0.02_0.2': 'PSARr',\n",
    "            'MACD_20_50_9': 'MACD',\n",
    "            'MACDh_20_50_9': 'MACDh',\n",
    "            'MACDs_20_50_9': 'MACDs',\n",
    "            'TSI_13_25_13': 'TSI',\n",
    "            'TSIs_13_25_13': 'TSIs',\n",
    "            'SUPERT_20_3.0': 'SUPERT',\n",
    "            'SUPERTd_20_3.0': 'SUPERTd',\n",
    "            'ADX_20': 'ADX',\n",
    "            'DMP_20': 'DMP',\n",
    "            'DMN_20': 'DMN',\n",
    "            'CMF_10': 'CMF',\n",
    "            'TRIX_18_9': 'TRIX',\n",
    "            'TRIXs_18_9': 'TRIXs',\n",
    "            'KVO_34_55_13': 'KVO',\n",
    "            'KVOs_34_55_13': 'KVOs',\n",
    "            'DCL_20_20': 'DCL',\n",
    "            'DCM_20_20': 'DCM',\n",
    "            'DCU_20_20': 'DCU',\n",
    "            'VTXP_20': 'VTXP',\n",
    "            'VTXM_20': 'VTXM',\n",
    "            'AROOND_20': 'AROOND',\n",
    "            'AROONU_20': 'AROONU',\n",
    "            'AROONOSC_20': 'AROONOSC',\n",
    "            'NVI_1': 'NVI',\n",
    "            'PVI_1': 'PVI',\n",
    "            'VHF_20': 'VHF',\n",
    "            'ATRr_14': 'ATR'\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    def __calcolo_drawdown_gain(df, periodo):\n",
    "        df[f\"Max_High_Futuro_{periodo}d\"] = df[\"High\"].shift(-periodo).rolling(periodo).max()\n",
    "        df[f\"Min_Low_Futuro_{periodo}d\"] = df[\"Low\"].shift(-periodo).rolling(periodo).min()\n",
    "        df[f\"Drawdown_{periodo}d\"] = df[\"Open\"] - df[f\"Min_Low_Futuro_{periodo}d\"]\n",
    "        df[f\"Drawdown_{periodo}d\"] = df[f\"Drawdown_{periodo}d\"].where(df[f\"Drawdown_{periodo}d\"] > 0, 0)\n",
    "        df[f\"Perc_Max_High_Futuro_{periodo}d\"] = ((df[f\"Max_High_Futuro_{periodo}d\"] - df[\"Open\"]) / df[\"Open\"]) * 100\n",
    "        df[f\"Perc_Drawdown_{periodo}d\"] = ((df[f\"Drawdown_{periodo}d\"]) / df[\"Open\"]) * 100 \n",
    "        df[f\"Perc_Drawdown_{periodo}d\"] = df[f\"Perc_Drawdown_{periodo}d\"].where(df[f\"Perc_Drawdown_{periodo}d\"] > 0, 0)\n",
    "        df.drop(columns=[f\"Max_High_Futuro_{periodo}d\", f\"Min_Low_Futuro_{periodo}d\", f\"Drawdown_{periodo}d\"], axis=1, inplace=True)\n",
    "        return df\n",
    "    \n",
    "    psar = ta.psar(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], af0=0.02, af=0.02, max_af=0.2)\n",
    "    psar[\"PSAR\"] = psar[\"PSARl_0.02_0.2\"].combine_first(psar[\"PSARs_0.02_0.2\"])\n",
    "    psar.drop([\"PSARl_0.02_0.2\", \"PSARs_0.02_0.2\"], axis=1, inplace=True)\n",
    "    macd = ta.macd(close=df[\"Close\"], fast=20, slow=50, signal=9)\n",
    "    tsi = ta.tsi(close=df[\"Close\"], fast=13, slow=25)\n",
    "    supertrend = ta.supertrend(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], length=20, multiplier=3)\n",
    "    supertrend.drop([\"SUPERTl_20_3.0\", \"SUPERTs_20_3.0\"], axis=1, inplace=True)\n",
    "    ema5 = ta.ema(close=df[\"Close\"], length=5)\n",
    "    ema20 = ta.ema(close=df[\"Close\"], length=20)\n",
    "    ema50 = ta.ema(close=df[\"Close\"], length=50)\n",
    "    ema100 = ta.ema(close=df[\"Close\"], length=100)\n",
    "    adx = ta.adx(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], length=20)\n",
    "    roc = ta.roc(close=df[\"Close\"], length=10)\n",
    "    cmf = ta.cmf(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], volume=df['Volume'], length=10)\n",
    "    trix = ta.trix(close=df['Close'], length=18)\n",
    "    klinger = ta.kvo(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], volume=df['Volume'], short=34, long=55)\n",
    "    vi = ta.vortex(high=df['High'], low=df['Low'], close=df['Close'], length=20)\n",
    "    aroon = ta.aroon(high=df['High'], low=df['Low'], close=df['Close'], length=20)\n",
    "    nvi = ta.nvi(close=df['Close'], volume=df['Volume'])\n",
    "    pvi = ta.pvi(close=df['Close'], volume=df['Volume'])\n",
    "    vhf = ta.vhf(close=df['Close'], length=20)\n",
    "    atr = ta.atr(high=df['High'], low=df['Low'], close=df['Close'])\n",
    "    obv = ta.obv(close=df[\"Close\"], volume=df[\"Volume\"])\n",
    "    #candele = ta.cdl_pattern(open_=df[\"Open\"], high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"])\n",
    "\n",
    "    df = pd.concat([df, ema5, ema20, ema50, ema100, psar, macd, tsi, supertrend, adx, trix, vi, aroon, nvi, pvi, atr, cmf, roc, klinger, vhf, obv], axis=1)\n",
    "\n",
    "    df = __rinomina_colonne(df)\n",
    "\n",
    "    df = __calcolo_drawdown_gain(df, 20)\n",
    "    df = __calcolo_drawdown_gain(df, 50)\n",
    "    df = __calcolo_drawdown_gain(df, 100)\n",
    "    df[\"max_gain\"] = df[[\"Perc_Max_High_Futuro_20d\", \"Perc_Max_High_Futuro_50d\", \"Perc_Max_High_Futuro_100d\"]].max(axis=1)\n",
    "    df[\"max_drawdown\"] = df[[\"Perc_Drawdown_20d\", \"Perc_Drawdown_50d\", \"Perc_Drawdown_100d\"]].min(axis=1)\n",
    "\n",
    "    df['EMA_20_5d'] = df['EMA_20'].shift(-5)\n",
    "    df['EMA_20_10d'] = df['EMA_20'].shift(-10)\n",
    "    df['EMA_20_15d'] = df['EMA_20'].shift(-15)\n",
    "    df['EMA_20_20d'] = df['EMA_20'].shift(-20)\n",
    "    df['EMA_50_5d'] = df['EMA_50'].shift(-5)\n",
    "    df['EMA_50_10d'] = df['EMA_50'].shift(-10)\n",
    "    df['EMA_50_15d'] = df['EMA_50'].shift(-15)\n",
    "    df['EMA_50_20d'] = df['EMA_50'].shift(-20)\n",
    "    df['Close_5d'] = df['Close'].shift(-5)\n",
    "    df['Close_10d'] = df['Close'].shift(-10)\n",
    "    df['Close_15d'] = df['Close'].shift(-15)\n",
    "    df['Close_20d'] = df['Close'].shift(-20)\n",
    "    df['EMA_5_5d'] = df['EMA_5'].shift(-5)\n",
    "    df['EMA_5_10d'] = df['EMA_5'].shift(-10)\n",
    "    df['EMA_5_15d'] = df['EMA_5'].shift(-15)\n",
    "    df['EMA_5_20d'] = df['EMA_5'].shift(-20)\n",
    "    #df['Close_1d'] = df['Close'].shift(-1)\n",
    "    #df['perc_EMA_5_20d'] = ((df['EMA_5_20d'] - df['EMA_5']) / df['EMA_5']) * 100\n",
    "    #df['perc_Close_20d'] = ((df['Close_20d'] - df['Close']) / df['Close']) * 100\n",
    "    #df['incrocio_verde_gialla'] = (ta.cross(df['EMA_20'], df['EMA_50'], above=True)).astype(\"int8\")\n",
    "    #df[\"incrocio_passato_verde_gialla_10d\"] = df[\"incrocio_verde_gialla\"].rolling(10).sum()\n",
    "    \n",
    "    #df['HLC3'] = ((df['High'] + df['Low'] + df['Close']) / 3)\n",
    "    df[\"DM_OSC\"] = (df[\"DMP\"] - df[\"DMN\"])\n",
    "    df[\"VTX_OSC\"] = (df[\"VTXP\"] - df[\"VTXM\"])\n",
    "    df[\"VI_OSC\"] = (df[\"PVI\"] - df[\"NVI\"])\n",
    "    \n",
    "    df.drop(columns=[\"DMP\", \"DMN\", \"VTXP\", \"VTXM\", \"PVI\", \"NVI\", \"AROOND\", \"AROONU\"], inplace=True, axis=1)\n",
    "    \n",
    "    df[\"MaxMinRel\"] = 0\n",
    "    df = __trova_massimi_minimi(df, 20)   \n",
    "    df = __trova_massimi_minimi(df, 50)   \n",
    "    df = __trova_massimi_minimi(df, 100)         \n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def _scarica(nome_simbolo, scarica_prima, scarica_dopo, data_inizio, data_fine, append):\n",
    "    try:\n",
    "        if append == True:\n",
    "            ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', \"ticker\")\n",
    "            ti_min = ticker.index.min()\n",
    "            ti_max = ticker.index.max()\n",
    "            if scarica_prima:\n",
    "                inizio = data_inizio - pd.Timedelta(days=365)\n",
    "                fine = ti_min - pd.Timedelta(days=1)\n",
    "                df_inizio = analizza_ticker(nome_simbolo, start=inizio, end=fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "                ticker = pd.concat([df_inizio, ticker], axis=0, ignore_index=False)\n",
    "            if scarica_dopo:\n",
    "                inizio = ti_max - pd.Timedelta(days=365) \n",
    "                fine = data_fine\n",
    "                df_fine = analizza_ticker(nome_simbolo, start=inizio, end=fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "                ticker = ticker[ticker.index < df_fine.index.min()]\n",
    "                ticker = pd.concat([ticker, df_fine], axis=0, ignore_index=False)\n",
    "        else:\n",
    "            ticker = analizza_ticker(nome_simbolo, start=data_inizio, end=data_fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "        return nome_simbolo, ticker, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_simbolo, None, str(e)\n",
    "\n",
    "def _callback_tickers(result, totale_processati, tot_tickers):\n",
    "    nome_simbolo, ticker, error = result\n",
    "    if error == \"\":\n",
    "        ticker.to_hdf(f'tickers/{nome_simbolo}.h5', key='ticker', mode='w')\n",
    "    else:\n",
    "        print(f\"Errore su funzione di callback per {nome_simbolo}: {error}\")\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "    print(f\"{totale_processati.value}/{tot_tickers}) Scaricato ticker {nome_simbolo}\")   \n",
    "    \n",
    "def _carica_screener(nome_simbolo, lista_scr, prob):\n",
    "    try:\n",
    "        df = pd.read_hdf(f'screeners/{nome_simbolo}.h5', 'screener')\n",
    "        df.index.set_names(['Data'], inplace=True)\n",
    "        df['Ticker'] = nome_simbolo\n",
    "        df.set_index('Ticker', append=True, inplace=True)\n",
    "        df = df[df['Previsione'] > prob]\n",
    "        lista_scr.append(df)\n",
    "        return nome_simbolo, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_simbolo, str(e)\n",
    "\n",
    "def _carica_screener_callback(result, totale_processati, tot_tickers):\n",
    "    nome_simbolo, error = result\n",
    "    if error != \"\":\n",
    "        print(f\"Errore su funzione di callback per {nome_simbolo}: {error}\")\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "    print(f\"{totale_processati.value}/{tot_tickers}) Caricato su screener ticker {nome_simbolo}\")   \n",
    "\n",
    "def to_XY(dati_ticker, timesteps, giorni_previsione, features, targets, bilanciamento=0):\n",
    "    features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in features]\n",
    "    features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in features]\n",
    "    features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in features]\n",
    "    features_no_scala = [ft for ft in _features_no_scala_tutte if ft in features]\n",
    "    features_candele = [ft for ft in _features_candele_tutte if ft in features]\n",
    "\n",
    "    scalers_prezzo = []\n",
    "    scaler_meno_piu = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_standard = MinMaxScaler()\n",
    "\n",
    "    new_dates = pd.bdate_range(start=dati_ticker.index[-1] + pd.Timedelta(days=1), periods=giorni_previsione)\n",
    "    df_new = pd.DataFrame(index=new_dates)\n",
    "    dati_ticker = pd.concat([dati_ticker, df_new])\n",
    "\n",
    "    ft_prezzo = dati_ticker[features_scala_prezzo]\n",
    "    ft_standard = dati_ticker[features_da_scalare_singolarmente]\n",
    "    ft_meno_piu = dati_ticker[features_oscillatori]\n",
    "    ft_no_scala = dati_ticker[features_no_scala]\n",
    "    ft_candele = dati_ticker[features_candele] \n",
    "\n",
    "    _targets = dati_ticker[targets]\n",
    "\n",
    "    i_tot = len(dati_ticker) - giorni_previsione\n",
    "\n",
    "    tot_elementi = i_tot - (timesteps-1)    \n",
    "    \n",
    "    X_prezzo = X_standard = X_meno_piu = X_no_scala = X_candele = None\n",
    "    if len(features_scala_prezzo) > 0:\n",
    "        tot_col_prezzo_x = len(ft_prezzo.columns)\n",
    "        X_prezzo = np.zeros((tot_elementi, timesteps, tot_col_prezzo_x))\n",
    "    if len(features_da_scalare_singolarmente) > 0:\n",
    "        tot_col_standard_x = len(ft_standard.columns)\n",
    "        X_standard = np.zeros((tot_elementi, timesteps, tot_col_standard_x))\n",
    "    if len(features_oscillatori) > 0:\n",
    "        tot_col_meno_piu_x = len(ft_meno_piu.columns)\n",
    "        X_meno_piu = np.zeros((tot_elementi, timesteps, tot_col_meno_piu_x))\n",
    "    if len(features_no_scala) > 0:\n",
    "        tot_col_no_scala_x = len(ft_no_scala.columns)\n",
    "        X_no_scala = np.zeros((tot_elementi, timesteps, tot_col_no_scala_x))\n",
    "    if len(features_candele) > 0:\n",
    "        tot_col_candele_x = len(ft_candele.columns)\n",
    "        X_candele = np.zeros((tot_elementi, timesteps, tot_col_candele_x))\n",
    "    if len(targets) > 0:\n",
    "        #tot_col_targets_y = len(targets.columns)\n",
    "        #Y = np.zeros((tot_elementi, giorni_previsione, tot_col_targets_y)) # togliere se classificazione binaria\n",
    "        Y = np.zeros(tot_elementi) #solo per classificazione binaria\n",
    "    \n",
    "    for i in range(timesteps - 1, i_tot):\n",
    "        if len(features_scala_prezzo) > 0:\n",
    "            arr_x = np.array(ft_prezzo.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_res = arr_x.reshape(-1, 1)\n",
    "            scaler_prezzo = MinMaxScaler()\n",
    "            scaler_prezzo.fit(arr_res)\n",
    "            arr_sc = scaler_prezzo.transform(arr_res).reshape(timesteps, tot_col_prezzo_x)\n",
    "            X_prezzo[i - (timesteps - 1)] = arr_sc\n",
    "            scalers_prezzo.append(scaler_prezzo)\n",
    "\n",
    "        if len(features_da_scalare_singolarmente) > 0:\n",
    "            arr_x = np.array(ft_standard.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_standard.fit_transform(arr_x)   \n",
    "            X_standard[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(features_oscillatori) > 0:\n",
    "            arr_x = np.array(ft_meno_piu.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_meno_piu.fit_transform(arr_x)   \n",
    "            X_meno_piu[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(features_no_scala) > 0:\n",
    "            arr_x = np.array(ft_no_scala.iloc[i - (timesteps - 1):i + 1])\n",
    "            X_no_scala[i - (timesteps - 1)] = arr_x\n",
    "\n",
    "        if len(features_candele) > 0:\n",
    "            arr_x = np.array(ft_candele.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_meno_piu.fit_transform(arr_x) \n",
    "            X_candele[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(targets) > 0:\n",
    "            # arr_y = np.array(targets.iloc[i + 1:i + 1 + giorni_previsione]) # togliere in caso di classificazione binaria\n",
    "            # arr_res = arr_y.reshape(-1, 1) # togliere in caso di classificazione binaria\n",
    "            # arr_sc = scaler_prezzo.transform(arr_res).reshape(giorni_previsione, tot_col_targets_y) # togliere in caso di classificazione binaria\n",
    "            # Y[i - (timesteps - 1)] = arr_sc  # togliere in caso di classificazione binaria\n",
    "            Y[i - (timesteps - 1)] = np.array(_targets.iloc[i]) #solo per classificazione binaria\n",
    "\n",
    "    X_list = [x for x in [X_prezzo, X_standard, X_meno_piu, X_no_scala, X_candele] if x is not None and x.size > 0]\n",
    "    X = np.concatenate(X_list, axis=2) if X_list else np.array([])\n",
    "    idx = dati_ticker.index[timesteps - 1:i_tot]\n",
    "\n",
    "    if bilanciamento > 0:\n",
    "        rus = RandomUnderSampler(sampling_strategy=bilanciamento)\n",
    "        dim1 = X.shape[1]\n",
    "        dim2 = X.shape[2]\n",
    "        X = X.reshape(-1, dim1 * dim2)\n",
    "        X, Y = rus.fit_resample(X, Y)\n",
    "        X = X.reshape(-1, dim1, dim2)\n",
    "\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    return idx, X, Y, scalers_prezzo\n",
    "\n",
    "def concatena(array_list, hdf5_file, dataset_name='my_dataset'):\n",
    "    \"\"\"\n",
    "    Concatena una lista di array NumPy a un dataset in un file HDF5, creando il file se non esiste.\n",
    "    \n",
    "    :param array_list: Lista di array NumPy da aggiungere.\n",
    "    :param hdf5_file: Percorso del file HDF5.\n",
    "    :param dataset_name: Nome del dataset all'interno del file HDF5.\n",
    "    \"\"\"\n",
    "    # Apri o crea il file HDF5\n",
    "    with h5py.File(hdf5_file, 'a') as h5f:  # 'a' apre il file in modalità read/write e lo crea se non esiste\n",
    "        # Controlla se il dataset esiste già\n",
    "        if dataset_name in h5f:\n",
    "            # Il dataset esiste, leggi la sua lunghezza e aggiungi gli array\n",
    "            dset = h5f[dataset_name]\n",
    "        else:\n",
    "            # Il dataset non esiste, quindi dobbiamo crearlo\n",
    "            # Usiamo la forma del primo array per definire la forma del dataset\n",
    "            initial_shape = (0,) + array_list[0].shape[1:]\n",
    "            maxshape = (None,) + array_list[0].shape[1:]\n",
    "            \n",
    "            # Crea il dataset con shape iniziale e maxshape\n",
    "            dset = h5f.create_dataset(dataset_name, shape=initial_shape, maxshape=maxshape, chunks=True)\n",
    "            \n",
    "        # Itera su tutti gli array nella lista\n",
    "        for array in array_list:\n",
    "            # Calcola la nuova lunghezza del dataset\n",
    "            new_len = dset.shape[0] + array.shape[0]\n",
    "            # Ridimensiona il dataset per accogliere i nuovi dati\n",
    "            dset.resize(new_len, axis=0)\n",
    "            # Aggiungi il nuovo array alla fine del dataset\n",
    "            dset[-array.shape[0]:] = array\n",
    "\n",
    "class Posizione:\n",
    "    def __init__(self, simbolo, ticker, data, n_azioni, bilancio, max_giorni_apertura, stop_loss, take_profit) -> None:\n",
    "        self.simbolo = simbolo\n",
    "        self.ticker = ticker\n",
    "        self.data = data\n",
    "        self.prezzo_unitario = ticker['Open'].iloc[0]\n",
    "        self.n_azioni = n_azioni\n",
    "        self.stop_loss = self.prezzo_unitario * (1 - stop_loss)\n",
    "        self.take_profit = self.prezzo_unitario * (1 + take_profit)\n",
    "        self.prezzo_tot = self.n_azioni * self.prezzo_unitario\n",
    "        self.bilancio = bilancio - self.prezzo_tot \n",
    "        self.giorni_apertura = 1\n",
    "        self.max_giorni_apertura = max_giorni_apertura\n",
    "        self.valore = self.prezzo_tot\n",
    "        self._colonne = ['Simbolo', 'Data', 'Tipo', 'Prezzo_unitario', 'n_azioni', 'Prezzo_tot', 'SL', 'TP', 'Bilancio', 'Valore_posizione', 'Giorni_apertura', 'Esito', 'Vincita', 'Perc']\n",
    "        \n",
    "    def to_df(self) -> pd.DataFrame:\n",
    "        return pd.DataFrame([\n",
    "            [self.simbolo, self.data.normalize(), 'COMPRA', self.prezzo_unitario.round(2), int(self.n_azioni), self.prezzo_tot.round(2), self.stop_loss.round(2), self.take_profit.round(2), self.bilancio.round(2), self.valore.round(2), int(self.giorni_apertura), '', 0, 0]\n",
    "        ], columns=self._colonne)\n",
    "\n",
    "    def chiudi(self, data, bilancio, forza_chiusura=False):\n",
    "        if (data in self.ticker.index) or (forza_chiusura):\n",
    "            if forza_chiusura:\n",
    "                dati_attuali = self.ticker.iloc[-1:]\n",
    "            else:\n",
    "                dati_attuali = self.ticker[self.ticker.index == data]\n",
    "            self.giorni_apertura += 1\n",
    "            prezzo_attuale = dati_attuali['Open'].iloc[0]\n",
    "            prezzo_tot = prezzo_attuale * self.n_azioni\n",
    "            self.valore = prezzo_tot\n",
    "            if (self.giorni_apertura > self.max_giorni_apertura) or (prezzo_attuale < self.stop_loss) or (prezzo_attuale > self.take_profit) or (forza_chiusura):\n",
    "                bilancio += prezzo_tot\n",
    "                if prezzo_attuale > self.prezzo_unitario:\n",
    "                    esito = 'VINCITA'\n",
    "                else:\n",
    "                    esito = 'PERDITA'\n",
    "                esito_tot = prezzo_tot - self.prezzo_tot\n",
    "                esito_pct = pct_change(self.prezzo_unitario, prezzo_attuale)\n",
    "                return pd.DataFrame([\n",
    "                    [self.simbolo, data.normalize(), 'VENDI', prezzo_attuale.round(2), int(self.n_azioni), prezzo_tot.round(2), self.stop_loss.round(2), self.take_profit.round(2), bilancio.round(2), self.valore.round(2), int(self.giorni_apertura), esito, esito_tot.round(2), int(esito_pct)]\n",
    "                ], columns=self._colonne)\n",
    "        return None\n",
    "\n",
    "class Borsa:\n",
    "    def __init__(self, n_simboli_contemporanei, bilancio_iniziale, probabilità_per_acquisto, stop_loss, take_profit, giorni_max_posizione, data_inizio=pd.Timestamp(year=2005, month=1, day=1), data_fine=pd.Timestamp.now().normalize()):\n",
    "        self.N_SIMBOLI = n_simboli_contemporanei\n",
    "        self.DATA_INIZIO = pd.to_datetime(data_inizio)\n",
    "        self.DATA_FINE = pd.to_datetime(data_fine)\n",
    "        self.BILANCIO_INIZIALE = bilancio_iniziale\n",
    "        self.PROBABILITA_PER_ACQUISTO = probabilità_per_acquisto\n",
    "        self.SL = stop_loss\n",
    "        self.TP = take_profit\n",
    "        self.GIORNI_POS = giorni_max_posizione\n",
    "        self._posizioni = []\n",
    "        self._valore_posizioni = 0\n",
    "        self._bilancio = None\n",
    "        self._data_corrente = None\n",
    "        self._bilancio_per_simbolo = None\n",
    "        self.esito_trading = None\n",
    "\n",
    "        self.modello_ingresso = Modello()\n",
    "        self.modello_ingresso.carica(progetto='mod_1_in')\n",
    "        \n",
    "        self.modello_uscita = Modello()\n",
    "        self.modello_uscita.carica(progetto='mod_1_out')\n",
    "\n",
    "        self.lista_tickers = pd.read_parquet(\"lista_ticker.parquet\")\n",
    "        self.tot_tickers = len(self.lista_tickers)\n",
    "        self.screener = pd.DataFrame()\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(f'_indice.json'):\n",
    "                with open(f'_indice.json', 'r') as jsonfile:\n",
    "                    indice = json.load(jsonfile)\n",
    "                prima_data = pd.to_datetime(indice['prima_data'])\n",
    "                ultima_data = pd.to_datetime(indice['ultima_data'])\n",
    "                \n",
    "                if (self.DATA_INIZIO < prima_data):\n",
    "                    scarica_prima = True\n",
    "                else:\n",
    "                    scarica_prima = False\n",
    "\n",
    "                if (self.DATA_FINE > ultima_data):\n",
    "                    scarica_dopo = True\n",
    "                else:\n",
    "                    scarica_dopo = False\n",
    "\n",
    "                if scarica_prima or scarica_dopo:\n",
    "                    self.scarica_tickers(scarica_prima, scarica_dopo, self.DATA_INIZIO, self.DATA_FINE, append=True)\n",
    "                    self.avvia_screener(append=True)    \n",
    "                    self.carica_screener()\n",
    "                    self.screener.to_pickle('screeners/_screener.pickle')\n",
    "                else:\n",
    "                    self.screener = pd.read_pickle('screeners/_screener.pickle')\n",
    "            else:\n",
    "                self.scarica_tickers(scarica_prima=True, scarica_dopo=True, data_inizio=self.DATA_INIZIO, data_fine=self.DATA_FINE, append=False)\n",
    "                self.avvia_screener(append=False)    \n",
    "                self.carica_screener()\n",
    "                self.screener.to_pickle('screeners/_screener.pickle')\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        \n",
    "    def carica_screener(self):\n",
    "        manager = Manager()\n",
    "        lista_scr = manager.list()\n",
    "        totale_processati = Value('i', 0)\n",
    "        with Pool(cpu_count()) as p:\n",
    "            callback_with_args = partial(_carica_screener_callback, totale_processati=totale_processati, tot_tickers=self.tot_tickers)\n",
    "            for i in range(0, self.tot_tickers):\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                param = (nome_simbolo, lista_scr, self.PROBABILITA_PER_ACQUISTO)\n",
    "                p.apply_async(_carica_screener, args=param, callback=callback_with_args)\n",
    "            p.close()\n",
    "            p.join()  \n",
    "        self.screener = pd.concat(lista_scr, axis=0, ignore_index=False)\n",
    "        self.screener = self.screener.sort_values(by=['Data', 'Previsione'], ascending=[True, False])\n",
    "\n",
    "    def scarica_tickers(self, scarica_prima=True, scarica_dopo=True, data_inizio=pd.Timestamp(year=2005, month=1, day=1), data_fine=pd.Timestamp.now().normalize(), append=False) -> None: \n",
    "        totale_processati = Value('i', 0)\n",
    "        with Pool(cpu_count()) as p:\n",
    "            callback_with_args = partial(_callback_tickers, totale_processati=totale_processati, tot_tickers=self.tot_tickers)\n",
    "            for i in range(0, self.tot_tickers):\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                param = (nome_simbolo, scarica_prima, scarica_dopo, data_inizio, data_fine, append)\n",
    "                p.apply_async(_scarica, args=param, callback=callback_with_args)\n",
    "            p.close()\n",
    "            p.join()     \n",
    "\n",
    "        indice = {\n",
    "            'prima_data': data_inizio.strftime('%Y-%m-%d'),\n",
    "            'ultima_data': data_fine.strftime('%Y-%m-%d')\n",
    "        }\n",
    "        with open(f'_indice.json', 'w') as jsonfile:\n",
    "            json.dump(indice, jsonfile, indent=4)    \n",
    "\n",
    "    def avvia_screener(self, append=False) -> None:\n",
    "        tot_tickers = len(self.lista_tickers)\n",
    "\n",
    "        for i in range(0, tot_tickers):\n",
    "            try:\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                print(\"\\033[42m\" + f'{i+1}/{tot_tickers}) Caricamento ticker {nome_simbolo}' + \"\\033[0m\")\n",
    "                ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "                if append:\n",
    "                    scr = pd.read_hdf(f'screeners/{nome_simbolo}.h5', 'screener')\n",
    "                    inizio = scr.index.max() - pd.Timedelta(days=365)\n",
    "                    ticker_analisi = ticker[ticker.index >= inizio]\n",
    "                    if scr.index.max() == ticker.index.max():\n",
    "                        scr = scr.drop(scr.index[-1])\n",
    "                    idx, X, Y, _ = self.modello_ingresso.to_XY(ticker_analisi, bilanciamento=0)\n",
    "                    print(f'Aggiornamento previsione {nome_simbolo}')\n",
    "                    pred = self.modello_ingresso.model.predict(X)\n",
    "                    scr_temp = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "                    scr_temp = scr_temp[scr_temp.index > scr.index.max()]\n",
    "                    scr = pd.concat([scr, scr_temp], axis=0, ignore_index=False)\n",
    "                    print(f\"Aggiornamento file {nome_simbolo}.h5\")\n",
    "                    scr.to_hdf(f'screeners/{nome_simbolo}.h5', key='screener', mode='w')\n",
    "                else:\n",
    "                    idx, X, Y, _ = self.modello_ingresso.to_XY(ticker, bilanciamento=0)\n",
    "                    print(f'Previsione {nome_simbolo}')\n",
    "                    pred = self.modello_ingresso.model.predict(X)\n",
    "                    scr = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "                    scr = scr[scr.index >= self.DATA_INIZIO]\n",
    "                    print(f\"Salvataggio file {nome_simbolo}.h5\")\n",
    "                    scr.to_hdf(f'screeners/{nome_simbolo}.h5', key='screener', mode='w')\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "\n",
    "    def ultimo_screener(self) -> (pd.Timestamp, pd.DataFrame):\n",
    "        # Ottieni l'ultimo valore dell'indice di livello 0\n",
    "        ultimo_indice = self.screener.index.get_level_values(0)[-1]\n",
    "        # Usa .loc per selezionare tutte le righe con quell'indice di livello 0\n",
    "        return ultimo_indice.date(), self.screener.loc[ultimo_indice]\n",
    "\n",
    "    def reset_trading(self) -> None:\n",
    "        self._data_corrente = self.DATA_INIZIO\n",
    "        self._posizioni = []\n",
    "        self._bilancio = self.BILANCIO_INIZIALE\n",
    "        self.esito_trading = pd.DataFrame()\n",
    "        self._bilancio_per_simbolo = self.BILANCIO_INIZIALE / self.N_SIMBOLI\n",
    "        self._valore_posizioni = 0\n",
    "\n",
    "    def avvia_trading(self) -> None:\n",
    "        self.reset_trading()\n",
    "        while self._data_corrente <= self.DATA_FINE:\n",
    "            print(\"\\r\\033[31m\" + f'Data: {self._data_corrente.date()}, Pos. aperte: {len(self._posizioni)}' + \"\\033[0m\", end=' ')\n",
    "            self._chiudi_posizioni()\n",
    "            if self._data_corrente in self.screener.index.get_level_values(0):\n",
    "                scr = self.screener.loc[(self._data_corrente, slice(None)), :]\n",
    "                i = 0\n",
    "                esito = True\n",
    "                while (len(self._posizioni) < self.N_SIMBOLI) and (i < len(scr)) and esito == True:\n",
    "                    simbolo = scr.index[i][1]\n",
    "                    esito = self._apri_posizione(simbolo)\n",
    "                    i += 1\n",
    "            self._data_corrente += timedelta(days=1)\n",
    "        \n",
    "        self._chiudi_posizioni(forza_chiusura=True)\n",
    "\n",
    "    def _chiudi_posizioni(self, forza_chiusura=False):\n",
    "        posizioni_da_mantenere = []\n",
    "        for pos in self._posizioni:\n",
    "            df = pos.chiudi(self._data_corrente, self._bilancio, forza_chiusura=forza_chiusura)\n",
    "            if df is not None or forza_chiusura:\n",
    "                self.esito_trading = pd.concat([self.esito_trading, df], axis=0, ignore_index=True)\n",
    "                df = df.iloc[0]\n",
    "                self._valore_posizioni -= df['Valore_posizione']\n",
    "                self._bilancio = df['Bilancio']\n",
    "                pos_da_aprire = (self.N_SIMBOLI - len(self._posizioni))\n",
    "                if pos_da_aprire == 0:\n",
    "                    self._bilancio_per_simbolo = 0.\n",
    "                else:\n",
    "                    self._bilancio_per_simbolo = self._bilancio / pos_da_aprire\n",
    "                print(f\"VENDI {df['Simbolo']} n.{df['n_azioni']} azioni a {df['Prezzo_unitario']} € = {df['Prezzo_tot']} €, Esito: {df['Esito']}, Perc: {df['Perc']}, Bilancio: {round(self._bilancio, 2)} € + valore pos. aperte: {round(self._valore_posizioni, 2)} € = {round(self._bilancio + self._valore_posizioni, 2)} €, Bilancio per simbolo: {round(self._bilancio_per_simbolo, 2)}\")\n",
    "            else:\n",
    "                posizioni_da_mantenere.append(pos)\n",
    "        self._posizioni = posizioni_da_mantenere\n",
    "\n",
    "    def _apri_posizione(self, simbolo):\n",
    "        try:\n",
    "            ticker = pd.read_hdf(f'tickers/{simbolo}.h5', 'ticker')\n",
    "            ticker = ticker[ticker.index >= self._data_corrente]\n",
    "            prezzo = ticker[\"Open\"].iloc[0]\n",
    "            n_azioni = self._bilancio_per_simbolo // prezzo\n",
    "            if (prezzo < self._bilancio_per_simbolo):\n",
    "                pos = Posizione(simbolo, ticker, self._data_corrente, n_azioni, self._bilancio, self.GIORNI_POS, self.SL, self.TP)\n",
    "                self._posizioni.append(pos)\n",
    "                self._valore_posizioni += pos.valore\n",
    "                self.esito_trading = pd.concat([self.esito_trading, pos.to_df()], axis=0, ignore_index=True)\n",
    "                self._bilancio = pos.bilancio\n",
    "                pos_da_aprire = (self.N_SIMBOLI - len(self._posizioni))\n",
    "                if pos_da_aprire == 0:\n",
    "                    self._bilancio_per_simbolo = 0.\n",
    "                else:\n",
    "                    self._bilancio_per_simbolo = self._bilancio / pos_da_aprire\n",
    "                print(f'COMPRA {simbolo} n.{int(pos.n_azioni)} azioni a {round(pos.prezzo_unitario, 2)} € = {round(pos.prezzo_tot, 2)} €,  Bilancio: {round(self._bilancio, 2)} € + valore pos. aperte: {round(self._valore_posizioni, 2)} € = {round(self._bilancio + self._valore_posizioni, 2)} €, Bilancio per simbolo: {round(self._bilancio_per_simbolo, 2)}')\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f'Errore in apertura posizione: {str(e)}')\n",
    "            return False\n",
    "\n",
    "class Modello:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _crea_modello(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        input_layer = Input(shape=(self.timesteps, self.n_features))\n",
    "\n",
    "        # Convolutional layer\n",
    "        conv1 = Conv1D(filters=128, kernel_size=5, activation='relu')(input_layer)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "\n",
    "        # LSTM layer\n",
    "        lstm1 = LSTM(100, return_sequences=True)(conv1)\n",
    "        lstm1 = Dropout(0.5)(lstm1)\n",
    "\n",
    "        # Attention layer\n",
    "        attention = Attention()([lstm1, lstm1])\n",
    "\n",
    "        # Continuation of the model\n",
    "        lstm2 = LSTM(50)(attention)\n",
    "        lstm2 = Dropout(0.5)(lstm2)\n",
    "\n",
    "        dense1 = Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.02))(lstm2)\n",
    "        dense1 = Dropout(0.5)(dense1)\n",
    "\n",
    "        dense2 = Dense(80, activation='relu', kernel_regularizer=regularizers.l2(0.02))(dense1)\n",
    "        dense2 = Dropout(0.5)(dense2)\n",
    "\n",
    "        batch_norm1 = BatchNormalization()(dense2)\n",
    "\n",
    "        dense3 = Dense(40, activation='relu', kernel_regularizer=regularizers.l2(0.02))(batch_norm1)\n",
    "        dense3 = Dropout(0.5)(dense3)\n",
    "\n",
    "        batch_norm2 = BatchNormalization()(dense3)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(1, activation='sigmoid')(batch_norm2)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def crea(self, \n",
    "             progetto='default', \n",
    "             timesteps=120, \n",
    "             giorni_previsione=1, \n",
    "             features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"], \n",
    "             targets=[\"Target_ingresso\"], \n",
    "             n_ticker_batch=400, \n",
    "             bilanciamento=1, \n",
    "             epochs=100,\n",
    "             batch_size=2052, \n",
    "             soglia=0.5, \n",
    "             class_weights={0: 3, 1: 1},\n",
    "             learning_rate=0.001, \n",
    "             train_test_split=0.2\n",
    "            ):\n",
    "        self.progetto = progetto\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.timesteps = timesteps # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "        self.giorni_previsione = giorni_previsione  # giorni futuri di cui effettuare la previsione\n",
    "        self.features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in self.features]\n",
    "        self.features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in self.features]\n",
    "        self.features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in self.features]\n",
    "        self.features_no_scala = [ft for ft in _features_no_scala_tutte if ft in self.features]\n",
    "        self.features_candele = [ft for ft in _features_candele_tutte if ft in self.features]\n",
    "        self.targets = targets\n",
    "        self.n_ticker_batch = n_ticker_batch\n",
    "        self.bilanciamento = bilanciamento\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.soglia = soglia\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_test_split = train_test_split\n",
    "        self.class_weights = class_weights\n",
    "        self.n_features = len(self.features) \n",
    "        self.n_targets = len(self.targets) \n",
    "        self.model = self._crea_modello() \n",
    "        self.model_history = None\n",
    "\n",
    "    def carica(self, progetto='default'):\n",
    "        self.progetto = progetto\n",
    "        percorso_file = f'{self.progetto}/impostazioni.json'\n",
    "        try:\n",
    "            with open(percorso_file, \"r\") as file:\n",
    "                impostazioni = json.load(file) if os.path.getsize(percorso_file) > 0 else {}\n",
    "                self.timesteps = impostazioni.get(\"timesteps\", 120) # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "                self.giorni_previsione = impostazioni.get(\"giorni_previsione\", 1)  # giorni futuri di cui effettuare la previsione\n",
    "                self.features = impostazioni.get(\"features\", [\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"])\n",
    "                self.features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in self.features]\n",
    "                self.features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in self.features]\n",
    "                self.features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in self.features]\n",
    "                self.features_no_scala = [ft for ft in _features_no_scala_tutte if ft in self.features]\n",
    "                self.features_candele = [ft for ft in _features_candele_tutte if ft in self.features]                \n",
    "                self.targets = impostazioni.get(\"targets\", [\"Target_ingresso\"])\n",
    "                self.n_features = len(self.features)\n",
    "                self.n_targets = len(self.targets) \n",
    "                self.n_ticker_batch = impostazioni.get(\"n_ticker_batch\", 400)\n",
    "                self.bilanciamento = impostazioni.get(\"bilanciamento\", 1)\n",
    "                self.batch_size = impostazioni.get(\"batch_size\", 2052)\n",
    "                self.epochs = impostazioni.get(\"epochs\", 100)\n",
    "                self.soglia = impostazioni.get(\"soglia\", 0.5)\n",
    "                self.learning_rate = impostazioni.get(\"learnig_rate\", 0.001)\n",
    "                self.train_test_split = impostazioni.get(\"train_test_split\", 0.2)\n",
    "                self.class_weights = impostazioni.get(\"class_weights\", {0: 3, 1: 1})\n",
    "                self.n_features = len(self.features) \n",
    "                self.n_targets = len(self.targets) \n",
    "                self.model = load_model(f\"{self.progetto}/model.h5\")  \n",
    "                self.model_history = pd.read_hdf(f'{progetto}/model_history.h5', 'history')      \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del file: {e}\")\n",
    "\n",
    "    def _salva_impostazioni(self):\n",
    "        impostazioni = {\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"giorni_previsione\": self.giorni_previsione,\n",
    "            \"features\": self.features,\n",
    "            \"targets\": self.targets,\n",
    "            \"n_ticker_batch\": self.n_ticker_batch,\n",
    "            \"bilanciamento\": self.bilanciamento,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs, \n",
    "            \"soglia\": self.soglia,\n",
    "            \"class_weights\": self.class_weights\n",
    "        }\n",
    "        with open(f'{self.progetto}/impostazioni.json', \"w\") as file:\n",
    "            json.dump(impostazioni, file, indent=4)\n",
    "            \n",
    "        modello = json.loads(self.model.to_json())\n",
    "        with open(f'{self.progetto}/struttura.json', \"w\") as file:\n",
    "            json.dump(modello, file, indent=4)\n",
    "\n",
    "    def salva(self):\n",
    "        os.makedirs(self.progetto, exist_ok=True)\n",
    "        self._salva_impostazioni()\n",
    "        self.model.save(f'{self.progetto}/model.h5')\n",
    "        self.model_history.to_hdf(f'{self.progetto}/model_history.h5', key='history', mode='w')\n",
    "\n",
    "    def genera_XY(self, lista_files, salva_carica_file=''):\n",
    "        perc_file = f'XY/XY_{salva_carica_file}.h5'\n",
    "        if not os.path.exists(perc_file):\n",
    "            manager = Manager()\n",
    "            listaX = manager.list()\n",
    "            listaY = manager.list()\n",
    "            totale_processati = Value('i', 1)  \n",
    "            tot_files = len(lista_files)\n",
    "            with Pool(cpu_count()) as p:\n",
    "                for file_name in lista_files:\n",
    "                    param = (file_name, self.timesteps, self.giorni_previsione, self.features, self.targets, self.bilanciamento)\n",
    "                    p.apply_async(_process_ticker, args=param, callback=lambda result: _callback_XY(result, listaX, listaY, totale_processati, tot_files, hdf5_file=perc_file))\n",
    "\n",
    "                p.close()\n",
    "                p.join()\n",
    "\n",
    "            if len(listaX) > 0:\n",
    "                print('Salvataggio finale su file X')\n",
    "                concatena(listaX, perc_file, dataset_name='X')\n",
    "                print('Salvataggio finale su file Y')\n",
    "                concatena(listaY, perc_file, dataset_name='Y')\n",
    "                del listaX[:]\n",
    "                del listaY[:]\n",
    "                \n",
    "        print('Caricamento file X e Y')\n",
    "        with h5py.File(f'XY/XY_{salva_carica_file}.h5', 'r') as hdf:\n",
    "            Xtot = hdf['X'][:]\n",
    "            Ytot = hdf['Y'][:]\n",
    "        return Xtot, Ytot \n",
    "\n",
    "    def addestra(self):\n",
    "        #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001, verbose=1)\n",
    "        #model_checkpoint = ModelCheckpoint(f'{self.progetto}/model.h5', monitor='val_precision', save_best_only=True, verbose=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        callbacks = [early_stopping]\n",
    "        list_of_files = os.listdir('tickers')\n",
    "        random.shuffle(list_of_files)\n",
    "        train_files = list_of_files[:self.n_ticker_batch]\n",
    "        val_files = list_of_files[self.n_ticker_batch:int(self.n_ticker_batch*(1+self.train_test_split))]\n",
    "        \n",
    "        print(\"\\033[41m\" + 'Preparazione dati di train' + \"\\033[0m\")\n",
    "        X_train, Y_train = self.genera_XY(train_files, 'train')\n",
    "        print(\"\\033[41m\" + 'Preparazione dati di validazione' + \"\\033[0m\")\n",
    "        X_val, Y_val = self.genera_XY(val_files, 'val')\n",
    "        \n",
    "        history = self.model.fit(X_train, Y_train, epochs=self.epochs, batch_size=self.batch_size, validation_data=(X_val, Y_val), callbacks=callbacks, class_weight=self.class_weights)\n",
    "        self.model_history = pd.DataFrame(history.history)\n",
    "        self.salva()\n",
    "        self.grafico_loss(salva_su_file=True)\n",
    "        self.grafico_precision(salva_su_file=True)\n",
    "        df = self.test()\n",
    "        return df\n",
    "        \n",
    "    def grafico_loss(self, salva_su_file=False):\n",
    "        num_epochs = self.model_history.shape[0]\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['loss'], label=\"Training\")\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['val_loss'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title('Loss')\n",
    "        plt.tight_layout()\n",
    "        if salva_su_file:\n",
    "            plt.savefig(f'{self.progetto}/grafico_loss.png')\n",
    "        plt.show()        \n",
    "\n",
    "    def grafico_precision(self, salva_su_file=False):\n",
    "        num_epochs = self.model_history.shape[0]\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['precision'], label=\"Training\")\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['val_precision'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title('Precision')\n",
    "        plt.tight_layout()\n",
    "        if salva_su_file:\n",
    "            plt.savefig(f'{self.progetto}/grafico_precision.png')\n",
    "        plt.show()        \n",
    "\n",
    "    def test(self, nome_simbolo='BTG'):\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "        ticker = dropna_iniziali(ticker)\n",
    "        ticker = dropna_finali(ticker)\n",
    "        idx, X, Y, _ = to_XY(ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "        print(f'X.shape: {X.shape}')\n",
    "        print(f'Y.shape: {Y.shape}')\n",
    "        print(f'ticker.shape: {ticker.shape}')\n",
    "        pred = self.model.predict(X, batch_size=self.batch_size, verbose=1, use_multiprocessing=True)\n",
    "        pred_binary = (pred > self.soglia).astype(int)\n",
    "        \n",
    "        result = self.model.evaluate(X, Y, batch_size=self.batch_size, verbose=1, use_multiprocessing=True, return_dict=True)\n",
    "        print(result)\n",
    "        \n",
    "        # Visualizza come heatmap\n",
    "        matrice = confusion_matrix(Y, pred_binary)\n",
    "        sns.heatmap(matrice, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Previsti')\n",
    "        plt.ylabel('Reali')\n",
    "        plt.savefig(f'{self.progetto}/confusion_matrix.png')\n",
    "        plt.show()\n",
    "        print(f'idx: {idx.shape}')\n",
    "        print(f'pred: {pred.shape}')\n",
    "        print(f'real: {Y.shape}')\n",
    "        df = pd.DataFrame([pred.reshape(-1,), Y.reshape(-1,)], columns=['Prev', 'Real'], index=idx)\n",
    "        return df       \n",
    "        \n",
    "def _process_ticker(file_name, timesteps, giorni_previsione, features, targets, bilanciamento):\n",
    "    try:\n",
    "        ticker = pd.read_hdf(f'tickers/{file_name}', 'ticker')\n",
    "        ticker = dropna_iniziali(ticker)\n",
    "        ticker = dropna_finali(ticker)\n",
    "        _, X, Y, _ = to_XY(ticker, timesteps, giorni_previsione, features, targets, bilanciamento)\n",
    "        return file_name, X, Y, \"\"\n",
    "    except Exception as e:\n",
    "        return file_name, np.array([]), np.array([]), str(e)\n",
    "\n",
    "def _callback_XY(result, listaX, listaY, totale_processati, tot_files, hdf5_file):\n",
    "    nome_simbolo, X, Y, err = result\n",
    "    if err == \"\":\n",
    "        if X.shape[0] > 0 and Y.shape[0] > 0:  # Verifica se X e Y sono non vuoti\n",
    "            print(f'X.shape:{X.shape}')\n",
    "            print(f'Y.shape:{Y.shape}')\n",
    "            listaX.append(X)\n",
    "            listaY.append(Y)\n",
    "            print(\"\\033[42m\" + f\"{totale_processati.value}/{tot_files}) Completato ticker {nome_simbolo}\" + \"\\033[0m\")\n",
    "        else:\n",
    "            print(\"\\033[43m\" + f\"Ticker {nome_simbolo} ignorato a causa di dati mancanti o errati.\" + \"\\033[0m\")\n",
    "    else:\n",
    "        print(err)\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "        if len(listaX) >= 100:\n",
    "            print('Salvataggio su file X')\n",
    "            concatena(listaX, hdf5_file, dataset_name='X')\n",
    "            print('Salvataggio su file Y')\n",
    "            concatena(listaY, hdf5_file, dataset_name='Y')\n",
    "            del listaX[:]\n",
    "            del listaY[:]\n",
    "     \n",
    "def modifica_target():\n",
    "    totale_processati = Value('i', 1)\n",
    "    list_of_files = os.listdir('tickers')\n",
    "    tot_tickers = len(list_of_files)\n",
    "    with Pool(cpu_count()) as p:\n",
    "        callback_with_args = partial(_callback_modifica_target, totale_processati=totale_processati, tot_tickers=tot_tickers)\n",
    "        for i in range(0, tot_tickers):\n",
    "            nome_file = list_of_files[i]\n",
    "            param = (nome_file,)\n",
    "            p.apply_async(_modifica_target, args=param, callback=callback_with_args)\n",
    "        p.close()\n",
    "        p.join()          \n",
    "   \n",
    "def _modifica_target(nome_file):\n",
    "    try:\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_file}', 'ticker')\n",
    "        if 'Target_ingresso' in ticker.columns:\n",
    "            ticker.drop(['Target_ingresso'], axis=1, inplace=True)\n",
    "        if 'Target_uscita' in ticker.columns:\n",
    "            ticker.drop(['Target_uscita'], axis=1, inplace=True)\n",
    "        ticker = imposta_target(ticker)\n",
    "        return nome_file, ticker, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_file, ticker, str(e)\n",
    "\n",
    "def _callback_modifica_target(result, totale_processati, tot_tickers):\n",
    "    nome_file, ticker, err = result\n",
    "    if err == \"\":\n",
    "        print(f\"{totale_processati.value}/{tot_tickers}) Modificato target {nome_file}\")\n",
    "        ticker.to_hdf(f'tickers/{nome_file}', key='ticker', mode='w')\n",
    "    else:\n",
    "        print(err)\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "        \n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, directory_path, list_of_files, batch_size, timesteps, giorni_previsione, features, targets, bilanciamento):\n",
    "        self.directory_path = directory_path\n",
    "        self.batch_size = batch_size\n",
    "        self.list_of_files = list_of_files\n",
    "        self.indexes = np.arange(len(self.list_of_files))\n",
    "        self.timesteps = timesteps\n",
    "        self.giorni_previsione = giorni_previsione\n",
    "        self.bilanciamento = bilanciamento\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.list_of_files) / self.batch_size))\n",
    "\n",
    "    def __getitem__2(self, index):\n",
    "        batch_files = self.list_of_files[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        manager = Manager()\n",
    "        listaX = manager.list()\n",
    "        listaY = manager.list()\n",
    "        totale_processati = Value('i', 0)  \n",
    "    \n",
    "        with Pool(cpu_count()) as p:\n",
    "            for file_name in batch_files:\n",
    "                param = (file_name, self.timesteps, self.giorni_previsione, self.features, self.targets, self.bilanciamento)\n",
    "                p.apply_async(_process_ticker, args=param, callback=lambda result: _callback_result(result, listaX, listaY, totale_processati, self.batch_size))\n",
    "\n",
    "            p.close()\n",
    "            p.join()\n",
    "\n",
    "        print('Concatenamento')\n",
    "        return np.concatenate(listaX, axis=0), np.concatenate(listaY, axis=0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = (index + 1) * self.batch_size\n",
    "        batch_files = self.list_of_files[start_idx:end_idx]\n",
    "\n",
    "        listaX = []\n",
    "        listaY = []\n",
    "        i = 1\n",
    "        for file_name in batch_files:\n",
    "            try:\n",
    "                print(\"\\033[42m\" + f'{i}) Preparazione {file_name}' + \"\\033[0m\")\n",
    "                ticker = pd.read_hdf(f'tickers/{file_name}', 'ticker')\n",
    "                _, X, Y, _ = to_XY(ticker, self.timesteps, self.giorni_previsione, self.features, self.targets, self.bilanciamento)\n",
    "                listaX.append(X)\n",
    "                listaY.append(Y)\n",
    "                print(f'X.shape:{X.shape}')\n",
    "                print(f'Y.shape:{Y.shape}')\n",
    "            except Exception as e:\n",
    "                print(f\"Errore durante il processamento del ticker {file_name}: {e}\")\n",
    "            i += 1\n",
    "\n",
    "        print('Concatenamento')\n",
    "        return np.concatenate(listaX, axis=0), np.concatenate(listaY, axis=0)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e1218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "TP = 1\n",
    "SL = 0.01\n",
    "PROBABILITA_PER_ACQUISTO = 0.5\n",
    "BILANCIO_INIZIALE = 1000\n",
    "GIORNI_MAX_POSIZIONE = 20\n",
    "N_SIMBOLI = 10\n",
    "\n",
    "inizializza_gpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "borsa = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, PROBABILITA_PER_ACQUISTO, SL, TP, GIORNI_MAX_POSIZIONE)\n",
    "data, scr = borsa.ultimo_screener()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(data)\n",
    "scr.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "borsa.avvia_trading()\n",
    "borsa.esito_trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mPreparazione dati di train\u001b[0m\n",
      "Caricamento file X e Y\n",
      "\u001b[41mPreparazione dati di validazione\u001b[0m\n",
      "Caricamento file X e Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 15:55:25.562478: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 4566981120 exceeds 10% of free system memory.\n",
      "2023-11-27 15:55:38.797714: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.25GiB (rounded to 4566981120)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-11-27 15:55:38.797963: W tensorflow/core/common_runtime/bfc_allocator.cc:491] _____________**************************************______________________________________________*_*\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\enric\\Documents\\GitHub\\Borsa\\Test_trading_multiplo.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mod \u001b[39m=\u001b[39m Modello()\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mod\u001b[39m.\u001b[39mcrea(\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmod_1_in\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     class_weights\u001b[39m=\u001b[39m{\u001b[39m0\u001b[39m: \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m1\u001b[39m}, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     features\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mEMA_5\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEMA_20\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEMA_50\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mEMA_100\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mVolume\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMACDh\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAROONOSC\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTRIX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDM_OSC\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTSI\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mKVO\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mPSAR\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSUPERT\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df \u001b[39m=\u001b[39m mod\u001b[39m.\u001b[39;49maddestra()\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m df\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m mod\u001b[39m.\u001b[39msalva()\n",
      "\u001b[1;32mc:\\Users\\enric\\Documents\\GitHub\\Borsa\\Test_trading_multiplo.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m   <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=1158'>1159</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[41m\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPreparazione dati di validazione\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=1159'>1160</a>\u001b[0m X_val, Y_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenera_XY(val_files, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=1161'>1162</a>\u001b[0m history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(X_train, Y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, validation_data\u001b[39m=\u001b[39;49m(X_val, Y_val), callbacks\u001b[39m=\u001b[39;49mcallbacks, class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weights)\n\u001b[1;32m   <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=1162'>1163</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_history \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(history\u001b[39m.\u001b[39mhistory)\n\u001b[1;32m   <a href='vscode-notebook-cell:/c%3A/Users/enric/Documents/GitHub/Borsa/Test_trading_multiplo.ipynb#W5sZmlsZQ%3D%3D?line=1163'>1164</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msalva()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "mod = Modello()\n",
    "mod.crea(\n",
    "    'mod_1_in', \n",
    "    class_weights={0: 2, 1: 1}, \n",
    "    bilanciamento=1, \n",
    "    timesteps=240, \n",
    "    batch_size=1500, \n",
    "    learning_rate=0.01, \n",
    "    n_ticker_batch=400, \n",
    "    train_test_split=0.2,\n",
    "    features=[\"EMA_5\", \"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\", \"PSAR\", \"SUPERT\"]\n",
    ")\n",
    "df = mod.addestra()\n",
    "df\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modifica_target()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = pd.read_hdf('tickers/BTG.h5', 'ticker')\n",
    "fig = grafico(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
