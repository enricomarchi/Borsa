{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85a9176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from multiprocessing import Pool, cpu_count, Manager, Value\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Sequential, Model\n",
    "from tensorflow.keras.layers import Bidirectional, BatchNormalization, LSTM, Dropout, Dense, Conv1D, Flatten, GRU, Attention, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from datetime import timedelta\n",
    "import json\n",
    "import os\n",
    "from functools import partial\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas_ta as ta\n",
    "import random\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.subplots as sp\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "_features_scala_prezzo_tutte = [\n",
    "    \"Close\",\n",
    "    \"EMA_5\", \n",
    "    \"EMA_20\", \n",
    "    \"EMA_50\",\n",
    "    \"EMA_100\",\n",
    "    \"Open\",  \n",
    "    \"High\",\n",
    "    \"Low\",\n",
    "    \"PSAR\",\n",
    "    \"SUPERT\"\n",
    "]\n",
    "\n",
    "_features_da_scalare_singolarmente_tutte = [\n",
    "    \"Volume\",\n",
    "    \"ATR\",\n",
    "    \"PSARaf\",\n",
    "    \"ADX\",\n",
    "    \"OBV\"\n",
    "]\n",
    "\n",
    "_features_oscillatori_tutte = [\n",
    "    \"MACDh\",    \n",
    "    \"MACD\",\n",
    "    \"MACDs\",\n",
    "    \"AROONOSC\",\n",
    "    \"TRIX\",\n",
    "    \"TRIXs\",\n",
    "    \"DM_OSC\",\n",
    "    \"TSI\",\n",
    "    \"TSIs\",\n",
    "    \"ROC_10\",\n",
    "    \"KVO\",\n",
    "    \"KVOs\",\n",
    "    \"VI_OSC\"\n",
    "]\n",
    "\n",
    "_features_no_scala_tutte = [\n",
    "    \"SUPERTd\",  \n",
    "    \"PSARr\",\n",
    "    \"CMF\",\n",
    "    \"VHF\",\n",
    "    \"VTX_OSC\"\n",
    "]\n",
    "\n",
    "_features_candele_tutte = [\n",
    "    \"CDL_2CROWS\", \"CDL_3BLACKCROWS\", \"CDL_3INSIDE\", \"CDL_3LINESTRIKE\", \"CDL_3OUTSIDE\", \"CDL_3STARSINSOUTH\", \"CDL_3WHITESOLDIERS\", \"CDL_ABANDONEDBABY\", \"CDL_ADVANCEBLOCK\", \"CDL_BELTHOLD\", \"CDL_BREAKAWAY\", \"CDL_CLOSINGMARUBOZU\", \"CDL_CONCEALBABYSWALL\", \"CDL_COUNTERATTACK\", \"CDL_DARKCLOUDCOVER\", \"CDL_DOJI_10_0.1\", \"CDL_DOJISTAR\", \"CDL_DRAGONFLYDOJI\", \"CDL_ENGULFING\", \"CDL_EVENINGDOJISTAR\", \"CDL_EVENINGSTAR\", \"CDL_GAPSIDESIDEWHITE\", \"CDL_GRAVESTONEDOJI\", \"CDL_HAMMER\", \"CDL_HANGINGMAN\", \"CDL_HARAMI\", \"CDL_HARAMICROSS\", \"CDL_HIGHWAVE\", \"CDL_HIKKAKE\", \"CDL_HIKKAKEMOD\", \"CDL_HOMINGPIGEON\", \"CDL_IDENTICAL3CROWS\", \"CDL_INNECK\", \"CDL_INSIDE\", \"CDL_INVERTEDHAMMER\", \"CDL_KICKING\", \"CDL_KICKINGBYLENGTH\", \"CDL_LADDERBOTTOM\", \"CDL_LONGLEGGEDDOJI\", \"CDL_LONGLINE\", \"CDL_MARUBOZU\", \"CDL_MATCHINGLOW\", \"CDL_MATHOLD\", \"CDL_MORNINGDOJISTAR\", \"CDL_MORNINGSTAR\", \"CDL_ONNECK\", \"CDL_PIERCING\", \"CDL_RICKSHAWMAN\", \"CDL_RISEFALL3METHODS\", \"CDL_SEPARATINGLINES\", \"CDL_SHOOTINGSTAR\", \"CDL_SHORTLINE\", \"CDL_SPINNINGTOP\", \"CDL_STALLEDPATTERN\", \"CDL_STICKSANDWICH\", \"CDL_TAKURI\", \"CDL_TASUKIGAP\", \"CDL_THRUSTING\", \"CDL_TRISTAR\", \"CDL_UNIQUE3RIVER\", \"CDL_UPSIDEGAP2CROWS\", \"CDL_XSIDEGAP3METHODS\",\n",
    "]\n",
    "\n",
    "def inizializza_gpu():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"nessuna GPU\")\n",
    "    \n",
    "def pct_change(valore_iniziale, valore_finale):\n",
    "    try:\n",
    "        return ((valore_finale - valore_iniziale) / valore_iniziale) * 100\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def analizza_ticker(nome_simbolo, start, end, progress=True, dropna_iniziali=False, dropna_finali=False):\n",
    "    start_str = start.strftime('%Y-%m-%d')\n",
    "    end_str = end.strftime('%Y-%m-%d')\n",
    "    df = yf.download(nome_simbolo, start=start_str, end=end_str, progress=progress)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = crea_indicatori(df)\n",
    "    if dropna_iniziali:\n",
    "        idx = df[df.notna().all(axis=1) == True].index[0]\n",
    "        df = df[idx:]\n",
    "    if dropna_finali:\n",
    "        idx = df[df.notna().all(axis=1) == True].index[-1]\n",
    "        df = df[:idx]\n",
    "    return df\n",
    "\n",
    "def dropna_iniziali(df):\n",
    "    idx = df[df.notna().all(axis=1) == True].index[0]\n",
    "    df = df[idx:]\n",
    "    return df\n",
    "\n",
    "def dropna_finali(df):\n",
    "    idx = df[df.notna().all(axis=1) == True].index[-1]\n",
    "    df = df[:idx]\n",
    "    return df\n",
    "\n",
    "def imposta_target(df):\n",
    "    def __calcolo_drawdown_gain(df, periodo):\n",
    "        df[f\"Max_High_Futuro_{periodo}d\"] = df[\"High\"].shift(-periodo).rolling(periodo).max()\n",
    "        df[f\"Min_Low_Futuro_{periodo}d\"] = df[\"Low\"].shift(-periodo).rolling(periodo).min()\n",
    "        df[f\"Drawdown_{periodo}d\"] = df[\"Open\"] - df[f\"Min_Low_Futuro_{periodo}d\"]\n",
    "        df[f\"Drawdown_{periodo}d\"] = df[f\"Drawdown_{periodo}d\"].where(df[f\"Drawdown_{periodo}d\"] > 0, 0)\n",
    "        df[f\"Perc_Max_High_Futuro_{periodo}d\"] = ((df[f\"Max_High_Futuro_{periodo}d\"] - df[\"Open\"]) / df[\"Open\"]) * 100\n",
    "        df[f\"Perc_Drawdown_{periodo}d\"] = ((df[f\"Drawdown_{periodo}d\"]) / df[\"Open\"]) * 100 \n",
    "        df[f\"Perc_Drawdown_{periodo}d\"] = df[f\"Perc_Drawdown_{periodo}d\"].where(df[f\"Perc_Drawdown_{periodo}d\"] > 0, 0)\n",
    "        df.drop(columns=[f\"Max_High_Futuro_{periodo}d\", f\"Min_Low_Futuro_{periodo}d\", f\"Drawdown_{periodo}d\"], axis=1, inplace=True)\n",
    "        return df\n",
    "    def __trova_massimi_minimi(df, periodo):\n",
    "        mezzo_periodo = periodo // 2\n",
    "\n",
    "        massimi_passati = df['Close'].shift(1).rolling(mezzo_periodo).max()\n",
    "        massimi_futuri = df['Close'][::-1].shift(1).rolling(mezzo_periodo).max()[::-1]\n",
    "        idx_massimi = (df[\"Close\"] >= massimi_passati) & (df[\"Close\"] >= massimi_futuri)\n",
    "        df.loc[idx_massimi, \"MaxMinRel\"] = periodo\n",
    "\n",
    "        minimi_passati = df['Close'].shift(1).rolling(mezzo_periodo).min()\n",
    "        minimi_futuri = df['Close'][::-1].shift(1).rolling(mezzo_periodo).min()[::-1]\n",
    "        idx_minimi = (df[\"Close\"] <= minimi_passati) & (df[\"Close\"] <= minimi_futuri)\n",
    "        df.loc[idx_minimi, \"MaxMinRel\"] = -periodo\n",
    "            \n",
    "        return df\n",
    "    df = __calcolo_drawdown_gain(df, 5)\n",
    "    # df = __calcolo_drawdown_gain(df, 50)\n",
    "    # df = __calcolo_drawdown_gain(df, 100)\n",
    "    # df[\"max_gain\"] = df[[\"Perc_Max_High_Futuro_20d\", \"Perc_Max_High_Futuro_50d\", \"Perc_Max_High_Futuro_100d\"]].max(axis=1)\n",
    "    # df[\"max_drawdown\"] = df[[\"Perc_Drawdown_20d\", \"Perc_Drawdown_50d\", \"Perc_Drawdown_100d\"]].min(axis=1)\n",
    "\n",
    "    df['EMA_20_5d'] = df['EMA_20'].shift(-5)\n",
    "    df['EMA_20_10d'] = df['EMA_20'].shift(-10)\n",
    "    df['EMA_20_15d'] = df['EMA_20'].shift(-15)\n",
    "    df['EMA_20_20d'] = df['EMA_20'].shift(-20)\n",
    "    \n",
    "    df['EMA_50_5d'] = df['EMA_50'].shift(-5)\n",
    "    df['EMA_50_10d'] = df['EMA_50'].shift(-10)\n",
    "    df['EMA_50_15d'] = df['EMA_50'].shift(-15)\n",
    "    df['EMA_50_20d'] = df['EMA_50'].shift(-20)\n",
    "    \n",
    "    df['Close_5d'] = df['Close'].shift(-5)\n",
    "    df['Close_10d'] = df['Close'].shift(-10)\n",
    "    df['Close_15d'] = df['Close'].shift(-15)\n",
    "    df['Close_20d'] = df['Close'].shift(-20)\n",
    "    \n",
    "    df['EMA_5_5d'] = df['EMA_5'].shift(-5)\n",
    "    df['EMA_5_10d'] = df['EMA_5'].shift(-10)\n",
    "    df['EMA_5_15d'] = df['EMA_5'].shift(-15)\n",
    "    df['EMA_5_20d'] = df['EMA_5'].shift(-20)\n",
    "    #df['Close_1d'] = df['Close'].shift(-1)\n",
    "    #df['perc_EMA_5_20d'] = ((df['EMA_5_20d'] - df['EMA_5']) / df['EMA_5']) * 100\n",
    "    #df['perc_Close_20d'] = ((df['Close_20d'] - df['Close']) / df['Close']) * 100\n",
    "    #df['incrocio_verde_gialla'] = (ta.cross(df['EMA_20'], df['EMA_50'], above=True)).astype(\"int8\")\n",
    "    #df[\"incrocio_passato_verde_gialla_10d\"] = df[\"incrocio_verde_gialla\"].rolling(10).sum()\n",
    "    df['Max_Close_20d'] = df['Close'].shift(-20).rolling(window=20, min_periods=1).max()\n",
    "    df['pct_change_20d'] = df.apply(lambda row: pct_change(row['Close'], row['Max_Close_20d']), axis=1)\n",
    "    df.drop(columns=[\"Max_Close_20d\"], inplace=True, axis=1)\n",
    "\n",
    "    # df[\"MaxMinRel\"] = 0\n",
    "    # df = __trova_massimi_minimi(df, 20)   \n",
    "    # df = __trova_massimi_minimi(df, 50)   \n",
    "    # df = __trova_massimi_minimi(df, 100)         \n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    df['Target_ingresso'] = (\n",
    "        (df['pct_change_20d'] > 20)\n",
    "    )\n",
    "    df['Target_uscita'] = (\n",
    "        (df['EMA_5'] > df['EMA_20']) & (df['EMA_5_5d'] < df['EMA_20_5d']) & (df['EMA_5_10d'] < df['EMA_20_10d'])\n",
    "    )    \n",
    "    \n",
    "    df['Target_mod_2_in'] = (\n",
    "        (df[f\"Perc_Max_High_Futuro_5d\"] > 5)\n",
    "    )\n",
    "    return df\n",
    "    \n",
    "def grafico(df):\n",
    "    close = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['Close'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, .9)'),\n",
    "        name = 'Close'\n",
    "    )\n",
    "\n",
    "    close2 = go.Scatter( # serve solo per il fill del supertrend\n",
    "        x = df.index,\n",
    "        y = df['Close'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, 0)'),\n",
    "        showlegend=False,\n",
    "        name = 'Close2'\n",
    "    )\n",
    "\n",
    "    # min5 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -5].index,\n",
    "    #     y = df[df['MaxMinRel'] == -5]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 5,\n",
    "    #         color = 'rgba(255, 0, 0, .9)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel5'\n",
    "    # )\n",
    "  \n",
    "    # max5 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 5].index,\n",
    "    #     y = df[df['MaxMinRel'] == 5]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 5,\n",
    "    #         color = 'rgba(50, 205, 50, .9)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel5'\n",
    "    # )\n",
    "\n",
    "    # min10 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -10].index,\n",
    "    #     y = df[df['MaxMinRel'] == -10]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 15,\n",
    "    #         color = 'rgba(255, 0, 0, .4)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel10'\n",
    "    # )\n",
    "  \n",
    "    # max10 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 10].index,\n",
    "    #     y = df[df['MaxMinRel'] == 10]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 15,\n",
    "    #         color = 'rgba(50, 205, 50, .4)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel10'\n",
    "    # )\n",
    "\n",
    "    # min20 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -20].index,\n",
    "    #     y = df[df['MaxMinRel'] == -20]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 25,\n",
    "    #         color = 'rgba(255, 0, 0, .2)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel20'\n",
    "    # )\n",
    "  \n",
    "    # max20 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 20].index,\n",
    "    #     y = df[df['MaxMinRel'] == 20]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 25,\n",
    "    #         color = 'rgba(50, 205, 50, .2)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel20'\n",
    "    # )\n",
    "\n",
    "    # min60 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == -60].index,\n",
    "    #     y = df[df['MaxMinRel'] == -60]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 35,\n",
    "    #         color = 'rgba(255, 0, 0, .1)'\n",
    "    #     ),\n",
    "    #     name = 'MinRel60'\n",
    "    # )\n",
    "  \n",
    "    # max60 = go.Scatter(\n",
    "    #     x = df[df['MaxMinRel'] == 60].index,\n",
    "    #     y = df[df['MaxMinRel'] == 60]['Close'],\n",
    "    #     mode = 'markers',\n",
    "    #     marker = dict(\n",
    "    #         size = 35,\n",
    "    #         color = 'rgba(50, 205, 50, .1)'\n",
    "    #     ),\n",
    "    #     name = 'MaxRel60'\n",
    "    # )\n",
    "\n",
    "    ema5 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_5'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='blue'),\n",
    "        name = 'EMA5'\n",
    "    )\n",
    "\n",
    "    ema20 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_20'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='limegreen'),\n",
    "        name = 'EMA20'\n",
    "    )\n",
    "\n",
    "    ema50 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_50'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='orange'),\n",
    "        name = 'EMA50'\n",
    "    )\n",
    "    \n",
    "    ema100 = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['EMA_100'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='red'),\n",
    "        name = 'EMA100'\n",
    "    )\n",
    "    \n",
    "    psar = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = df['PSAR'], \n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 2, \n",
    "            color = 'rgba(0, 0, 0, .8)',  \n",
    "        ),\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SAR Parabolico'\n",
    "    )\n",
    "    \n",
    "    stup = np.where(df['SUPERTd'] == 1, df['SUPERT'], np.nan)\n",
    "    stdown = np.where(df['SUPERTd'] == -1, df['SUPERT'], np.nan)\n",
    "    stupfill = np.where(df['SUPERTd'] == 1, df['SUPERT'], df['Close'])\n",
    "    stdownfill = np.where(df['SUPERTd'] == -1, df['SUPERT'], df['Close'])\n",
    "    \n",
    "    strendup = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stup, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='limegreen',\n",
    "            width=1\n",
    "        ),\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        connectgaps = False,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "        \n",
    "    strendupfill = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stupfill, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0,0,0,0)',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        fill='tonexty',   \n",
    "        fillcolor='rgba(50, 205, 50, 0.1)', \n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "        \n",
    "    strenddown = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stdown, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='red',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "    \n",
    "    strenddownfill = go.Scatter(\n",
    "        x = df.index,\n",
    "        y = stdownfill, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0,0,0,0)',\n",
    "            width=1\n",
    "        ),\n",
    "        fill='tonexty',   \n",
    "        fillcolor='rgba(255, 0, 0, 0.1)', \n",
    "        connectgaps = False,\n",
    "        visible = False,\n",
    "        showlegend=True,\n",
    "        name = 'SuperTrend'\n",
    "    )\n",
    "    \n",
    "    target_in = go.Scatter(\n",
    "        x = df[df['Target_ingresso'] == 1].index,\n",
    "        y = df[df['Target_ingresso'] == 1]['Close'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(0, 200, 0, .9)'\n",
    "        ),\n",
    "        name = 'target_in'\n",
    "    )\n",
    "    \n",
    "    target_out = go.Scatter(\n",
    "        x = df[df['Target_uscita'] == 1].index,\n",
    "        y = df[df['Target_uscita'] == 1]['Close'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 10,\n",
    "            color = 'rgba(220, 0, 0, .9)'\n",
    "        ),\n",
    "        name = 'target_out'\n",
    "    )\n",
    "        \n",
    "    layout = dict(xaxis = dict(autorange=True),\n",
    "                  yaxis = dict(title = 'Close', autorange=True),\n",
    "                  autosize = True,\n",
    "                  margin = go.layout.Margin(\n",
    "                      l=0,  # Sinistra\n",
    "                      r=0,  # Destra\n",
    "                      b=0,  # Basso\n",
    "                      t=50,  # Alto\n",
    "                      pad=0  # Padding\n",
    "                  ),\n",
    "                  legend = dict(traceorder = 'normal', bordercolor = 'black')\n",
    "    )\n",
    "        \n",
    "    fig = sp.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    fig.update_layout(layout)\n",
    "    \n",
    "    # RIGA 1\n",
    "\n",
    "    fig.add_trace(close, row=1, col=1)\n",
    "    fig.add_trace(strendupfill, row=1, col=1)\n",
    "    fig.add_trace(strendup, row=1, col=1)\n",
    "    fig.add_trace(close2, row=1, col=1)\n",
    "    fig.add_trace(strenddownfill, row=1, col=1)\n",
    "    fig.add_trace(strenddown, row=1, col=1)\n",
    "    #fig.add_trace(min5, row=1, col=1); fig.add_trace(max5, row=1, col=1)\n",
    "    #fig.add_trace(min10, row=1, col=1); fig.add_trace(max10, row=1, col=1)\n",
    "    #fig.add_trace(min20, row=1, col=1); fig.add_trace(max20, row=1, col=1)\n",
    "    #fig.add_trace(min60, row=1, col=1); fig.add_trace(max60, row=1, col=1)\n",
    "    fig.add_trace(target_in, row=1, col=1); fig.add_trace(target_out, row=1, col=1)\n",
    "    fig.add_trace(ema5, row=1, col=1); fig.add_trace(ema20, row=1, col=1); fig.add_trace(ema50, row=1, col=1); fig.add_trace(ema100, row=1, col=1)\n",
    "    fig.add_trace(psar, row=1, col=1)\n",
    "    \n",
    "    pyo.plot(fig, filename=\"grafico_target.html\", auto_open=True)\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "def crea_indicatori(df):\n",
    "    def __rinomina_colonne(df):\n",
    "        df = df.rename(columns={\n",
    "            'PSARaf_0.02_0.2': 'PSARaf',\n",
    "            'PSARr_0.02_0.2': 'PSARr',\n",
    "            'MACD_20_50_9': 'MACD',\n",
    "            'MACDh_20_50_9': 'MACDh',\n",
    "            'MACDs_20_50_9': 'MACDs',\n",
    "            'TSI_13_25_13': 'TSI',\n",
    "            'TSIs_13_25_13': 'TSIs',\n",
    "            'SUPERT_20_3.0': 'SUPERT',\n",
    "            'SUPERTd_20_3.0': 'SUPERTd',\n",
    "            'ADX_20': 'ADX',\n",
    "            'DMP_20': 'DMP',\n",
    "            'DMN_20': 'DMN',\n",
    "            'CMF_10': 'CMF',\n",
    "            'TRIX_18_9': 'TRIX',\n",
    "            'TRIXs_18_9': 'TRIXs',\n",
    "            'KVO_34_55_13': 'KVO',\n",
    "            'KVOs_34_55_13': 'KVOs',\n",
    "            'DCL_20_20': 'DCL',\n",
    "            'DCM_20_20': 'DCM',\n",
    "            'DCU_20_20': 'DCU',\n",
    "            'VTXP_20': 'VTXP',\n",
    "            'VTXM_20': 'VTXM',\n",
    "            'AROOND_20': 'AROOND',\n",
    "            'AROONU_20': 'AROONU',\n",
    "            'AROONOSC_20': 'AROONOSC',\n",
    "            'NVI_1': 'NVI',\n",
    "            'PVI_1': 'PVI',\n",
    "            'VHF_20': 'VHF',\n",
    "            'ATRr_14': 'ATR'\n",
    "        })\n",
    "        return df\n",
    "    \n",
    "    psar = ta.psar(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], af0=0.02, af=0.02, max_af=0.2)\n",
    "    psar[\"PSAR\"] = psar[\"PSARl_0.02_0.2\"].combine_first(psar[\"PSARs_0.02_0.2\"])\n",
    "    psar.drop([\"PSARl_0.02_0.2\", \"PSARs_0.02_0.2\"], axis=1, inplace=True)\n",
    "    macd = ta.macd(close=df[\"Close\"], fast=20, slow=50, signal=9)\n",
    "    tsi = ta.tsi(close=df[\"Close\"], fast=13, slow=25)\n",
    "    supertrend = ta.supertrend(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], length=20, multiplier=3)\n",
    "    supertrend.drop([\"SUPERTl_20_3.0\", \"SUPERTs_20_3.0\"], axis=1, inplace=True)\n",
    "    ema5 = ta.ema(close=df[\"Close\"], length=5)\n",
    "    ema20 = ta.ema(close=df[\"Close\"], length=20)\n",
    "    ema50 = ta.ema(close=df[\"Close\"], length=50)\n",
    "    ema100 = ta.ema(close=df[\"Close\"], length=100)\n",
    "    adx = ta.adx(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], length=20)\n",
    "    roc = ta.roc(close=df[\"Close\"], length=10)\n",
    "    cmf = ta.cmf(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], volume=df['Volume'], length=10)\n",
    "    trix = ta.trix(close=df['Close'], length=18)\n",
    "    klinger = ta.kvo(high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"], volume=df['Volume'], short=34, long=55)\n",
    "    vi = ta.vortex(high=df['High'], low=df['Low'], close=df['Close'], length=20)\n",
    "    aroon = ta.aroon(high=df['High'], low=df['Low'], close=df['Close'], length=20)\n",
    "    nvi = ta.nvi(close=df['Close'], volume=df['Volume'])\n",
    "    pvi = ta.pvi(close=df['Close'], volume=df['Volume'])\n",
    "    vhf = ta.vhf(close=df['Close'], length=20)\n",
    "    atr = ta.atr(high=df['High'], low=df['Low'], close=df['Close'])\n",
    "    obv = ta.obv(close=df[\"Close\"], volume=df[\"Volume\"])\n",
    "    #candele = ta.cdl_pattern(open_=df[\"Open\"], high=df[\"High\"], low=df[\"Low\"], close=df[\"Close\"])\n",
    "\n",
    "    df = pd.concat([df, ema5, ema20, ema50, ema100, psar, macd, tsi, supertrend, adx, trix, vi, aroon, nvi, pvi, atr, cmf, roc, klinger, vhf, obv], axis=1)\n",
    "\n",
    "    df = __rinomina_colonne(df)\n",
    "    \n",
    "    #df['HLC3'] = ((df['High'] + df['Low'] + df['Close']) / 3)\n",
    "    df[\"DM_OSC\"] = (df[\"DMP\"] - df[\"DMN\"])\n",
    "    df[\"VTX_OSC\"] = (df[\"VTXP\"] - df[\"VTXM\"])\n",
    "    df[\"VI_OSC\"] = (df[\"PVI\"] - df[\"NVI\"])\n",
    "    \n",
    "    df.drop(columns=[\"DMP\", \"DMN\", \"VTXP\", \"VTXM\", \"PVI\", \"NVI\", \"AROOND\", \"AROONU\"], inplace=True, axis=1)\n",
    "    \n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    return df\n",
    "\n",
    "def _scarica(nome_simbolo, scarica_prima, scarica_dopo, data_inizio, data_fine, append):\n",
    "    try:\n",
    "        if append == True:\n",
    "            ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', \"ticker\")\n",
    "            ti_min = ticker.index.min()\n",
    "            ti_max = ticker.index.max()\n",
    "            if scarica_prima:\n",
    "                inizio = data_inizio - pd.Timedelta(days=365)\n",
    "                fine = ti_min - pd.Timedelta(days=1)\n",
    "                df_inizio = analizza_ticker(nome_simbolo, start=inizio, end=fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "                ticker = pd.concat([df_inizio, ticker], axis=0, ignore_index=False)\n",
    "            if scarica_dopo:\n",
    "                inizio = ti_max - pd.Timedelta(days=365) \n",
    "                fine = data_fine\n",
    "                df_fine = analizza_ticker(nome_simbolo, start=inizio, end=fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "                ticker = ticker[ticker.index < df_fine.index.min()]\n",
    "                ticker = pd.concat([ticker, df_fine], axis=0, ignore_index=False)\n",
    "        else:\n",
    "            ticker = analizza_ticker(nome_simbolo, start=data_inizio, end=data_fine, progress=False, dropna_iniziali=True, dropna_finali=False)\n",
    "        return nome_simbolo, ticker, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_simbolo, None, str(e)\n",
    "\n",
    "def _callback_tickers(result, totale_processati, tot_tickers):\n",
    "    nome_simbolo, ticker, error = result\n",
    "    if error == \"\":\n",
    "        ticker.to_hdf(f'tickers/{nome_simbolo}.h5', key='ticker', mode='w')\n",
    "    else:\n",
    "        print(f\"Errore su funzione di callback per {nome_simbolo}: {error}\")\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "    print(f\"{totale_processati.value}/{tot_tickers}) Scaricato ticker {nome_simbolo}\")   \n",
    "    \n",
    "def _carica_screener(nome_simbolo, lista_scr, prob, percorso_e_nome_file):\n",
    "    try:\n",
    "        df = pd.read_hdf(percorso_e_nome_file, 'screener')\n",
    "        df.index.set_names(['Data'], inplace=True)\n",
    "        df['Ticker'] = nome_simbolo\n",
    "        df.set_index('Ticker', append=True, inplace=True)\n",
    "        df = df.loc[df['Previsione'] > prob]\n",
    "        lista_scr.append(df)\n",
    "        return nome_simbolo, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_simbolo, str(e)\n",
    "\n",
    "def _carica_screener_callback(result, totale_processati, tot_tickers):\n",
    "    nome_simbolo, error = result\n",
    "    if error != \"\":\n",
    "        print(f\"Errore su funzione di callback per {nome_simbolo}: {error}\")\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "    print(f\"{totale_processati.value}/{tot_tickers}) Caricato su screener ticker {nome_simbolo}\")   \n",
    "\n",
    "def to_XY(dati_ticker, timesteps, giorni_previsione, features, targets, bilanciamento=0):\n",
    "    dati_ticker = imposta_target(dati_ticker)\n",
    "    features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in features]\n",
    "    features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in features]\n",
    "    features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in features]\n",
    "    features_no_scala = [ft for ft in _features_no_scala_tutte if ft in features]\n",
    "    features_candele = [ft for ft in _features_candele_tutte if ft in features]\n",
    "\n",
    "    scalers_prezzo = []\n",
    "    scaler_meno_piu = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler_standard = MinMaxScaler()\n",
    "\n",
    "    new_dates = pd.bdate_range(start=dati_ticker.index[-1] + pd.Timedelta(days=1), periods=giorni_previsione)\n",
    "    df_new = pd.DataFrame(index=new_dates)\n",
    "    dati_ticker = pd.concat([dati_ticker, df_new])\n",
    "\n",
    "    ft_prezzo = dati_ticker[features_scala_prezzo]\n",
    "    ft_standard = dati_ticker[features_da_scalare_singolarmente]\n",
    "    ft_meno_piu = dati_ticker[features_oscillatori]\n",
    "    ft_no_scala = dati_ticker[features_no_scala]\n",
    "    ft_candele = dati_ticker[features_candele] \n",
    "\n",
    "    _targets = dati_ticker[targets]\n",
    "\n",
    "    i_tot = len(dati_ticker) - giorni_previsione\n",
    "\n",
    "    tot_elementi = i_tot - (timesteps-1)    \n",
    "    \n",
    "    X_prezzo = X_standard = X_meno_piu = X_no_scala = X_candele = None\n",
    "    if len(features_scala_prezzo) > 0:\n",
    "        tot_col_prezzo_x = len(ft_prezzo.columns)\n",
    "        X_prezzo = np.zeros((tot_elementi, timesteps, tot_col_prezzo_x))\n",
    "    if len(features_da_scalare_singolarmente) > 0:\n",
    "        tot_col_standard_x = len(ft_standard.columns)\n",
    "        X_standard = np.zeros((tot_elementi, timesteps, tot_col_standard_x))\n",
    "    if len(features_oscillatori) > 0:\n",
    "        tot_col_meno_piu_x = len(ft_meno_piu.columns)\n",
    "        X_meno_piu = np.zeros((tot_elementi, timesteps, tot_col_meno_piu_x))\n",
    "    if len(features_no_scala) > 0:\n",
    "        tot_col_no_scala_x = len(ft_no_scala.columns)\n",
    "        X_no_scala = np.zeros((tot_elementi, timesteps, tot_col_no_scala_x))\n",
    "    if len(features_candele) > 0:\n",
    "        tot_col_candele_x = len(ft_candele.columns)\n",
    "        X_candele = np.zeros((tot_elementi, timesteps, tot_col_candele_x))\n",
    "    if len(targets) > 0:\n",
    "        #tot_col_targets_y = len(targets.columns)\n",
    "        #Y = np.zeros((tot_elementi, giorni_previsione, tot_col_targets_y)) # togliere se classificazione binaria\n",
    "        Y = np.zeros(tot_elementi) #solo per classificazione binaria\n",
    "    \n",
    "    for i in range(timesteps - 1, i_tot):\n",
    "        if len(features_scala_prezzo) > 0:\n",
    "            arr_x = np.array(ft_prezzo.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_res = arr_x.reshape(-1, 1)\n",
    "            scaler_prezzo = MinMaxScaler()\n",
    "            scaler_prezzo.fit(arr_res)\n",
    "            arr_sc = scaler_prezzo.transform(arr_res).reshape(timesteps, tot_col_prezzo_x)\n",
    "            X_prezzo[i - (timesteps - 1)] = arr_sc\n",
    "            scalers_prezzo.append(scaler_prezzo)\n",
    "\n",
    "        if len(features_da_scalare_singolarmente) > 0:\n",
    "            arr_x = np.array(ft_standard.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_standard.fit_transform(arr_x)   \n",
    "            X_standard[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(features_oscillatori) > 0:\n",
    "            arr_x = np.array(ft_meno_piu.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_meno_piu.fit_transform(arr_x)   \n",
    "            X_meno_piu[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(features_no_scala) > 0:\n",
    "            arr_x = np.array(ft_no_scala.iloc[i - (timesteps - 1):i + 1])\n",
    "            X_no_scala[i - (timesteps - 1)] = arr_x\n",
    "\n",
    "        if len(features_candele) > 0:\n",
    "            arr_x = np.array(ft_candele.iloc[i - (timesteps - 1):i + 1])\n",
    "            arr_sc = scaler_meno_piu.fit_transform(arr_x) \n",
    "            X_candele[i - (timesteps - 1)] = arr_sc\n",
    "\n",
    "        if len(targets) > 0:\n",
    "            # arr_y = np.array(targets.iloc[i + 1:i + 1 + giorni_previsione]) # togliere in caso di classificazione binaria\n",
    "            # arr_res = arr_y.reshape(-1, 1) # togliere in caso di classificazione binaria\n",
    "            # arr_sc = scaler_prezzo.transform(arr_res).reshape(giorni_previsione, tot_col_targets_y) # togliere in caso di classificazione binaria\n",
    "            # Y[i - (timesteps - 1)] = arr_sc  # togliere in caso di classificazione binaria\n",
    "            Y[i - (timesteps - 1)] = np.array(_targets.iloc[i]) #solo per classificazione binaria\n",
    "\n",
    "    X_list = [x for x in [X_prezzo, X_standard, X_meno_piu, X_no_scala, X_candele] if x is not None and x.size > 0]\n",
    "    X = np.concatenate(X_list, axis=2) if X_list else np.array([])\n",
    "    idx = dati_ticker.index[timesteps - 1:i_tot]\n",
    "\n",
    "    if bilanciamento > 0:\n",
    "        #rus = RandomUnderSampler(sampling_strategy=bilanciamento)\n",
    "        smote = SMOTE(sampling_strategy=bilanciamento)\n",
    "        dim1 = X.shape[1]\n",
    "        dim2 = X.shape[2]\n",
    "        X_flat = X.reshape(-1, dim1 * dim2)\n",
    "        # Applica l'undersampling\n",
    "        X_flat_resampled, Y = smote.fit_resample(X_flat, Y)\n",
    "\n",
    "        # Ridimensiona X tornando alla forma originale\n",
    "        X = X_flat_resampled.reshape(-1, dim1, dim2)\n",
    "\n",
    "        # Ottieni gli indici originali dopo l'undersampling\n",
    "        #idx_resampled = rus.sample_indices_\n",
    "\n",
    "        # Usa idx_resampled per ottenere gli indici originali\n",
    "        #idx = idx[idx_resampled]\n",
    "\n",
    "    Y = Y.reshape(-1, 1)\n",
    "    return idx, X, Y, scalers_prezzo\n",
    "\n",
    "def concatena(array_list, hdf5_file, dataset_name):\n",
    "    \"\"\"\n",
    "    Concatena una lista di array NumPy a un dataset in un file HDF5, creando il file se non esiste.\n",
    "    \n",
    "    :param array_list: Lista di array NumPy da aggiungere.\n",
    "    :param hdf5_file: Percorso del file HDF5.\n",
    "    :param dataset_name: Nome del dataset all'interno del file HDF5.\n",
    "    \"\"\"\n",
    "    # Apri o crea il file HDF5\n",
    "    with h5py.File(hdf5_file, 'a') as h5f:  # 'a' apre il file in modalità read/write e lo crea se non esiste\n",
    "        # Controlla se il dataset esiste già\n",
    "        if dataset_name in h5f:\n",
    "            # Il dataset esiste, leggi la sua lunghezza e aggiungi gli array\n",
    "            dset = h5f[dataset_name]\n",
    "        else:\n",
    "            # Il dataset non esiste, quindi dobbiamo crearlo\n",
    "            # Usiamo la forma del primo array per definire la forma del dataset\n",
    "            initial_shape = (0,) + array_list[0].shape[1:]\n",
    "            maxshape = (None,) + array_list[0].shape[1:]\n",
    "            \n",
    "            # Crea il dataset con shape iniziale e maxshape\n",
    "            dset = h5f.create_dataset(dataset_name, shape=initial_shape, maxshape=maxshape, chunks=True)\n",
    "            \n",
    "        # Itera su tutti gli array nella lista\n",
    "        for array in array_list:\n",
    "            # Calcola la nuova lunghezza del dataset\n",
    "            new_len = dset.shape[0] + array.shape[0]\n",
    "            # Ridimensiona il dataset per accogliere i nuovi dati\n",
    "            dset.resize(new_len, axis=0)\n",
    "            # Aggiungi il nuovo array alla fine del dataset\n",
    "            dset[-array.shape[0]:] = array\n",
    "\n",
    "def crea_cartella(percorso_e_nome_cartella):\n",
    "    if not os.path.exists(percorso_e_nome_cartella):\n",
    "        os.makedirs(percorso_e_nome_cartella)\n",
    "\n",
    "class Borsa:\n",
    "    def __init__(self, n_simboli_contemporanei=10, bilancio_iniziale=1000, probabilità_per_acquisto=0.5, giorni_max_posizione=20, stop_loss=None, take_profit=None, data_inizio=pd.Timestamp(year=2005, month=1, day=1), data_fine=pd.Timestamp.now().normalize(), nome_modello='mod_1_in'):\n",
    "        self.N_SIMBOLI = n_simboli_contemporanei\n",
    "        self.DATA_INIZIO = pd.to_datetime(data_inizio)\n",
    "        self.DATA_FINE = pd.to_datetime(data_fine)\n",
    "        self.BILANCIO_INIZIALE = bilancio_iniziale\n",
    "        self.PROBABILITA_PER_ACQUISTO = probabilità_per_acquisto\n",
    "        self.SL = stop_loss\n",
    "        self.TP = take_profit\n",
    "        self.GIORNI_POS = giorni_max_posizione\n",
    "\n",
    "        self.nome_modello = nome_modello\n",
    "        self.modello_ingresso = Modello()\n",
    "        self.modello_ingresso.carica(progetto=self.nome_modello)\n",
    "        self.timesteps = self.modello_ingresso.timesteps\n",
    "        self.giorni_previsione = self.modello_ingresso.giorni_previsione\n",
    "        self.features = self.modello_ingresso.features\n",
    "        self.targets = self.modello_ingresso.targets\n",
    "        \n",
    "        self.lista_tickers = pd.read_parquet(\"lista_ticker.parquet\")\n",
    "        self.tot_tickers = len(self.lista_tickers)\n",
    "        self.screener = pd.DataFrame()\n",
    "\n",
    "    def _verifica_date_aggiornamento(self, nome_file='_indice.json'):\n",
    "        if os.path.exists(nome_file):\n",
    "            file_esistente = True\n",
    "            with open(nome_file, 'r') as jsonfile:\n",
    "                indice = json.load(jsonfile)\n",
    "            prima_data = pd.to_datetime(indice['prima_data'])\n",
    "            ultima_data = pd.to_datetime(indice['ultima_data'])\n",
    "            \n",
    "            if (self.DATA_INIZIO < prima_data):\n",
    "                scarica_prima = True\n",
    "            else:\n",
    "                scarica_prima = False\n",
    "\n",
    "            if (self.DATA_FINE > ultima_data):\n",
    "                scarica_dopo = True\n",
    "            else:\n",
    "                scarica_dopo = False\n",
    "        else:\n",
    "            file_esistente=False\n",
    "            scarica_prima = True\n",
    "            scarica_dopo = True\n",
    "        return file_esistente, scarica_prima, scarica_dopo\n",
    "\n",
    "    def aggiorna_dati(self):\n",
    "        try:\n",
    "            file_esistente, scarica_prima, scarica_dopo = self._verifica_date_aggiornamento('_indice.json')\n",
    "            if file_esistente:\n",
    "                if scarica_prima or scarica_dopo:\n",
    "                    print('Caricamento nuovi dati ticker')\n",
    "                    self.scarica_tickers(scarica_prima, scarica_dopo, append=True)\n",
    "                    print('Aggiornamento lista tickers')  \n",
    "                    self.aggiorna_lista_tickers()      \n",
    "            else:\n",
    "                print('Scarico totale dati ticker')\n",
    "                self.scarica_tickers(scarica_prima=True, scarica_dopo=True, append=False) \n",
    "                print('Aggiornamento lista tickers')  \n",
    "                self.aggiorna_lista_tickers()       \n",
    "                \n",
    "            file_esistente, scarica_prima, scarica_dopo = self._verifica_date_aggiornamento(f'{self.nome_modello}/_indice_screeners.json')\n",
    "            if file_esistente:\n",
    "                if scarica_prima or scarica_dopo:\n",
    "                    print('Aggiornamento screener')\n",
    "                    self.avvia_screener(append=True)    \n",
    "                else:\n",
    "                    print('Caricamento screener esistenti')\n",
    "                    self.screener = pd.read_hdf(f'{self.nome_modello}/screeners/_screener.h5', 'screener')\n",
    "            else:\n",
    "                print('Creazione nuovi screener')\n",
    "                self.avvia_screener(append=False)                 \n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "        \n",
    "    def scarica_tickers(self, scarica_prima=True, scarica_dopo=True, append=False) -> None: \n",
    "        totale_processati = Value('i', 0)\n",
    "        data_inizio = self.DATA_INIZIO\n",
    "        data_fine = self.DATA_FINE\n",
    "        with Pool(cpu_count()) as p:\n",
    "            callback_with_args = partial(_callback_tickers, totale_processati=totale_processati, tot_tickers=self.tot_tickers)\n",
    "            for i in range(0, self.tot_tickers):\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                param = (nome_simbolo, scarica_prima, scarica_dopo, data_inizio, data_fine, append)\n",
    "                p.apply_async(_scarica, args=param, callback=callback_with_args)\n",
    "            p.close()\n",
    "            p.join()     \n",
    "\n",
    "        indice = {\n",
    "            'prima_data': data_inizio.strftime('%Y-%m-%d'),\n",
    "            'ultima_data': data_fine.strftime('%Y-%m-%d')\n",
    "        }\n",
    "        with open(f'_indice.json', 'w') as jsonfile:\n",
    "            json.dump(indice, jsonfile, indent=4)    \n",
    "\n",
    "    def aggiorna_lista_tickers(self):\n",
    "        cartella_tickers = 'tickers'\n",
    "        cartella_screeners = f'{self.nome_modello}/screeners'\n",
    "        files_tickers = os.listdir(cartella_tickers)\n",
    "\n",
    "        tickers_da_rimuovere = []\n",
    "\n",
    "        for file in files_tickers:\n",
    "            percorso_file = os.path.join(cartella_tickers, file)\n",
    "\n",
    "            # Controlla il numero di righe nel file HDF5\n",
    "            try:\n",
    "                df_temp = pd.read_hdf(percorso_file)\n",
    "                if len(df_temp) < 100:\n",
    "                    # Elimina il file da tickers\n",
    "                    os.remove(percorso_file)\n",
    "\n",
    "                    # Elimina il corrispondente file in screeners\n",
    "                    percorso_file_screener = os.path.join(cartella_screeners, file)\n",
    "                    if os.path.exists(percorso_file_screener):\n",
    "                        os.remove(percorso_file_screener)\n",
    "\n",
    "                    # Aggiungi il nome del ticker alla lista dei tickers da rimuovere\n",
    "                    nome_ticker = os.path.splitext(file)[0]\n",
    "                    tickers_da_rimuovere.append(nome_ticker)\n",
    "            except Exception as e:\n",
    "                print(f\"Errore nella lettura del file {file}: {e}\")\n",
    "\n",
    "        # Rimuovi i tickers dalla lista\n",
    "        self.lista_tickers = self.lista_tickers[~self.lista_tickers['Ticker'].isin(tickers_da_rimuovere)]\n",
    "\n",
    "        # Aggiorna la lista dei tickers nel DataFrame\n",
    "        lista_files_aggiornata = os.listdir(cartella_tickers)\n",
    "        lista_files_aggiornata = [os.path.splitext(file)[0] for file in lista_files_aggiornata]\n",
    "        self.lista_tickers = self.lista_tickers[self.lista_tickers['Ticker'].isin(lista_files_aggiornata)]\n",
    "\n",
    "        # Salva il DataFrame aggiornato\n",
    "        self.lista_tickers.to_parquet('lista_ticker.parquet')\n",
    "       \n",
    "    def avvia_screener(self, append=False, inizia_da=0) -> None:\n",
    "        tot_tickers = len(self.lista_tickers)\n",
    "        crea_cartella(f'{self.nome_modello}/screeners')\n",
    "        data_inizio = self.DATA_INIZIO\n",
    "        data_fine = self.DATA_FINE\n",
    "        \n",
    "        for i in range(inizia_da, tot_tickers):\n",
    "            try:\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                print(\"\\033[42m\" + f'{i+1}/{tot_tickers}) Calcolo screeners per {nome_simbolo}' + \"\\033[0m\")\n",
    "                ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "                if append:\n",
    "                    scr = pd.read_hdf(f'{self.nome_modello}/screeners/{nome_simbolo}.h5', 'screener')\n",
    "                    inizio = scr.index.max() - pd.Timedelta(days=365)\n",
    "                    ticker_analisi = ticker.loc[ticker.index >= inizio].copy()\n",
    "                    if scr.index.max() == ticker.index.max():\n",
    "                        scr = scr.drop(scr.index[-1]).copy()\n",
    "                    idx, X, Y, _ = to_XY(dati_ticker=ticker_analisi, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "                    print(f'Aggiornamento previsione {nome_simbolo}')\n",
    "                    pred = self.modello_ingresso.model.predict(X)\n",
    "                    scr_temp = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "                    scr_temp = scr_temp.loc[scr_temp.index > scr.index.max()].copy()\n",
    "                    scr = pd.concat([scr, scr_temp], axis=0, ignore_index=False)\n",
    "                    print(f\"Aggiornamento file di screener {nome_simbolo}.h5\")\n",
    "                    scr.to_hdf(f'{self.nome_modello}/screeners/{nome_simbolo}.h5', key='screener', mode='w')\n",
    "                else:\n",
    "                    idx, X, Y, _ = to_XY(dati_ticker=ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "                    print(f'Previsione {nome_simbolo}')\n",
    "                    pred = self.modello_ingresso.model.predict(X)\n",
    "                    scr = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "                    scr = scr.loc[scr.index >= self.DATA_INIZIO]\n",
    "                    print(f\"Salvataggio file di screener {nome_simbolo}.h5\")\n",
    "                    scr.to_hdf(f'{self.nome_modello}/screeners/{nome_simbolo}.h5', key='screener', mode='w')\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "        indice = {\n",
    "            'prima_data': data_inizio.strftime('%Y-%m-%d'),\n",
    "            'ultima_data': data_fine.strftime('%Y-%m-%d')\n",
    "        }\n",
    "        with open(f'{self.nome_modello}/_indice_screeners.json', 'w') as jsonfile:\n",
    "            json.dump(indice, jsonfile, indent=4)    \n",
    "        self.carica_screener()\n",
    "        self.screener.to_hdf(f'{self.nome_modello}/screeners/_screener.h5', key='screener', mode='w')\n",
    "\n",
    "    def carica_screener(self):\n",
    "        manager = Manager()\n",
    "        lista_scr = manager.list()\n",
    "        totale_processati = Value('i', 0)\n",
    "        with Pool(cpu_count()) as p:\n",
    "            callback_with_args = partial(_carica_screener_callback, totale_processati=totale_processati, tot_tickers=self.tot_tickers)\n",
    "            for i in range(0, self.tot_tickers):\n",
    "                nome_simbolo = self.lista_tickers.iloc[i][\"Ticker\"]\n",
    "                param = (nome_simbolo, lista_scr, self.PROBABILITA_PER_ACQUISTO, f'{self.nome_modello}/screeners/{nome_simbolo}.h5')\n",
    "                p.apply_async(_carica_screener, args=param, callback=callback_with_args)\n",
    "            p.close()\n",
    "            p.join()  \n",
    "        self.screener = pd.concat(lista_scr, axis=0, ignore_index=False)\n",
    "        self.screener = self.screener.sort_values(by=['Data', 'Previsione'], ascending=[True, False])\n",
    "\n",
    "    def ultimo_screener(self) -> (pd.Timestamp, pd.DataFrame):\n",
    "        # Ottieni l'ultimo valore dell'indice di livello 0\n",
    "        ultimo_indice = self.screener.index.get_level_values(0)[-1]\n",
    "        # Usa .loc per selezionare tutte le righe con quell'indice di livello 0\n",
    "        return ultimo_indice.date(), self.screener.loc[ultimo_indice]\n",
    "\n",
    "    def calcola_n_azioni(self, prezzo_unitario_acquisto, budget_per_simbolo):\n",
    "        commissione_massima = self.applica_commissione(budget_per_simbolo)\n",
    "        n_azioni_acquisto = (budget_per_simbolo - commissione_massima) // prezzo_unitario_acquisto\n",
    "        return n_azioni_acquisto\n",
    "\n",
    "    def compra(self, data, simbolo, prezzo_unitario_acquisto, n_azioni_acquisto):\n",
    "        prezzo_azioni = prezzo_unitario_acquisto * n_azioni_acquisto\n",
    "        commissione = self.applica_commissione(prezzo_azioni)\n",
    "        totale_transazione = prezzo_azioni + commissione \n",
    "        SL = prezzo_unitario_acquisto * (1 - self.SL) if self.SL is not None else -1\n",
    "        TP = prezzo_unitario_acquisto * (1 + self.TP) if self.TP is not None else np.inf\n",
    "        posizione = {\n",
    "            'data': data.date(),\n",
    "            'simbolo': simbolo,\n",
    "            'prezzo_unitario': prezzo_unitario_acquisto,\n",
    "            'n_azioni': n_azioni_acquisto,\n",
    "            'commissione': -commissione, \n",
    "            'totale_transazione': -totale_transazione,\n",
    "            'SL': SL, \n",
    "            'TP': TP,\n",
    "            'giorni_apertura': 1,\n",
    "            'movimento': 'ACQUISTO'\n",
    "        }\n",
    "        return posizione\n",
    "\n",
    "    def vendi(self, data, simbolo, prezzo_unitario_vendita, n_azioni_vendita):\n",
    "        prezzo_azioni = prezzo_unitario_vendita * n_azioni_vendita\n",
    "        commissione = self.applica_commissione(prezzo_azioni)\n",
    "        totale_transazione = prezzo_azioni - commissione \n",
    "        SL = None\n",
    "        TP = None\n",
    "        posizione = {\n",
    "            'data': data.date(),\n",
    "            'simbolo': simbolo,\n",
    "            'prezzo_unitario': prezzo_unitario_vendita,\n",
    "            'n_azioni': n_azioni_vendita,\n",
    "            'commissione': -commissione, \n",
    "            'totale_transazione': totale_transazione,\n",
    "            'SL': SL, \n",
    "            'TP': TP,\n",
    "            'movimento': 'VENDITA'\n",
    "        }\n",
    "        return posizione\n",
    "    \n",
    "    def simulazione_trading(self):\n",
    "        print('Inizio simulazione trading')\n",
    "        screener = self.screener.loc[(self.screener.index.get_level_values(0) >= self.DATA_INIZIO, slice(None)), :]\n",
    "        self.posizioni_aperte = pd.DataFrame(columns=['data', 'simbolo', 'prezzo_apertura', 'valore_apertura', 'prezzo_chiusura', 'valore_chiusura', 'prezzo_attuale', 'valore_attuale', 'P_L_perc', 'P_L_valore', 'n_azioni', 'commissione', 'totale_transazione', 'SL', 'TP', 'giorni_apertura', 'movimento', 'open', 'high', 'low', 'close'])    \n",
    "        self.storico = pd.DataFrame(columns=['data', 'simbolo', 'prezzo_apertura', 'valore_apertura', 'prezzo_chiusura', 'valore_chiusura', 'prezzo_attuale', 'valore_attuale', 'P_L_perc', 'P_L_valore', 'n_azioni', 'commissione', 'totale_transazione', 'SL', 'TP', 'giorni_apertura', 'movimento', 'open', 'high', 'low', 'close'])\n",
    "        self.stato = pd.DataFrame(columns=['data', 'bilancio', 'n_pos_aperte', 'valore_pos_aperte', 'budget_per_simbolo'])\n",
    "        self.previsioni = pd.DataFrame(columns=['simbolo', 'previsione'])\n",
    "        bilancio = self.BILANCIO_INIZIALE\n",
    "        n_posizioni_libere = self.N_SIMBOLI - len(self.posizioni_aperte)\n",
    "        budget_per_simbolo = bilancio / n_posizioni_libere if n_posizioni_libere != 0 else 0      \n",
    "        data_corrente = self.DATA_INIZIO\n",
    "        while data_corrente <= self.DATA_FINE:\n",
    "            \n",
    "            # APERTURA BORSA\n",
    "            \n",
    "            print(f\"\\033[42m {data_corrente.date()} \\033[0m\")\n",
    "\n",
    "            for i, previsione in self.previsioni.iterrows():\n",
    "                simbolo = previsione['simbolo']\n",
    "                ticker = pd.read_hdf(f'tickers/{simbolo}.h5', 'ticker')    \n",
    "                if data_corrente in ticker.index:\n",
    "                    prezzi_del_giorno = ticker.loc[data_corrente]\n",
    "                    prezzo_unitario = prezzi_del_giorno['Open']\n",
    "                    if (previsione['previsione'] == 'VENDI') and (simbolo in self.posizioni_aperte['simbolo']):\n",
    "                        n_azioni = self.posizioni_aperte.loc[self.posizioni_aperte['simbolo'] == simbolo, 'n_azioni']\n",
    "                        prezzo_tot = prezzo_unitario * n_azioni\n",
    "                        posizione = self.vendi(data_corrente, simbolo, prezzo_unitario, n_azioni)\n",
    "                        for simbolo in self.posizioni_aperte:\n",
    "                            # Trova gli indici delle righe dove la colonna 'simbolo' ha il valore specificato\n",
    "                            indici_da_eliminare = self.posizioni_aperte[self.posizioni_aperte['simbolo'] == simbolo].index\n",
    "                            # Elimina queste righe\n",
    "                            self.posizioni_aperte.drop(indici_da_eliminare, inplace=True)\n",
    "\n",
    "                        bilancio += posizione['totale_transazione']\n",
    "                        df_pos = pd.DataFrame([posizione])\n",
    "                        self.storico = pd.concat([self.storico, df_pos], ignore_index=True)\n",
    "                        print(f\"VENDITA {simbolo} n. {n_azioni} azioni a {prezzo_unitario}, Tot.trans. {posizione['totale_transazione']}, nuovo bilancio: {bilancio}\")\n",
    "                    elif previsione['previsione'] == 'COMPRA':\n",
    "                        n_azioni = self.calcola_n_azioni(prezzo_unitario, budget_per_simbolo)\n",
    "                        prezzo_tot = prezzo_unitario * n_azioni\n",
    "                        commissione = self.applica_commissione(prezzo_tot)\n",
    "                        if (prezzo_tot + commissione) <= budget_per_simbolo:\n",
    "                            posizione = self.compra(data_corrente, simbolo, prezzo_unitario, n_azioni) \n",
    "                            df_pos = pd.DataFrame([posizione])\n",
    "                            self.posizioni_aperte = pd.concat([self.posizioni_aperte, df_pos], ignore_index=True)\n",
    "                            bilancio += posizione['totale_transazione']\n",
    "                            self.storico = pd.concat([self.storico, df_pos], ignore_index=True)\n",
    "                            print(f\"ACQUISTO {simbolo} n. {posizione['n_azioni']} azioni a {prezzo_unitario}, Tot.trans. {posizione['totale_transazione']}, nuovo bilancio: {bilancio}\")\n",
    "                                \n",
    "            # CHIUSURA BORSA\n",
    "            \n",
    "            n_posizioni_libere = self.N_SIMBOLI - len(self.posizioni_aperte)\n",
    "            budget_per_simbolo = bilancio / n_posizioni_libere if n_posizioni_libere != 0 else 0\n",
    "            self.previsioni = pd.DataFrame(columns=['simbolo', 'previsione'])\n",
    "            \n",
    "            posizioni_da_eliminare = [] # Conterrà le posizioni che vanno in SL o TP\n",
    "            # Calcolo valore posizioni aperte e posizioni da chiudere\n",
    "            for _, pos in self.posizioni_aperte.iterrows():\n",
    "                simbolo = pos['simbolo']\n",
    "                ticker = pd.read_hdf(f'tickers/{simbolo}.h5', 'ticker') \n",
    "                if data_corrente in ticker.index:\n",
    "                    giorni_apertura = pos['giorni_apertura']\n",
    "                    prezzi_del_giorno = ticker.loc[data_corrente]\n",
    "                    pos['open'] = prezzi_del_giorno['Open']\n",
    "                    pos['close'] = prezzi_del_giorno['Close']\n",
    "                    pos['high'] = prezzi_del_giorno['High']\n",
    "                    pos['low'] = prezzi_del_giorno['Low']\n",
    "                    prezzo_unitario = prezzi_del_giorno['Close']\n",
    "                    n_azioni = pos['n_azioni']\n",
    "                    importo_totale = prezzo_unitario * n_azioni\n",
    "                    pos['giorni_apertura'] += 1\n",
    "                    pos['prezzo_attuale'] = prezzo_unitario  \n",
    "                    pos['valore_attuale'] = importo_totale\n",
    "                    pos['P_L_perc'] = pct_change(pos['prezzo_apertura'], pos['prezzo_attuale'])   \n",
    "                    pos['P_L_valore'] = pos['valore_attuale'] - pos['valore_apertura'] \n",
    "                    pos['movimento'] = 'TIENI'\n",
    "                                  \n",
    "                    # Verifica condizioni per chiusura posizioni\n",
    "                    \n",
    "                    # 1) Se scatta lo Stop Loss o il Take Profit chiude in automatico la posizione\n",
    "                    if prezzo_unitario < pos['SL']:                       \n",
    "                        posizione = self.vendi(data_corrente, simbolo, prezzo_unitario, n_azioni)\n",
    "                        posizioni_da_eliminare.append(simbolo)\n",
    "                        bilancio += posizione['totale_transazione']\n",
    "                        posizione['movimento'] = 'VENDITA SU STOP LOSS'\n",
    "                        posizione['prezzo_chiusura'] = pos['SL']\n",
    "                        df_pos = pd.DataFrame([posizione])\n",
    "                        self.storico = pd.concat([self.storico, df_pos], ignore_index=True)\n",
    "                        print(f'VENDITA SU STOP LOSS {simbolo}')\n",
    "                    elif prezzo_unitario > pos['TP']:                       \n",
    "                        posizione = self.vendi(data_corrente, simbolo, prezzo_unitario, n_azioni)\n",
    "                        posizioni_da_eliminare.append(simbolo)\n",
    "                        bilancio += posizione['totale_transazione']\n",
    "                        posizione['movimento'] = 'VENDITA SU TAKE PROFIT'\n",
    "                        df_pos = pd.DataFrame([posizione])\n",
    "                        self.storico = pd.concat([self.storico, df_pos], ignore_index=True)\n",
    "                        print(f'VENDITA SU TAKE PROFIT {simbolo}')\n",
    "                    elif giorni_apertura > self.GIORNI_POS:                       \n",
    "                        posizione = self.vendi(data_corrente, simbolo, prezzo_unitario, n_azioni)\n",
    "                        posizioni_da_eliminare.append(simbolo)\n",
    "                        bilancio += posizione['totale_transazione']\n",
    "                        posizione['movimento'] = 'VENDITA SU MAX GIORNI APERTURA'\n",
    "                        df_pos = pd.DataFrame([posizione])\n",
    "                        self.storico = pd.concat([self.storico, df_pos], ignore_index=True)\n",
    "                        print(f'VENDITA SU MAX GIORNI APERTURA {simbolo}')                      \n",
    "                                             \n",
    "                    # 2) Verifica le condizioni per la chiusura manuale il giorno successivo\n",
    "                    \n",
    "                    # CONDIZIONI BASATE SU SCREENER OUT\n",
    "                    # if data_corrente in screener_out.index:\n",
    "                    #     screener_out_corrente = screener_out.loc[data_corrente]  \n",
    "                    #     if simbolo in screener_out_corrente:\n",
    "                    #         prev = pd.DataFrame([{\n",
    "                    #             'simbolo': simbolo,\n",
    "                    #             'previsione': 'VENDI'\n",
    "                    #         }])\n",
    "                    #         self.previsioni = pd.concat([self.previsioni, prev])                   \n",
    "         \n",
    "            for simbolo in posizioni_da_eliminare:\n",
    "                # Trova gli indici delle righe dove la colonna 'simbolo' ha il valore specificato\n",
    "                indici_da_eliminare = self.posizioni_aperte[self.posizioni_aperte['simbolo'] == simbolo].index\n",
    "                # Elimina queste righe\n",
    "                self.posizioni_aperte.drop(indici_da_eliminare, inplace=True)\n",
    "\n",
    "            n_posizioni_libere = self.N_SIMBOLI - len(self.posizioni_aperte) \n",
    "            budget_per_simbolo = bilancio / n_posizioni_libere if n_posizioni_libere != 0 else 0                     \n",
    "\n",
    "            # Verifica condizioni per apertura nuove posizioni\n",
    "            if data_corrente in screener.index:\n",
    "                screener_corrente = screener.loc[data_corrente]           \n",
    "                i_scr = 0\n",
    "                while (n_posizioni_libere > 0) and (i_scr < len(screener_corrente)) and (budget_per_simbolo > 0):\n",
    "                    # Condizioni per apertura posizione\n",
    "                    simbolo = screener_corrente.index[i_scr]  \n",
    "                    if simbolo not in self.posizioni_aperte:                  \n",
    "                        ticker = pd.read_hdf(f'tickers/{simbolo}.h5', 'ticker')    \n",
    "                        prezzi_del_giorno = ticker.loc[data_corrente]\n",
    "                        prezzo_unitario_acquisto = prezzi_del_giorno['Close']\n",
    "                        n_azioni = self.calcola_n_azioni(prezzo_unitario_acquisto, budget_per_simbolo)\n",
    "                        prezzo_tot = prezzo_unitario_acquisto * n_azioni\n",
    "                        commissione = self.applica_commissione(prezzo_tot)\n",
    "                        if (prezzo_tot + commissione) <= budget_per_simbolo:\n",
    "                            posizione = self.compra(data_corrente, simbolo, prezzo_unitario_acquisto, n_azioni) \n",
    "                            prev = pd.DataFrame([{\n",
    "                                'simbolo': simbolo, \n",
    "                                'previsione': 'COMPRA'\n",
    "                            }])\n",
    "                            self.previsioni = pd.concat([self.previsioni, prev], ignore_index=True)\n",
    "                            n_posizioni_libere -= 1\n",
    "                    i_scr += 1\n",
    "            \n",
    "            # Calcolo situazione a fine giornata\n",
    "            stato_attuale = {\n",
    "                'data': data_corrente.date(),\n",
    "                'bilancio': bilancio,\n",
    "                'n_pos_aperte': len(self.posizioni_aperte),\n",
    "                'valore_pos_aperte': valore_pos_aperte, \n",
    "                'budget_per_simbolo': budget_per_simbolo\n",
    "            }\n",
    "            print(f\"Data: {stato_attuale['data']}, Bilancio: {stato_attuale['bilancio']}, N. pos aperte: {stato_attuale['n_pos_aperte']}, Valore pos aperte:{stato_attuale['valore_pos_aperte']}, Budget per simbolo: {stato_attuale['budget_per_simbolo']}, BILANCIO TOT.:{stato_attuale['bilancio'] + stato_attuale['valore_pos_aperte']}\")\n",
    "            stato_attuale_df = pd.DataFrame([stato_attuale])\n",
    "            self.stato = pd.concat([self.stato, stato_attuale_df])\n",
    "            \n",
    "            # Passa al giorno successivo\n",
    "            data_corrente += pd.Timedelta(days=1)   \n",
    "         \n",
    "    def applica_commissione(self, importo_transazione, broker='FINECO'):\n",
    "        if broker == 'FINECO':\n",
    "            tot = importo_transazione * 0.0019\n",
    "            if tot < 2.95:\n",
    "                return 2.95\n",
    "            elif tot > 19:\n",
    "                return 19\n",
    "            else:\n",
    "                return tot \n",
    "        else:\n",
    "            return 0.\n",
    "\n",
    "    # Funzione obiettivo per l'ottimizzazione\n",
    "    def funzione_obiettivo(self, N_SIMBOLI, GIORNI_POS, SL, TP):\n",
    "        # Imposta i parametri\n",
    "        self.N_SIMBOLI = N_SIMBOLI\n",
    "        self.GIORNI_POS = GIORNI_POS\n",
    "        self.SL = SL\n",
    "        self.TP = TP\n",
    "        print('PARAMETRI:')\n",
    "        print(f'N.simboli = {N_SIMBOLI}')\n",
    "        print(f'Giorni pos. = {GIORNI_POS}')\n",
    "        print(f'SL = {SL}')\n",
    "        print(f'TP = {TP}')\n",
    "        # Esegui il trading\n",
    "        self.avvia_trading()\n",
    "\n",
    "        # Restituisce il bilancio finale in modo negativo per la minimizzazione\n",
    "        return -self._bilancio\n",
    "\n",
    "class Modello:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _crea_modello(self):      \n",
    "        # Input layer\n",
    "        input_layer = Input(shape=(self.timesteps, self.n_features))\n",
    "\n",
    "        # Convolutional layer\n",
    "        conv1 = Conv1D(filters=128, kernel_size=5, activation='relu')(input_layer)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "\n",
    "        # Continuation of the model\n",
    "        lstm2 = GRU(50)(conv1)\n",
    "        lstm2 = Dropout(0.5)(lstm2)\n",
    "\n",
    "        dense2 = Dense(80, activation='relu', kernel_regularizer=regularizers.l2(0.02))(lstm2)\n",
    "        dense2 = Dropout(0.5)(dense2)\n",
    "\n",
    "        batch_norm1 = BatchNormalization()(dense2)\n",
    "\n",
    "        dense3 = Dense(40, activation='relu', kernel_regularizer=regularizers.l2(0.02))(batch_norm1)\n",
    "        dense3 = Dropout(0.5)(dense3)\n",
    "\n",
    "        batch_norm2 = BatchNormalization()(dense3)\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = Dense(1, activation='sigmoid')(batch_norm2)\n",
    "\n",
    "        adam = Adam(learning_rate=self.learning_rate)\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy', Precision(), Recall(), AUC(curve='PR')])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def crea(self, \n",
    "             progetto='default', \n",
    "             timesteps=120, \n",
    "             giorni_previsione=1, \n",
    "             features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"], \n",
    "             targets=[\"Target_ingresso\"], \n",
    "             n_ticker_batch=400, \n",
    "             bilanciamento=1, \n",
    "             epochs=100,\n",
    "             batch_size=2052, \n",
    "             soglia=0.5, \n",
    "             class_weights={0: 3, 1: 1},\n",
    "             learning_rate=0.001, \n",
    "             train_test_split=0.2\n",
    "            ):\n",
    "        self.progetto = progetto\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.timesteps = timesteps # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "        self.giorni_previsione = giorni_previsione  # giorni futuri di cui effettuare la previsione\n",
    "        self.features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in self.features]\n",
    "        self.features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in self.features]\n",
    "        self.features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in self.features]\n",
    "        self.features_no_scala = [ft for ft in _features_no_scala_tutte if ft in self.features]\n",
    "        self.features_candele = [ft for ft in _features_candele_tutte if ft in self.features]\n",
    "        self.targets = targets\n",
    "        self.n_ticker_batch = n_ticker_batch\n",
    "        self.bilanciamento = bilanciamento\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.soglia = soglia\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_test_split = train_test_split\n",
    "        self.class_weights = class_weights\n",
    "        self.n_features = len(self.features) \n",
    "        self.n_targets = len(self.targets) \n",
    "        self.model = self._crea_modello() \n",
    "        self.model_history = None\n",
    "\n",
    "    def carica(self, progetto='default'):\n",
    "        self.progetto = progetto\n",
    "        percorso_file = f'{self.progetto}/impostazioni.json'\n",
    "        try:\n",
    "            with open(percorso_file, \"r\") as file:\n",
    "                impostazioni = json.load(file) if os.path.getsize(percorso_file) > 0 else {}\n",
    "                self.timesteps = impostazioni.get(\"timesteps\", 120) # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "                self.giorni_previsione = impostazioni.get(\"giorni_previsione\", 1)  # giorni futuri di cui effettuare la previsione\n",
    "                self.features = impostazioni.get(\"features\", [\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\", \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"])\n",
    "                self.features_scala_prezzo = [ft for ft in _features_scala_prezzo_tutte if ft in self.features]\n",
    "                self.features_da_scalare_singolarmente = [ft for ft in _features_da_scalare_singolarmente_tutte if ft in self.features]\n",
    "                self.features_oscillatori = [ft for ft in _features_oscillatori_tutte if ft in self.features]\n",
    "                self.features_no_scala = [ft for ft in _features_no_scala_tutte if ft in self.features]\n",
    "                self.features_candele = [ft for ft in _features_candele_tutte if ft in self.features]                \n",
    "                self.targets = impostazioni.get(\"targets\", [\"Target_ingresso\"])\n",
    "                self.n_features = len(self.features)\n",
    "                self.n_targets = len(self.targets) \n",
    "                self.n_ticker_batch = impostazioni.get(\"n_ticker_batch\", 400)\n",
    "                self.bilanciamento = impostazioni.get(\"bilanciamento\", 1)\n",
    "                self.batch_size = impostazioni.get(\"batch_size\", 2052)\n",
    "                self.epochs = impostazioni.get(\"epochs\", 100)\n",
    "                self.soglia = impostazioni.get(\"soglia\", 0.5)\n",
    "                self.learning_rate = impostazioni.get(\"learnig_rate\", 0.001)\n",
    "                self.train_test_split = impostazioni.get(\"train_test_split\", 0.2)\n",
    "                self.class_weights = impostazioni.get(\"class_weights\", {0: 3, 1: 1})\n",
    "                self.n_features = len(self.features) \n",
    "                self.n_targets = len(self.targets) \n",
    "                self.model = load_model(f\"{self.progetto}/model.h5\")  \n",
    "                self.model_history = pd.read_hdf(f'{progetto}/model_history.h5', 'history')      \n",
    "        except Exception as e:\n",
    "            print(f\"Errore durante il caricamento del file: {e}\")\n",
    "\n",
    "    def _salva_impostazioni(self):\n",
    "        impostazioni = {\n",
    "            \"timesteps\": self.timesteps,\n",
    "            \"giorni_previsione\": self.giorni_previsione,\n",
    "            \"features\": self.features,\n",
    "            \"targets\": self.targets,\n",
    "            \"n_ticker_batch\": self.n_ticker_batch,\n",
    "            \"bilanciamento\": self.bilanciamento,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"epochs\": self.epochs, \n",
    "            \"soglia\": self.soglia,\n",
    "            \"class_weights\": self.class_weights\n",
    "        }\n",
    "        with open(f'{self.progetto}/impostazioni.json', \"w\") as file:\n",
    "            json.dump(impostazioni, file, indent=4)\n",
    "            \n",
    "        modello = json.loads(self.model.to_json())\n",
    "        with open(f'{self.progetto}/struttura.json', \"w\") as file:\n",
    "            json.dump(modello, file, indent=4)\n",
    "\n",
    "    def salva(self):\n",
    "        os.makedirs(self.progetto, exist_ok=True)\n",
    "        self._salva_impostazioni()\n",
    "        self.model.save(f'{self.progetto}/model.h5')\n",
    "        self.model_history.to_hdf(f'{self.progetto}/model_history.h5', key='history', mode='w')\n",
    "\n",
    "    def genera_XY(self, lista_files, nome_file=''):\n",
    "        perc_file = f'XY/XY_{nome_file}.h5'\n",
    "        if not os.path.exists(perc_file):\n",
    "            manager = Manager()\n",
    "            listaX = manager.list()\n",
    "            listaY = manager.list()\n",
    "            totale_processati = Value('i', 1)  \n",
    "            tot_files = len(lista_files)\n",
    "            with Pool(cpu_count()) as p:\n",
    "                for file_name in lista_files:\n",
    "                    param = (file_name, self.timesteps, self.giorni_previsione, self.features, self.targets, self.bilanciamento)\n",
    "                    p.apply_async(_process_ticker, args=param, callback=lambda result: _callback_XY(result, listaX, listaY, totale_processati, tot_files, hdf5_file=perc_file))\n",
    "\n",
    "                p.close()\n",
    "                p.join()\n",
    "\n",
    "            if len(listaX) > 0:\n",
    "                print(\"\\033[43m\" + 'Salvataggio finale su file X' + \"\\033[0m\")\n",
    "                concatena(listaX, perc_file, dataset_name='X')\n",
    "                print(\"\\033[43m\" + 'Salvataggio finale su file Y' + \"\\033[0m\")\n",
    "                concatena(listaY, perc_file, dataset_name='Y')\n",
    "                del listaX[:]\n",
    "                del listaY[:]\n",
    "\n",
    "    def addestra(self):\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.0001, verbose=1)\n",
    "        #model_checkpoint = ModelCheckpoint(f'{self.progetto}/model.h5', monitor='val_precision', save_best_only=True, verbose=1)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "        list_of_files = os.listdir('tickers')\n",
    "        random.shuffle(list_of_files)\n",
    "        train_files = list_of_files[:self.n_ticker_batch]\n",
    "        val_files = list_of_files[self.n_ticker_batch:int(self.n_ticker_batch*(1+self.train_test_split))]\n",
    "        \n",
    "        print(\"\\033[41m\" + 'Preparazione dati di train' + \"\\033[0m\")\n",
    "        self.genera_XY(train_files, 'train')\n",
    "        print(\"\\033[41m\" + 'Preparazione dati di validazione' + \"\\033[0m\")\n",
    "        self.genera_XY(val_files, 'val')\n",
    "        train_generator = DataGenerator('XY/XY_train.h5', self.batch_size)\n",
    "        val_generator = DataGenerator('XY/XY_val.h5', self.batch_size)    \n",
    "           \n",
    "        history = self.model.fit(train_generator, epochs=self.epochs, validation_data=val_generator, callbacks=callbacks, class_weight=self.class_weights, steps_per_epoch=len(train_generator), validation_steps=len(val_generator))\n",
    "        self.model_history = pd.DataFrame(history.history)\n",
    "        self.salva()\n",
    "        self.grafico_loss(salva_su_file=True)\n",
    "        self.grafico_precision(salva_su_file=True)\n",
    "        df = self.test()\n",
    "        return df\n",
    "        \n",
    "    def previsione_singola(self, nome_simbolo):\n",
    "        print(f'Caricamento dati ticker {nome_simbolo}')\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "        print('Generazione X e Y')\n",
    "        idx, X, Y, _ = to_XY(dati_ticker=ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "        print(f'Previsione')\n",
    "        pred = self.modello_ingresso.model.predict(X)\n",
    "        scr = pd.DataFrame({'Previsione': pred.flatten().round(2), 'Reale': Y.flatten()}, index=idx)\n",
    "        scr = scr[scr.index >= self.DATA_INIZIO]\n",
    "        print(f\"Salvataggio file {nome_simbolo}.h5\")\n",
    "    \n",
    "    def grafico_loss(self, salva_su_file=False):\n",
    "        num_epochs = self.model_history.shape[0]\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['loss'], label=\"Training\")\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['val_loss'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title('Loss')\n",
    "        plt.tight_layout()\n",
    "        if salva_su_file:\n",
    "            plt.savefig(f'{self.progetto}/grafico_loss.png')\n",
    "        plt.show()        \n",
    "\n",
    "    def grafico_precision(self, salva_su_file=False):\n",
    "        num_epochs = self.model_history.shape[0]\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['precision'], label=\"Training\")\n",
    "        plt.plot(np.arange(0, num_epochs), self.model_history['val_precision'], label=\"Validation\")\n",
    "        plt.legend()\n",
    "        plt.title('Precision')\n",
    "        plt.tight_layout()\n",
    "        if salva_su_file:\n",
    "            plt.savefig(f'{self.progetto}/grafico_precision.png')\n",
    "        plt.show()        \n",
    "\n",
    "    def test(self, nome_simbolo='BTG'):\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_simbolo}.h5', 'ticker')\n",
    "        ticker = dropna_iniziali(ticker)\n",
    "        ticker = dropna_finali(ticker)\n",
    "        idx, X, Y, _ = to_XY(ticker, timesteps=self.timesteps, giorni_previsione=self.giorni_previsione, features=self.features, targets=self.targets, bilanciamento=0)\n",
    "        print(f'X.shape: {X.shape}')\n",
    "        print(f'Y.shape: {Y.shape}')\n",
    "        print(f'ticker.shape: {ticker.shape}')\n",
    "        pred = self.model.predict(X, batch_size=self.batch_size, verbose=1, use_multiprocessing=True)\n",
    "        pred_binary = (pred > self.soglia).astype(int)\n",
    "        \n",
    "        result = self.model.evaluate(X, Y, batch_size=self.batch_size, verbose=1, use_multiprocessing=True, return_dict=True)\n",
    "        print(result)\n",
    "        \n",
    "        # Visualizza come heatmap\n",
    "        matrice = confusion_matrix(Y, pred_binary)\n",
    "        sns.heatmap(matrice, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "        plt.xlabel('Previsti')\n",
    "        plt.ylabel('Reali')\n",
    "        plt.savefig(f'{self.progetto}/confusion_matrix.png')\n",
    "        plt.show()\n",
    "        print(f'idx: {idx.shape}')\n",
    "        print(f'pred: {pred.shape}')\n",
    "        print(f'real: {Y.shape}')\n",
    "        df = pd.DataFrame({'Prev': pred.flatten().round(2), 'Real': Y.flatten()}, index=idx)\n",
    "        return df       \n",
    "\n",
    "def _process_ticker(file_name, timesteps, giorni_previsione, features, targets, bilanciamento):\n",
    "    try:\n",
    "        ticker = pd.read_hdf(f'tickers/{file_name}', 'ticker')\n",
    "        ticker = dropna_iniziali(ticker)\n",
    "        ticker = dropna_finali(ticker)\n",
    "        _, X, Y, _ = to_XY(ticker, timesteps, giorni_previsione, features, targets, bilanciamento)\n",
    "        return file_name, X, Y, \"\"\n",
    "    except Exception as e:\n",
    "        return file_name, np.array([]), np.array([]), str(e)\n",
    "\n",
    "def _callback_XY(result, listaX, listaY, totale_processati, tot_files, hdf5_file):\n",
    "    nome_simbolo, X, Y, err = result\n",
    "    if err == \"\":\n",
    "        if X.shape[0] > 0 and Y.shape[0] > 0:  # Verifica se X e Y sono non vuoti\n",
    "            print(f'X.shape:{X.shape}')\n",
    "            print(f'Y.shape:{Y.shape}')\n",
    "            listaX.append(X)\n",
    "            listaY.append(Y)\n",
    "            print(\"\\033[42m\" + f\"{totale_processati.value}/{tot_files}) Completato ticker {nome_simbolo}\" + \"\\033[0m\")\n",
    "        else:\n",
    "            print(\"\\033[43m\" + f\"Ticker {nome_simbolo} ignorato a causa di dati mancanti o errati.\" + \"\\033[0m\")\n",
    "    else:\n",
    "        print(err)\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "        if len(listaX) >= 100:\n",
    "            print(\"\\033[43m\" + 'Salvataggio su file X' + \"\\033[0m\")\n",
    "            concatena(listaX, hdf5_file, dataset_name='X')\n",
    "            print(\"\\033[43m\" + 'Salvataggio su file Y' + \"\\033[0m\")\n",
    "            concatena(listaY, hdf5_file, dataset_name='Y')\n",
    "            del listaX[:]\n",
    "            del listaY[:]\n",
    "     \n",
    "def modifica_target():\n",
    "    totale_processati = Value('i', 1)\n",
    "    list_of_files = os.listdir('tickers')\n",
    "    tot_tickers = len(list_of_files)\n",
    "    with Pool(cpu_count()) as p:\n",
    "        callback_with_args = partial(_callback_modifica_target, totale_processati=totale_processati, tot_tickers=tot_tickers)\n",
    "        for i in range(0, tot_tickers):\n",
    "            nome_file = list_of_files[i]\n",
    "            param = (nome_file,)\n",
    "            p.apply_async(_modifica_target, args=param, callback=callback_with_args)\n",
    "        p.close()\n",
    "        p.join()          \n",
    "   \n",
    "def _modifica_target(nome_file):\n",
    "    try:\n",
    "        ticker = pd.read_hdf(f'tickers/{nome_file}', 'ticker')\n",
    "        # if 'Target_ingresso' in ticker.columns:\n",
    "        #     ticker.drop(['Target_ingresso'], axis=1, inplace=True)\n",
    "        # if 'Target_uscita' in ticker.columns:\n",
    "        #     ticker.drop(['Target_uscita'], axis=1, inplace=True)\n",
    "        ticker = imposta_target(ticker)\n",
    "        return nome_file, ticker, \"\"\n",
    "    except Exception as e:\n",
    "        return nome_file, ticker, str(e)\n",
    "\n",
    "def _callback_modifica_target(result, totale_processati, tot_tickers):\n",
    "    nome_file, ticker, err = result\n",
    "    if err == \"\":\n",
    "        print(f\"{totale_processati.value}/{tot_tickers}) Modificato target {nome_file}\")\n",
    "        ticker.to_hdf(f'tickers/{nome_file}', key='ticker', mode='w')\n",
    "    else:\n",
    "        print(err)\n",
    "\n",
    "    with totale_processati.get_lock(): \n",
    "        totale_processati.value += 1\n",
    "        \n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, file_path, batch_size):\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        # Apriamo il file in modalità lettura e salviamo i riferimenti ai dataset\n",
    "        self.file = h5py.File(file_path, 'r')\n",
    "        self.X = self.file['X']\n",
    "        self.Y = self.file['Y']\n",
    "        self.num_samples = self.X.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calcola gli indici per il batch corrente\n",
    "        start = idx * self.batch_size\n",
    "        end = (idx + 1) * self.batch_size\n",
    "\n",
    "        # Legge solo i dati necessari per il batch corrente\n",
    "        batch_x = self.X[start:end]\n",
    "        batch_y = self.Y[start:end]\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Eventuali azioni alla fine di ogni epoca, se necessario\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        # Assicurati di chiudere il file quando il generatore viene distrutto\n",
    "        self.file.close()\n",
    "\n",
    "def ottimizzazione_parametri():\n",
    "    PROBABILITA_PER_ACQUISTO = 0.5\n",
    "    BILANCIO_INIZIALE = 1000\n",
    "    GIORNI_MAX_POSIZIONE = 40\n",
    "    N_SIMBOLI = 10\n",
    "    DATA_INIZIO = pd.Timestamp(2013, 1, 1)\n",
    "    \n",
    "    inizializza_gpu()\n",
    "\n",
    "    borsa = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, PROBABILITA_PER_ACQUISTO, GIORNI_MAX_POSIZIONE, data_inizio=DATA_INIZIO)\n",
    "    borsa.aggiorna_dati()\n",
    "    \n",
    "    valori_N_SIMBOLI = list(range(1, 11))  # Valori da 1 a 10\n",
    "    valori_GIORNI_POS = list(range(5, 61, 5))  # Valori da 5 a 60 con step di 5\n",
    "    valori_SL = list(np.arange(0.01, 0.1, 0.01))\n",
    "    valori_TP = list(np.arange(0.1, 1, 0.05))\n",
    "\n",
    "    space  = [\n",
    "        Categorical(valori_N_SIMBOLI, name='N_SIMBOLI'),  \n",
    "        Categorical(valori_GIORNI_POS, name='GIORNI_POS'),  \n",
    "        Categorical(valori_SL, name='SL'),  \n",
    "        Categorical(valori_TP, name='TP')\n",
    "    ]\n",
    "\n",
    "    @use_named_args(space)\n",
    "    def objective(**params):\n",
    "        return borsa.funzione_obiettivo(**params)\n",
    "\n",
    "    risultato = gp_minimize(objective, space, n_calls=50, random_state=0)\n",
    "\n",
    "    print(f\"Migliori parametri: {risultato.x}\")\n",
    "\n",
    "    x_iters = risultato.x_iters  # Lista di parametri testati\n",
    "    fun_values = risultato.func_vals  # Lista di valori della funzione obiettivo\n",
    "\n",
    "    df_results = pd.DataFrame(x_iters, columns=['N_SIMBOLI', 'GIORNI_POS', 'SL', 'TP'])\n",
    "    df_results['Valore_Funzione'] = fun_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e1218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Caricamento screener esistenti\n",
      "Caricamento screener esistenti\n"
     ]
    }
   ],
   "source": [
    "PROBABILITA_PER_ACQUISTO = 0.5\n",
    "BILANCIO_INIZIALE = 1000\n",
    "GIORNI_MAX_POSIZIONE = 40\n",
    "N_SIMBOLI = 10\n",
    "DATA_INIZIO = pd.Timestamp(2013, 1, 1)\n",
    "\n",
    "inizializza_gpu()\n",
    "\n",
    "nome_modello = 'mod_2_in'\n",
    "borsa5d = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, PROBABILITA_PER_ACQUISTO, GIORNI_MAX_POSIZIONE, data_inizio=DATA_INIZIO, nome_modello=nome_modello)\n",
    "borsa5d.aggiorna_dati()\n",
    "data5d, scr5d = borsa5d.ultimo_screener()\n",
    "\n",
    "nome_modello2 = 'mod_1_in'\n",
    "borsa20d = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, PROBABILITA_PER_ACQUISTO, GIORNI_MAX_POSIZIONE, data_inizio=DATA_INIZIO, nome_modello=nome_modello2)\n",
    "borsa20d.aggiorna_dati()\n",
    "data20d, scr20d = borsa20d.ultimo_screener()\n",
    "\n",
    "scr_unione = pd.merge(scr5d, scr20d, on='Ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d772bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-05\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Previsione_x</th>\n",
       "      <th>Reale_x</th>\n",
       "      <th>Previsione_y</th>\n",
       "      <th>Reale_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AGL</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEXI</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNF</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEM</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WBA</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIM</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LPL</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PNM</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Previsione_x  Reale_x  Previsione_y  Reale_y\n",
       "Ticker                                              \n",
       "AGL             0.70      0.0          0.95      0.0\n",
       "NEXI            0.60      0.0          0.82      0.0\n",
       "UNF             0.69      0.0          0.81      0.0\n",
       "SEM             0.89      0.0          0.80      0.0\n",
       "WBA             0.64      0.0          0.80      0.0\n",
       "ZIM             0.97      0.0          0.79      0.0\n",
       "LPL             0.88      0.0          0.78      0.0\n",
       "PNM             0.74      0.0          0.76      0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data5d)\n",
    "scr_unione.sort_values(by=['Previsione_y', 'Previsione_x'], ascending=False, inplace=True)\n",
    "scr_unione.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd30936",
   "metadata": {},
   "source": [
    "TP = 0.10\n",
    "SL = 0.01\n",
    "BILANCIO_INIZIALE = 1000\n",
    "N_SIMBOLI = 5\n",
    "DATA_INIZIO = pd.Timestamp(2013, 1, 1)\n",
    "\n",
    "inizializza_gpu()\n",
    "\n",
    "borsa = Borsa(N_SIMBOLI, BILANCIO_INIZIALE, data_inizio=DATA_INIZIO, stop_loss=SL, take_profit=TP)\n",
    "borsa.aggiorna_dati()\n",
    "pos = borsa.simulazione_trading()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892bd212",
   "metadata": {},
   "source": [
    "mod = Modello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mod.carica(progetto='mod_1_in')\n",
    "mod.class_weights = {0: 1, 1: 1}\n",
    "df = mod.addestra()\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3120ae03",
   "metadata": {},
   "source": [
    "\n",
    "mod.crea(\n",
    "    'mod_1_in', \n",
    "    class_weights={0: 1, 1: 1}, \n",
    "    bilanciamento=1, \n",
    "    timesteps=240, \n",
    "    batch_size=8000, \n",
    "    learning_rate=0.01, \n",
    "    n_ticker_batch=300, \n",
    "    train_test_split=0.2,\n",
    "    features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\"]#, \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"]\n",
    ")\n",
    "df = mod.addestra()\n",
    "df\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "mod.crea(\n",
    "    'mod_1_out', \n",
    "    class_weights={0: 1, 1: 1}, \n",
    "    bilanciamento=1, \n",
    "    timesteps=240, \n",
    "    batch_size=4000, \n",
    "    learning_rate=0.01, \n",
    "    n_ticker_batch=300, \n",
    "    train_test_split=0.2,\n",
    "    features=[\"EMA_20\", \"EMA_50\", \"EMA_100\", \"Volume\"],#, \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"]\n",
    "    targets=['Target_uscita']\n",
    ")\n",
    "df = mod.addestra()\n",
    "df\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93261fb5",
   "metadata": {},
   "source": [
    "mod.crea(\n",
    "    'mod_2_in', \n",
    "    class_weights={0: 1, 1: 1}, \n",
    "    bilanciamento=1, \n",
    "    timesteps=240, \n",
    "    batch_size=8000, \n",
    "    learning_rate=0.01, \n",
    "    n_ticker_batch=300, \n",
    "    train_test_split=0.2,\n",
    "    features=[\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"],#, \"MACDh\", \"AROONOSC\", \"TRIX\", \"DM_OSC\", \"TSI\", \"KVO\"]\n",
    "    targets=['Target_mod_2_in']\n",
    ")\n",
    "df = mod.addestra()\n",
    "df\n",
    "mod.salva()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4272395e",
   "metadata": {},
   "source": [
    "df = mod.test()\n",
    "df.to_excel('test_mod_2.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
