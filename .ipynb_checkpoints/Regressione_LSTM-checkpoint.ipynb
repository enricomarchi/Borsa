{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99bae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "simbolo_test = \"BTG\"\n",
    "simbolo_validazione = \"DHT\"\n",
    "n_simboli_addestramento = 10\n",
    "epochs = 33\n",
    "batch_size = 26\n",
    "neuroni = 55\n",
    "l2_rate = 4.38\n",
    "dropout_rate = 0.004\n",
    "max_evals = 50\n",
    "optimizer = \"adam\"\n",
    "#learning_rate = 0.001\n",
    "n_timesteps = 10 # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "giorni_previsione = 10 # n. barre nel futuro di cui si desidera prevedere il prezzo\n",
    "seed = 10 \n",
    "elenco_features = [\n",
    "    \"Close\",\n",
    "    #\"EMA_5\", \n",
    "    #\"EMA_20\", \n",
    "    #\"EMA_50\",\n",
    "    \"Open\",  \n",
    "    \"High\",\n",
    "    \"Low\",\n",
    "    \"Volume\",\n",
    "    #\"MACDh\",\n",
    "    #\"PSAR\",\n",
    "    #\"PSARaf\",\n",
    "    #\"SUPERT\", \n",
    "    #\"TRIX\",\n",
    "    #\"ATR\"\n",
    "]\n",
    "elenco_target = [\n",
    "    #\"Close\",\n",
    "    \"EMA_5\",\n",
    "    \"EMA_20\", \n",
    "    \"EMA_50\"    \n",
    "]\n",
    "\n",
    "col_features = {col: idx for idx, col in enumerate(elenco_features)}\n",
    "col_targets = {col: idx for idx, col in enumerate(elenco_target)}\n",
    "n_features = len(col_features)\n",
    "n_targets = len(col_targets)\n",
    "\n",
    "def crea_modello(giorni_previsione=giorni_previsione, n_targets=n_targets, n_features=n_features, neuroni=neuroni, l2_rate=l2_rate, dropout_rate=dropout_rate, optimizer='adam'):\n",
    "    model = Sequential([\n",
    "        LSTM(neuroni, input_shape=(n_timesteps, n_features)),\n",
    "        RepeatVector(giorni_previsione),\n",
    "        LSTM(neuroni, return_sequences=True, kernel_regularizer=l2(l2_rate)),\n",
    "        Dropout(dropout_rate),\n",
    "        #LSTM(100, return_sequences=True, kernel_regularizer=l2(0.01)),\n",
    "        #Dropout(0.2),\n",
    "        #LSTM(50, kernel_regularizer=l2(0.01)),\n",
    "        #Dropout(0.2),\n",
    "        #Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        #Dropout(0.2),\n",
    "        TimeDistributed(Dense(n_targets, activation='linear'))\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='mean_absolute_percentage_error')#, metrics=['mse', 'mae', 'msle', Huber()])\n",
    "    return model\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "#from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import clone\n",
    "scaler = PowerTransformer()\n",
    "#scaler = RobustScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25570dc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importa librerie............=\n",
      "Download lista ticker\n",
      "Creazione modello\n"
     ]
    }
   ],
   "source": [
    "print(\"Importa librerie\", end=\"\", flush=True)\n",
    "\n",
    "#from keras.losses import Huber\n",
    "#print(\".\", end=\"\", flush=True)\n",
    "import pandas as pd\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import yfinance as yf\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import numpy as np\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import tensorflow as tf\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import funzioni as fx\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.layers import Dropout\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.layers import RepeatVector\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from keras.regularizers import l2\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras import Sequential\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "print(\".\", end=\"\", flush=True)\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#print(\".\", end=\"\", flush=True)\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.models import load_model\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import os\n",
    "print(\"=\", end=\"\\n\", flush=True)\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"Download lista ticker\")\n",
    "lista_ticker = pd.read_parquet(\"Tickers_De_Giro.parquet\")\n",
    "lista_ticker = lista_ticker.sample(frac=1).reset_index(drop=True)\n",
    "lista_ticker = lista_ticker.loc[(lista_ticker[\"Ticker\"] != simbolo_test) & (lista_ticker[\"Ticker\"] != simbolo_validazione) & (lista_ticker[\"Categoria\"] != \"D\"), :]\n",
    "\n",
    "if os.path.exists('LSTM.keras'):\n",
    "    print(\"Caricamento modello esistente\")\n",
    "    model = load_model(\"LSTM.keras\")\n",
    "else:\n",
    "    print(\"Creazione modello\")\n",
    "    model = crea_modello(giorni_previsione, n_targets, n_features)\n",
    "\n",
    "def to_XY(dati_ticker, n_timesteps, giorni_previsione, addestramento=True):\n",
    "    new_dates = pd.bdate_range(start=dati_ticker.index[-1] + pd.Timedelta(days=1), periods=giorni_previsione)\n",
    "    df_new = pd.DataFrame(index=new_dates)\n",
    "    dati_ticker = pd.concat([dati_ticker, df_new])\n",
    "\n",
    "    features = dati_ticker[elenco_features]\n",
    "    targets = dati_ticker[elenco_target]\n",
    "\n",
    "    if addestramento:\n",
    "        i_tot = len(features) - giorni_previsione*2\n",
    "    else:\n",
    "        i_tot = len(features) - giorni_previsione\n",
    "    X, Y = [], []\n",
    "    for i in range(n_timesteps - 1, i_tot):\n",
    "        X.append(features.iloc[i - (n_timesteps - 1):i + 1])\n",
    "        Y.append(targets.iloc[i + 1:i + 1 + giorni_previsione])\n",
    "    \n",
    "    idx = dati_ticker.index[n_timesteps - 1:i_tot]\n",
    "    \n",
    "    return idx, np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c6e9b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download dati ticker validazione\n",
      "Calcolo indicatori ticker validazione\n",
      "\u001b[48;5;42m1 di 10: Ticker AVNS\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "82/82 [==============================] - 8s 36ms/step - loss: 281.6786 - val_loss: 215.3965\n",
      "Epoch 2/33\n",
      "82/82 [==============================] - 2s 22ms/step - loss: 118.7553 - val_loss: 119.1135\n",
      "Epoch 3/33\n",
      "82/82 [==============================] - 1s 18ms/step - loss: 85.0335 - val_loss: 109.9342\n",
      "Epoch 4/33\n",
      "82/82 [==============================] - 1s 18ms/step - loss: 73.5307 - val_loss: 103.1551\n",
      "Epoch 5/33\n",
      "82/82 [==============================] - 2s 19ms/step - loss: 66.8338 - val_loss: 119.4328\n",
      "Epoch 6/33\n",
      "82/82 [==============================] - 1s 17ms/step - loss: 58.9336 - val_loss: 105.2740\n",
      "Epoch 7/33\n",
      "82/82 [==============================] - 1s 18ms/step - loss: 57.9612 - val_loss: 92.7316\n",
      "Epoch 8/33\n",
      "82/82 [==============================] - 1s 18ms/step - loss: 53.2223 - val_loss: 91.6186\n",
      "Epoch 9/33\n",
      "82/82 [==============================] - 2s 20ms/step - loss: 50.6003 - val_loss: 83.4200\n",
      "Epoch 10/33\n",
      "82/82 [==============================] - 2s 22ms/step - loss: 51.2651 - val_loss: 85.3052\n",
      "Epoch 11/33\n",
      "82/82 [==============================] - 2s 19ms/step - loss: 48.4734 - val_loss: 81.4684\n",
      "Epoch 12/33\n",
      "82/82 [==============================] - 2s 20ms/step - loss: 47.4967 - val_loss: 87.7757\n",
      "Epoch 13/33\n",
      "82/82 [==============================] - 2s 22ms/step - loss: 48.5820 - val_loss: 89.5785\n",
      "Epoch 14/33\n",
      "82/82 [==============================] - 2s 21ms/step - loss: 47.7739 - val_loss: 97.0437\n",
      "Epoch 15/33\n",
      "82/82 [==============================] - 2s 21ms/step - loss: 45.2157 - val_loss: 74.4111\n",
      "Epoch 16/33\n",
      "82/82 [==============================] - 2s 20ms/step - loss: 43.4024 - val_loss: 73.1409\n",
      "Epoch 17/33\n",
      "82/82 [==============================] - 1s 17ms/step - loss: 43.1344 - val_loss: 72.5210\n",
      "Epoch 18/33\n",
      "82/82 [==============================] - 2s 19ms/step - loss: 44.6749 - val_loss: 72.3624\n",
      "Epoch 19/33\n",
      "82/82 [==============================] - 2s 20ms/step - loss: 41.3173 - val_loss: 68.2233\n",
      "Epoch 20/33\n",
      "82/82 [==============================] - 2s 19ms/step - loss: 44.1129 - val_loss: 71.7062\n",
      "Epoch 21/33\n",
      "82/82 [==============================] - 2s 20ms/step - loss: 41.5534 - val_loss: 77.4644\n",
      "Epoch 22/33\n",
      "82/82 [==============================] - 2s 20ms/step - loss: 41.3621 - val_loss: 73.1500\n",
      "Epoch 23/33\n",
      "82/82 [==============================] - 2s 22ms/step - loss: 40.5426 - val_loss: 83.4716\n",
      "Epoch 24/33\n",
      "82/82 [==============================] - 2s 22ms/step - loss: 40.5953 - val_loss: 70.7922\n",
      "Epoch 25/33\n",
      "82/82 [==============================] - 2s 23ms/step - loss: 40.0300 - val_loss: 90.4499\n",
      "Epoch 26/33\n",
      "82/82 [==============================] - 2s 21ms/step - loss: 40.1276 - val_loss: 63.1134\n",
      "Epoch 27/33\n",
      "82/82 [==============================] - 1s 17ms/step - loss: 39.7911 - val_loss: 69.6037\n",
      "Epoch 28/33\n",
      "82/82 [==============================] - 1s 18ms/step - loss: 39.1362 - val_loss: 64.4520\n",
      "Epoch 29/33\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 39.9423 - val_loss: 68.5407\n",
      "Epoch 30/33\n",
      "82/82 [==============================] - 1s 18ms/step - loss: 37.2807 - val_loss: 66.8167\n",
      "Epoch 31/33\n",
      "82/82 [==============================] - 2s 21ms/step - loss: 40.4284 - val_loss: 71.8085\n",
      "Epoch 32/33\n",
      "82/82 [==============================] - 2s 19ms/step - loss: 37.4255 - val_loss: 77.5149\n",
      "Epoch 33/33\n",
      "82/82 [==============================] - 2s 22ms/step - loss: 37.5333 - val_loss: 72.5720\n",
      "\u001b[48;5;42m2 di 10: Ticker EC\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 41.1298 - val_loss: 72.6368\n",
      "Epoch 2/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 32.4300 - val_loss: 69.8843\n",
      "Epoch 3/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 31.3280 - val_loss: 59.7802\n",
      "Epoch 4/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 29.8985 - val_loss: 65.1601\n",
      "Epoch 5/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 31.0667 - val_loss: 63.8336\n",
      "Epoch 6/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 28.5258 - val_loss: 62.5329\n",
      "Epoch 7/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 29.1814 - val_loss: 66.0270\n",
      "Epoch 8/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 27.4251 - val_loss: 62.7328\n",
      "Epoch 9/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 27.9993 - val_loss: 67.4543\n",
      "Epoch 10/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.9716 - val_loss: 60.1400\n",
      "Epoch 11/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.3333 - val_loss: 59.5250\n",
      "Epoch 12/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 25.5093 - val_loss: 64.9970\n",
      "Epoch 13/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.8947 - val_loss: 62.3483\n",
      "Epoch 14/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 27.6380 - val_loss: 56.5505\n",
      "Epoch 15/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 27.1867 - val_loss: 64.7181\n",
      "Epoch 16/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 26.3980 - val_loss: 61.3378\n",
      "Epoch 17/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 25.2281 - val_loss: 60.6529\n",
      "Epoch 18/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.4016 - val_loss: 59.8239\n",
      "Epoch 19/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 25.3411 - val_loss: 60.9778\n",
      "Epoch 20/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.1006 - val_loss: 59.1730\n",
      "Epoch 21/33\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 25.5053 - val_loss: 60.7577\n",
      "Epoch 22/33\n",
      "129/129 [==============================] - 3s 21ms/step - loss: 24.6586 - val_loss: 55.8730\n",
      "Epoch 23/33\n",
      "129/129 [==============================] - 3s 21ms/step - loss: 25.4839 - val_loss: 57.3831\n",
      "Epoch 24/33\n",
      "129/129 [==============================] - 3s 20ms/step - loss: 25.3727 - val_loss: 61.0573\n",
      "Epoch 25/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 25.2153 - val_loss: 57.5316\n",
      "Epoch 26/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 24.5154 - val_loss: 61.9484\n",
      "Epoch 27/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 25.4774 - val_loss: 62.8659\n",
      "Epoch 28/33\n",
      "129/129 [==============================] - 3s 20ms/step - loss: 24.7552 - val_loss: 59.7349\n",
      "Epoch 29/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 24.1436 - val_loss: 55.5921\n",
      "Epoch 30/33\n",
      "129/129 [==============================] - 3s 25ms/step - loss: 24.2686 - val_loss: 58.9480\n",
      "Epoch 31/33\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 24.4737 - val_loss: 56.1643\n",
      "Epoch 32/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 24.3301 - val_loss: 56.8510\n",
      "Epoch 33/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 24.0940 - val_loss: 59.4787\n",
      "\u001b[48;5;42m3 di 10: Ticker CLVT\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 32.8225 - val_loss: 53.0979\n",
      "Epoch 2/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 32.3091 - val_loss: 48.8069\n",
      "Epoch 3/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 29.2592 - val_loss: 53.9375\n",
      "Epoch 4/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 27.9786 - val_loss: 57.9036\n",
      "Epoch 5/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 26.7482 - val_loss: 60.0693\n",
      "Epoch 6/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 27.9386 - val_loss: 55.6881\n",
      "Epoch 7/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 26.2717 - val_loss: 60.8055\n",
      "Epoch 8/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 24.4377 - val_loss: 66.8804\n",
      "Epoch 9/33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 24ms/step - loss: 25.6422 - val_loss: 64.9612\n",
      "Epoch 10/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 27.0346 - val_loss: 58.3237\n",
      "Epoch 11/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 26.5401 - val_loss: 59.7333\n",
      "Epoch 12/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 25.0881 - val_loss: 64.4311\n",
      "Epoch 13/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 23.2814 - val_loss: 62.8883\n",
      "Epoch 14/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 24.0680 - val_loss: 60.7205\n",
      "Epoch 15/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 23.9083 - val_loss: 64.3540\n",
      "Epoch 16/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 23.6066 - val_loss: 61.7243\n",
      "Epoch 17/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 24.3935 - val_loss: 59.4507\n",
      "Epoch 18/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 23.8178 - val_loss: 62.8086\n",
      "Epoch 19/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 22.7891 - val_loss: 62.6091\n",
      "Epoch 20/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 24.0019 - val_loss: 66.2685\n",
      "Epoch 21/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 22.3261 - val_loss: 66.3329\n",
      "Epoch 22/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 22.8037 - val_loss: 67.9797\n",
      "Epoch 23/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 22.7138 - val_loss: 73.0391\n",
      "Epoch 24/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 25.8085 - val_loss: 67.8165\n",
      "Epoch 25/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 23.0481 - val_loss: 69.4646\n",
      "Epoch 26/33\n",
      "43/43 [==============================] - 1s 29ms/step - loss: 21.7689 - val_loss: 71.8523\n",
      "Epoch 27/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 24.9444 - val_loss: 77.5712\n",
      "Epoch 28/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 23.5845 - val_loss: 77.0799\n",
      "Epoch 29/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 22.8295 - val_loss: 82.8409\n",
      "Epoch 30/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 22.3058 - val_loss: 82.0841\n",
      "Epoch 31/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 23.7843 - val_loss: 80.6264\n",
      "Epoch 32/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 21.0496 - val_loss: 89.0830\n",
      "Epoch 33/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 22.9892 - val_loss: 84.2627\n",
      "\u001b[48;5;42m4 di 10: Ticker NTST\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 316.9979 - val_loss: 134.2477\n",
      "Epoch 2/33\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 231.9322 - val_loss: 114.8348\n",
      "Epoch 3/33\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 172.0040 - val_loss: 137.2307\n",
      "Epoch 4/33\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 156.0787 - val_loss: 100.4706\n",
      "Epoch 5/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 97.8351 - val_loss: 95.7412\n",
      "Epoch 6/33\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 82.1952 - val_loss: 88.4514\n",
      "Epoch 7/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 78.8489 - val_loss: 87.0809\n",
      "Epoch 8/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 82.6154 - val_loss: 86.9979\n",
      "Epoch 9/33\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 65.6028 - val_loss: 81.3986\n",
      "Epoch 10/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 56.3308 - val_loss: 75.4869\n",
      "Epoch 11/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 61.7526 - val_loss: 76.1243\n",
      "Epoch 12/33\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 65.8391 - val_loss: 80.8911\n",
      "Epoch 13/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 65.4029 - val_loss: 77.6824\n",
      "Epoch 14/33\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 57.8662 - val_loss: 73.5167\n",
      "Epoch 15/33\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 62.6636 - val_loss: 77.9813\n",
      "Epoch 16/33\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 67.1859 - val_loss: 80.1734\n",
      "Epoch 17/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 57.9520 - val_loss: 78.4429\n",
      "Epoch 18/33\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 65.3151 - val_loss: 77.7855\n",
      "Epoch 19/33\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 64.9532 - val_loss: 72.2040\n",
      "Epoch 20/33\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 49.6719 - val_loss: 71.9644\n",
      "Epoch 21/33\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 49.6284 - val_loss: 73.9355\n",
      "Epoch 22/33\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 61.1092 - val_loss: 75.1684\n",
      "Epoch 23/33\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 49.2615 - val_loss: 76.2621\n",
      "Epoch 24/33\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 59.2522 - val_loss: 77.3898\n",
      "Epoch 25/33\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 46.9847 - val_loss: 75.6004\n",
      "Epoch 26/33\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 51.5017 - val_loss: 76.8616\n",
      "Epoch 27/33\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 48.2652 - val_loss: 74.2036\n",
      "Epoch 28/33\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 46.5166 - val_loss: 74.2411\n",
      "Epoch 29/33\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 48.2799 - val_loss: 77.0108\n",
      "Epoch 30/33\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 47.3821 - val_loss: 77.4044\n",
      "Epoch 31/33\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 44.9561 - val_loss: 76.5892\n",
      "Epoch 32/33\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 50.7298 - val_loss: 75.9726\n",
      "Epoch 33/33\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 49.4928 - val_loss: 76.3728\n",
      "\u001b[48;5;42m5 di 10: Ticker FDP\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 46.2859 - val_loss: 58.6414\n",
      "Epoch 2/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 40.2638 - val_loss: 57.3317\n",
      "Epoch 3/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 38.7289 - val_loss: 55.1257\n",
      "Epoch 4/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 37.5696 - val_loss: 57.3197\n",
      "Epoch 5/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 36.2560 - val_loss: 55.9574\n",
      "Epoch 6/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 35.4880 - val_loss: 59.9294\n",
      "Epoch 7/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 35.4414 - val_loss: 58.3993\n",
      "Epoch 8/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 34.3826 - val_loss: 54.8842\n",
      "Epoch 9/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 34.4801 - val_loss: 55.9400\n",
      "Epoch 10/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 33.9124 - val_loss: 58.2894\n",
      "Epoch 11/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 34.4994 - val_loss: 62.1399\n",
      "Epoch 12/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 33.4201 - val_loss: 59.8037\n",
      "Epoch 13/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 32.6421 - val_loss: 58.2116\n",
      "Epoch 14/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 32.7346 - val_loss: 59.5304\n",
      "Epoch 15/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 34.2362 - val_loss: 57.1635\n",
      "Epoch 16/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 32.4541 - val_loss: 60.0638\n",
      "Epoch 17/33\n",
      "129/129 [==============================] - 3s 20ms/step - loss: 32.1998 - val_loss: 57.3615\n",
      "Epoch 18/33\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 31.4996 - val_loss: 58.2752\n",
      "Epoch 19/33\n",
      "129/129 [==============================] - 3s 21ms/step - loss: 31.6368 - val_loss: 57.7509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/33\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 30.8745 - val_loss: 67.0610\n",
      "Epoch 21/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 30.9648 - val_loss: 68.8050\n",
      "Epoch 22/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 30.7897 - val_loss: 65.6117\n",
      "Epoch 23/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 30.6050 - val_loss: 60.2454\n",
      "Epoch 24/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 30.7381 - val_loss: 67.5312\n",
      "Epoch 25/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 30.4867 - val_loss: 66.6362\n",
      "Epoch 26/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 30.2003 - val_loss: 66.5307\n",
      "Epoch 27/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 30.5897 - val_loss: 75.0291\n",
      "Epoch 28/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 30.1338 - val_loss: 68.7304\n",
      "Epoch 29/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 30.2984 - val_loss: 75.9309\n",
      "Epoch 30/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 29.7221 - val_loss: 70.8571\n",
      "Epoch 31/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 29.9856 - val_loss: 73.0592\n",
      "Epoch 32/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 29.0828 - val_loss: 73.4773\n",
      "Epoch 33/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 29.1433 - val_loss: 73.2058\n",
      "\u001b[48;5;42m6 di 10: Ticker CBU\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 40.2415 - val_loss: 135.6535\n",
      "Epoch 2/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 43.3790 - val_loss: 140.8674\n",
      "Epoch 3/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 56.0552 - val_loss: 162.1118\n",
      "Epoch 4/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 113.1766 - val_loss: 151.3986\n",
      "Epoch 5/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 66.4717 - val_loss: 196.9629\n",
      "Epoch 6/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 48.9079 - val_loss: 116.8132\n",
      "Epoch 7/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 26.2521 - val_loss: 139.1816\n",
      "Epoch 8/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 33.3414 - val_loss: 117.1117\n",
      "Epoch 9/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 21.0926 - val_loss: 118.5660\n",
      "Epoch 10/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.8707 - val_loss: 132.9904\n",
      "Epoch 11/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 20.7126 - val_loss: 114.9330\n",
      "Epoch 12/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 25.9262 - val_loss: 127.3407\n",
      "Epoch 13/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 19.1929 - val_loss: 110.5551\n",
      "Epoch 14/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 23.2827 - val_loss: 121.2409\n",
      "Epoch 15/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 21.1072 - val_loss: 121.8573\n",
      "Epoch 16/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 19.9025 - val_loss: 108.0818\n",
      "Epoch 17/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 17.7526 - val_loss: 119.0974\n",
      "Epoch 18/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 21.5750 - val_loss: 121.2952\n",
      "Epoch 19/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 17.9537 - val_loss: 119.8714\n",
      "Epoch 20/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 18.6402 - val_loss: 114.5915\n",
      "Epoch 21/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 16.9272 - val_loss: 107.2494\n",
      "Epoch 22/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 17.9471 - val_loss: 112.6501\n",
      "Epoch 23/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 17.1730 - val_loss: 104.3865\n",
      "Epoch 24/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 17.1823 - val_loss: 117.0935\n",
      "Epoch 25/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 17.3489 - val_loss: 114.5178\n",
      "Epoch 26/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 17.2677 - val_loss: 119.3618\n",
      "Epoch 27/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 21.7067 - val_loss: 103.9352\n",
      "Epoch 28/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 17.6948 - val_loss: 110.3822\n",
      "Epoch 29/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 17.1225 - val_loss: 109.3243\n",
      "Epoch 30/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 16.8446 - val_loss: 110.3427\n",
      "Epoch 31/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 16.0691 - val_loss: 114.2663\n",
      "Epoch 32/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 17.0948 - val_loss: 114.0569\n",
      "Epoch 33/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 15.5563 - val_loss: 117.0771\n",
      "\u001b[48;5;42m7 di 10: Ticker BGS\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 38.5382 - val_loss: 63.2843\n",
      "Epoch 2/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 33.0524 - val_loss: 56.3011\n",
      "Epoch 3/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 30.2918 - val_loss: 55.0515\n",
      "Epoch 4/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 29.7648 - val_loss: 50.7880\n",
      "Epoch 5/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 29.1261 - val_loss: 53.0558\n",
      "Epoch 6/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 29.2081 - val_loss: 48.5702\n",
      "Epoch 7/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 28.6830 - val_loss: 51.0820\n",
      "Epoch 8/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 28.5616 - val_loss: 52.6331\n",
      "Epoch 9/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 28.6753 - val_loss: 49.3192\n",
      "Epoch 10/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 28.0652 - val_loss: 51.9497\n",
      "Epoch 11/33\n",
      "129/129 [==============================] - 2s 19ms/step - loss: 27.9356 - val_loss: 50.0225\n",
      "Epoch 12/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 27.5470 - val_loss: 53.7648\n",
      "Epoch 13/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.3284 - val_loss: 51.7177\n",
      "Epoch 14/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.2697 - val_loss: 54.1576\n",
      "Epoch 15/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.9416 - val_loss: 52.5100\n",
      "Epoch 16/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.9226 - val_loss: 53.6712\n",
      "Epoch 17/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.6959 - val_loss: 53.9394\n",
      "Epoch 18/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 27.0278 - val_loss: 52.0553\n",
      "Epoch 19/33\n",
      "129/129 [==============================] - 2s 18ms/step - loss: 27.2733 - val_loss: 51.7954\n",
      "Epoch 20/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.8605 - val_loss: 54.0308\n",
      "Epoch 21/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.7142 - val_loss: 56.3539\n",
      "Epoch 22/33\n",
      "129/129 [==============================] - 2s 15ms/step - loss: 26.6515 - val_loss: 55.8203\n",
      "Epoch 23/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.3878 - val_loss: 55.6268\n",
      "Epoch 24/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 26.3854 - val_loss: 57.8559\n",
      "Epoch 25/33\n",
      "129/129 [==============================] - 2s 17ms/step - loss: 26.4661 - val_loss: 55.3442\n",
      "Epoch 26/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.0713 - val_loss: 54.0999\n",
      "Epoch 27/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.0633 - val_loss: 55.3953\n",
      "Epoch 28/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 25.6864 - val_loss: 58.1696\n",
      "Epoch 29/33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 2s 16ms/step - loss: 32.8177 - val_loss: 61.7618\n",
      "Epoch 30/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 27.2276 - val_loss: 62.0643\n",
      "Epoch 31/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.8186 - val_loss: 59.9509\n",
      "Epoch 32/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.7006 - val_loss: 61.2670\n",
      "Epoch 33/33\n",
      "129/129 [==============================] - 2s 16ms/step - loss: 26.0882 - val_loss: 64.9973\n",
      "\u001b[48;5;42m8 di 10: Ticker PSN\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 91.0425 - val_loss: 66.6582\n",
      "Epoch 2/33\n",
      "38/38 [==============================] - 1s 25ms/step - loss: 71.7275 - val_loss: 59.7021\n",
      "Epoch 3/33\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 63.7861 - val_loss: 66.1786\n",
      "Epoch 4/33\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 54.7995 - val_loss: 66.6213\n",
      "Epoch 5/33\n",
      "38/38 [==============================] - 1s 27ms/step - loss: 54.0853 - val_loss: 60.9496\n",
      "Epoch 6/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 53.0280 - val_loss: 53.8063\n",
      "Epoch 7/33\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 49.3794 - val_loss: 57.4243\n",
      "Epoch 8/33\n",
      "38/38 [==============================] - 2s 48ms/step - loss: 48.3810 - val_loss: 54.2473\n",
      "Epoch 9/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 49.4625 - val_loss: 59.6879\n",
      "Epoch 10/33\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 46.2319 - val_loss: 52.3462\n",
      "Epoch 11/33\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 46.2305 - val_loss: 57.0809\n",
      "Epoch 12/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 46.9498 - val_loss: 53.8055\n",
      "Epoch 13/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 43.5864 - val_loss: 54.3780\n",
      "Epoch 14/33\n",
      "38/38 [==============================] - 1s 27ms/step - loss: 44.7899 - val_loss: 58.2306\n",
      "Epoch 15/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 45.3955 - val_loss: 61.1170\n",
      "Epoch 16/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 43.3336 - val_loss: 54.5925\n",
      "Epoch 17/33\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 43.3306 - val_loss: 55.0905\n",
      "Epoch 18/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 43.3755 - val_loss: 63.7358\n",
      "Epoch 19/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 43.7265 - val_loss: 60.5943\n",
      "Epoch 20/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 43.8971 - val_loss: 51.7938\n",
      "Epoch 21/33\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 42.7389 - val_loss: 59.8761\n",
      "Epoch 22/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 42.3872 - val_loss: 59.7555\n",
      "Epoch 23/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 43.0173 - val_loss: 56.4605\n",
      "Epoch 24/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 41.0182 - val_loss: 54.5903\n",
      "Epoch 25/33\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 42.9074 - val_loss: 65.0848\n",
      "Epoch 26/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 41.4654 - val_loss: 61.1823\n",
      "Epoch 27/33\n",
      "38/38 [==============================] - 1s 29ms/step - loss: 40.4750 - val_loss: 57.4289\n",
      "Epoch 28/33\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 43.2519 - val_loss: 57.5054\n",
      "Epoch 29/33\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 41.3879 - val_loss: 65.9124\n",
      "Epoch 30/33\n",
      "38/38 [==============================] - 1s 26ms/step - loss: 40.6643 - val_loss: 57.5913\n",
      "Epoch 31/33\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 40.5644 - val_loss: 57.9066\n",
      "Epoch 32/33\n",
      "38/38 [==============================] - 1s 28ms/step - loss: 41.3380 - val_loss: 57.5979\n",
      "Epoch 33/33\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 39.2244 - val_loss: 62.7952\n",
      "\u001b[48;5;42m9 di 10: Ticker BRP\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 122.8587 - val_loss: 112.5996\n",
      "Epoch 2/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 126.9588 - val_loss: 83.7690\n",
      "Epoch 3/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 95.8336 - val_loss: 79.4967\n",
      "Epoch 4/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 158.7474 - val_loss: 96.2175\n",
      "Epoch 5/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 121.3382 - val_loss: 80.8392\n",
      "Epoch 6/33\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 108.1129 - val_loss: 87.9217\n",
      "Epoch 7/33\n",
      "34/34 [==============================] - 1s 35ms/step - loss: 91.1526 - val_loss: 71.2679\n",
      "Epoch 8/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 92.5671 - val_loss: 73.1808\n",
      "Epoch 9/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 85.9196 - val_loss: 72.4080\n",
      "Epoch 10/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 87.7454 - val_loss: 70.1149\n",
      "Epoch 11/33\n",
      "34/34 [==============================] - 1s 27ms/step - loss: 81.8889 - val_loss: 64.2736\n",
      "Epoch 12/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 89.7555 - val_loss: 66.7866\n",
      "Epoch 13/33\n",
      "34/34 [==============================] - 1s 31ms/step - loss: 82.3286 - val_loss: 72.4144\n",
      "Epoch 14/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 83.6845 - val_loss: 65.7293\n",
      "Epoch 15/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 77.4683 - val_loss: 63.2033\n",
      "Epoch 16/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 81.7982 - val_loss: 68.3194\n",
      "Epoch 17/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 76.9519 - val_loss: 70.3503\n",
      "Epoch 18/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 79.6007 - val_loss: 67.2723\n",
      "Epoch 19/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 76.3492 - val_loss: 71.3275\n",
      "Epoch 20/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 75.2747 - val_loss: 65.7826\n",
      "Epoch 21/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 75.2177 - val_loss: 70.5633\n",
      "Epoch 22/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 75.4332 - val_loss: 68.6117\n",
      "Epoch 23/33\n",
      "34/34 [==============================] - 1s 29ms/step - loss: 74.7319 - val_loss: 67.3135\n",
      "Epoch 24/33\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 73.8142 - val_loss: 63.9660\n",
      "Epoch 25/33\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 84.5873 - val_loss: 71.2984\n",
      "Epoch 26/33\n",
      "34/34 [==============================] - 1s 39ms/step - loss: 76.3037 - val_loss: 68.5351\n",
      "Epoch 27/33\n",
      "34/34 [==============================] - 1s 37ms/step - loss: 77.4641 - val_loss: 68.7751\n",
      "Epoch 28/33\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 70.4275 - val_loss: 66.3684\n",
      "Epoch 29/33\n",
      "34/34 [==============================] - 1s 34ms/step - loss: 73.7926 - val_loss: 69.3463\n",
      "Epoch 30/33\n",
      "34/34 [==============================] - 1s 36ms/step - loss: 69.7613 - val_loss: 65.4675\n",
      "Epoch 31/33\n",
      "34/34 [==============================] - 1s 30ms/step - loss: 69.1126 - val_loss: 67.6081\n",
      "Epoch 32/33\n",
      "34/34 [==============================] - 1s 32ms/step - loss: 66.6156 - val_loss: 62.7417\n",
      "Epoch 33/33\n",
      "34/34 [==============================] - 1s 28ms/step - loss: 69.3670 - val_loss: 68.3961\n",
      "\u001b[48;5;42m10 di 10: Ticker YETI\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 57.7741 - val_loss: 59.0908\n",
      "Epoch 2/33\n",
      "43/43 [==============================] - 1s 30ms/step - loss: 58.8008 - val_loss: 49.8601\n",
      "Epoch 3/33\n",
      "43/43 [==============================] - 1s 29ms/step - loss: 50.0603 - val_loss: 51.2767\n",
      "Epoch 4/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 47.9718 - val_loss: 52.0134\n",
      "Epoch 5/33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 26ms/step - loss: 45.9650 - val_loss: 52.5280\n",
      "Epoch 6/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 41.6118 - val_loss: 56.5659\n",
      "Epoch 7/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 44.8322 - val_loss: 55.5396\n",
      "Epoch 8/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 39.1554 - val_loss: 55.4412\n",
      "Epoch 9/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 41.5057 - val_loss: 59.0570\n",
      "Epoch 10/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 38.2299 - val_loss: 62.3045\n",
      "Epoch 11/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 39.9601 - val_loss: 63.4522\n",
      "Epoch 12/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 36.5424 - val_loss: 73.4039\n",
      "Epoch 13/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 47.2668 - val_loss: 74.6670\n",
      "Epoch 14/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 37.5826 - val_loss: 60.5511\n",
      "Epoch 15/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 38.2242 - val_loss: 66.4150\n",
      "Epoch 16/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 40.4720 - val_loss: 61.2739\n",
      "Epoch 17/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 36.9905 - val_loss: 63.5021\n",
      "Epoch 18/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 38.2971 - val_loss: 62.7236\n",
      "Epoch 19/33\n",
      "43/43 [==============================] - 1s 29ms/step - loss: 36.6473 - val_loss: 60.1377\n",
      "Epoch 20/33\n",
      "43/43 [==============================] - 1s 29ms/step - loss: 38.9968 - val_loss: 59.9688\n",
      "Epoch 21/33\n",
      "43/43 [==============================] - 1s 29ms/step - loss: 34.3045 - val_loss: 62.4011\n",
      "Epoch 22/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 37.2925 - val_loss: 64.7754\n",
      "Epoch 23/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 33.5878 - val_loss: 65.8540\n",
      "Epoch 24/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 34.8175 - val_loss: 66.2705\n",
      "Epoch 25/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 40.4028 - val_loss: 57.4241\n",
      "Epoch 26/33\n",
      "43/43 [==============================] - 1s 28ms/step - loss: 32.3534 - val_loss: 69.3427\n",
      "Epoch 27/33\n",
      "43/43 [==============================] - 1s 27ms/step - loss: 37.3793 - val_loss: 62.0348\n",
      "Epoch 28/33\n",
      "43/43 [==============================] - 1s 29ms/step - loss: 33.3734 - val_loss: 73.7854\n",
      "Epoch 29/33\n",
      "43/43 [==============================] - 1s 24ms/step - loss: 38.6530 - val_loss: 66.9729\n",
      "Epoch 30/33\n",
      "43/43 [==============================] - 1s 25ms/step - loss: 37.5746 - val_loss: 68.6082\n",
      "Epoch 31/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 32.8846 - val_loss: 66.7902\n",
      "Epoch 32/33\n",
      "43/43 [==============================] - 1s 23ms/step - loss: 36.6782 - val_loss: 75.5365\n",
      "Epoch 33/33\n",
      "43/43 [==============================] - 1s 26ms/step - loss: 36.9242 - val_loss: 69.8819\n",
      "\u001b[48;5;42m11 di 10: Ticker VST\u001b[0m\n",
      "Download dati ticker\n",
      "Calcolo indicatori ticker\n",
      "Definizione features e target\n",
      "Scaler\n",
      "Addestramento modello epochs=33, batch_size=26\n",
      "Epoch 1/33\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 95.6687 - val_loss: 81.8423\n",
      "Epoch 2/33\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 62.0225 - val_loss: 106.5464\n",
      "Epoch 3/33\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 90.1218 - val_loss: 77.6994\n",
      "Epoch 4/33\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 47.1171 - val_loss: 78.9204\n",
      "Epoch 5/33\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 61.3175 - val_loss: 70.5495\n",
      "Epoch 6/33\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 40.3286 - val_loss: 71.5483\n",
      "Epoch 7/33\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 48.7143 - val_loss: 71.8776\n",
      "Epoch 8/33\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 45.1328 - val_loss: 70.4936\n",
      "Epoch 9/33\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 46.8910 - val_loss: 68.4714\n",
      "Epoch 10/33\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 40.5982 - val_loss: 70.9196\n",
      "Epoch 11/33\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 38.5974 - val_loss: 72.5984\n",
      "Epoch 12/33\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 40.4496 - val_loss: 71.9152\n",
      "Epoch 13/33\n",
      "63/63 [==============================] - 1s 21ms/step - loss: 35.8659 - val_loss: 77.5603\n",
      "Epoch 14/33\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 40.3343 - val_loss: 70.7190\n",
      "Epoch 15/33\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 33.4775 - val_loss: 76.2579\n",
      "Epoch 16/33\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 37.7736 - val_loss: 77.3206\n",
      "Epoch 17/33\n",
      "63/63 [==============================] - 1s 20ms/step - loss: 34.4095 - val_loss: 78.9705\n",
      "Epoch 18/33\n",
      "63/63 [==============================] - 1s 19ms/step - loss: 36.8187 - val_loss: 80.0044\n",
      "Epoch 19/33\n",
      "63/63 [==============================] - 2s 24ms/step - loss: 33.4129 - val_loss: 76.1876\n",
      "Epoch 20/33\n",
      "63/63 [==============================] - 1s 22ms/step - loss: 37.4879 - val_loss: 72.1381\n",
      "Epoch 21/33\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 33.7907 - val_loss: 77.8555\n",
      "Epoch 22/33\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 38.3065 - val_loss: 80.0455\n",
      "Epoch 23/33\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 32.3851 - val_loss: 75.3934\n",
      "Epoch 24/33\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 34.4451 - val_loss: 78.5153\n",
      "Epoch 25/33\n",
      "63/63 [==============================] - 2s 27ms/step - loss: 35.5474 - val_loss: 86.4952\n",
      "Epoch 26/33\n",
      "63/63 [==============================] - 2s 25ms/step - loss: 36.2002 - val_loss: 81.6828\n",
      "Epoch 27/33\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 36.1410 - val_loss: 82.7467\n",
      "Epoch 28/33\n",
      "63/63 [==============================] - 2s 26ms/step - loss: 31.1950 - val_loss: 79.2670\n",
      "Epoch 29/33\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 36.7484 - val_loss: 90.1086\n",
      "Epoch 30/33\n",
      "63/63 [==============================] - 2s 30ms/step - loss: 34.0521 - val_loss: 82.6457\n",
      "Epoch 31/33\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 33.1697 - val_loss: 90.6663\n",
      "Epoch 32/33\n",
      "63/63 [==============================] - 1s 24ms/step - loss: 34.3138 - val_loss: 84.2357\n",
      "Epoch 33/33\n",
      "63/63 [==============================] - 1s 23ms/step - loss: 31.9771 - val_loss: 79.5881\n"
     ]
    }
   ],
   "source": [
    "print(\"Download dati ticker validazione\")\n",
    "ticker_val = yf.download(simbolo_validazione, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "ticker_val.index = ticker_val.index.date\n",
    "print(\"Calcolo indicatori ticker validazione\")\n",
    "ticker_val = fx.crea_indicatori(ticker_val)\n",
    "ticker_val.dropna(axis=0, inplace=True)\n",
    "\n",
    "for i_ticker in range (n_simboli_addestramento + 1):\n",
    "    nome_simbolo = lista_ticker[\"Ticker\"].iloc[i_ticker]\n",
    "    print(f\"\\033[48;5;42m{i_ticker+1} di {n_simboli_addestramento}: Ticker {nome_simbolo}\\033[0m\")\n",
    "    print(\"Download dati ticker\")\n",
    "    ticker = yf.download(nome_simbolo, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "    if ticker[\"Close\"].iloc[-1] >= 1:\n",
    "        ticker.index = ticker.index.date\n",
    "        print(\"Calcolo indicatori ticker\")\n",
    "        ticker = fx.crea_indicatori(ticker)\n",
    "        ticker.dropna(axis=0, inplace=True)\n",
    "\n",
    "        print(\"Definizione features e target\")\n",
    "        idx, X, Y = to_XY(ticker, n_timesteps, giorni_previsione, addestramento=True)\n",
    "        idx_val, X_val, Y_val = to_XY(ticker_val, n_timesteps, giorni_previsione, addestramento=True)\n",
    "\n",
    "        print(\"Scaler\")\n",
    "        X = X.reshape((-1, n_timesteps * n_features))\n",
    "        Y = Y.reshape((-1, giorni_previsione * n_targets))\n",
    "        X_val = X_val.reshape((-1, n_timesteps * n_features))\n",
    "        Y_val = Y_val.reshape((-1, giorni_previsione * n_targets))\n",
    "\n",
    "        #X_train, X_test, Y_train, Y_test, idx_train, idx_test = train_test_split(X, Y, idx, test_size=0.2, random_state=seed)\n",
    "        X_train = X\n",
    "        Y_train = Y\n",
    "\n",
    "        X_scaler = clone(scaler)\n",
    "        Y_scaler = clone(scaler)\n",
    "        X_scaler_val = clone(scaler)\n",
    "        Y_scaler_val = clone(scaler)\n",
    "        X_scaler.fit(X_train)\n",
    "        Y_scaler.fit(Y_train)\n",
    "        X_scaler_val.fit(X_val)\n",
    "        Y_scaler_val.fit(Y_val)\n",
    "        X_train = X_scaler.fit_transform(X_train)\n",
    "        #X_test = X_scaler.fit_transform(X_test)\n",
    "        Y_train = Y_scaler.fit_transform(Y_train)\n",
    "        #Y_test = Y_scaler.fit_transform(Y_test)\n",
    "        X_val = X_scaler_val.fit_transform(X_val)\n",
    "        Y_val = Y_scaler_val.fit_transform(Y_val)\n",
    "\n",
    "        X_train = X_train.reshape((-1, n_timesteps, n_features))\n",
    "        X_val = X_val.reshape((-1, n_timesteps, n_features))\n",
    "        #X_test = X_test.reshape((-1, n_timesteps, n_features))\n",
    "        Y_train = Y_train.reshape((-1, giorni_previsione, n_targets))\n",
    "        Y_val = Y_val.reshape((-1, giorni_previsione, n_targets))\n",
    "        #Y_test = Y_test.reshape((-1, giorni_previsione, n_targets))\n",
    "\n",
    "        print(f\"Addestramento modello epochs={epochs}, batch_size={batch_size}\")\n",
    "        model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val))\n",
    "        model.save(\"LSTM.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7dc8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download dati ticker per previsione\n",
      "108/108 [==============================] - 3s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Download dati ticker per previsione\")\n",
    "dati_previsione = yf.download(simbolo_test, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "dati_previsione.index = dati_previsione.index.date\n",
    "dati_previsione = fx.crea_indicatori(dati_previsione)\n",
    "dati_previsione.iloc[:100] = dati_previsione.iloc[:100].dropna(axis=0)\n",
    "\n",
    "indice_prev, X_prev, Y_prev = to_XY(dati_previsione, n_timesteps, giorni_previsione, addestramento=False)\n",
    "\n",
    "X_prev = X_prev.reshape(-1, n_timesteps * n_features)\n",
    "Y_prev = Y_prev.reshape(-1, giorni_previsione * n_targets)\n",
    "X_scaler_prev = clone(scaler)\n",
    "Y_scaler_prev = clone(scaler)\n",
    "X_prev = X_scaler_prev.fit_transform(X_prev)\n",
    "Y_prev = Y_scaler_prev.fit_transform(Y_prev)\n",
    "X_prev = X_prev.reshape(-1, n_timesteps, n_features)\n",
    "Y_prev = Y_prev.reshape(-1, giorni_previsione, n_targets)\n",
    "\n",
    "pred = model.predict(X_prev)\n",
    "#pred_loss, pred_mae = model.evaluate(X_prev, Y_prev)\n",
    "\n",
    "X_prev = X_prev.reshape(-1, n_timesteps * n_features)\n",
    "X_prev = X_scaler_prev.inverse_transform(X_prev)\n",
    "X_prev = X_prev.reshape(-1, n_timesteps, n_features)\n",
    "\n",
    "pred = pred.reshape(-1, giorni_previsione * n_targets)\n",
    "pred = Y_scaler_prev.inverse_transform(pred)\n",
    "pred = pred.reshape(-1, giorni_previsione, n_targets)\n",
    "\n",
    "col_analisi = \"EMA_5\"\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.subplots as sp\n",
    "import yfinance as yf\n",
    "\n",
    "def grafico1(dati_previsione, pred):\n",
    "    dati_previsione = dati_previsione.loc[indice_prev]\n",
    "    new_dates = pd.bdate_range(start=dati_previsione.index[-1] + pd.Timedelta(days=1), periods=giorni_previsione)\n",
    "    df_new = pd.DataFrame(index=new_dates.date)\n",
    "    dati_previsione = pd.concat([dati_previsione, df_new])\n",
    "    risultato = pd.DataFrame({\"Previsione\": pred[:, giorni_previsione-1, col_targets[col_analisi]].round(2)}, index=dati_previsione.index[giorni_previsione:])\n",
    "    risultato = pd.concat([dati_previsione[col_analisi], risultato], axis=1)\n",
    "    \n",
    "    target = go.Scatter(\n",
    "        x = risultato.index,\n",
    "        y = risultato[col_analisi],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, .9)'),\n",
    "        name = col_analisi\n",
    "    )\n",
    "\n",
    "    previsione = go.Scatter(\n",
    "        x = risultato.index,\n",
    "        y = risultato['Previsione'],\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 250, .9)'),\n",
    "        name = 'Previsione'\n",
    "    )\n",
    "    '''\n",
    "    err_meno = go.Scatter(\n",
    "        x = risultato.index,\n",
    "        y = risultato['Previsione'] - err, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0, 0, 250, .9)',\n",
    "            width=1\n",
    "        ),\n",
    "        connectgaps = False,\n",
    "        name = 'err-'\n",
    "    )                \n",
    "    err_piu = go.Scatter(\n",
    "        x = risultato.index,\n",
    "        y = risultato['Previsione'] + err, \n",
    "        mode = 'lines',\n",
    "        line = dict(\n",
    "            color='rgba(0, 0, 250, .9)',\n",
    "            width=1\n",
    "        ),\n",
    "        fill='tonexty',\n",
    "        fillcolor='rgba(0, 0, 250, .2)', # Puoi cambiare il valore di alpha per regolare la trasparenza\n",
    "        connectgaps = False,\n",
    "        name = 'err+'\n",
    "    )\n",
    "    '''\n",
    "    layout = dict(xaxis = dict(autorange=True),\n",
    "                  yaxis = dict(title = 'Previsioni', autorange=True),\n",
    "                  autosize = True,\n",
    "                  margin = go.layout.Margin(\n",
    "                      l=0,  # Sinistra\n",
    "                      r=0,  # Destra\n",
    "                      b=0,  # Basso\n",
    "                      t=50,  # Alto\n",
    "                      pad=0  # Padding\n",
    "                  ),\n",
    "                  legend = dict(traceorder = 'normal', bordercolor = 'black')\n",
    "    )\n",
    "    fig = sp.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    fig.update_layout(layout)\n",
    "    fig.add_trace(target, row=1, col=1)\n",
    "    fig.add_trace(previsione, row=1, col=1)\n",
    "    #fig.add_trace(err_meno, row=1, col=1)\n",
    "    #fig.add_trace(err_piu, row=1, col=1)\n",
    "    pyo.plot(fig, filename=\"grafico1.html\", auto_open=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3527f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafico2(dati_previsione, pred):\n",
    "    dati2 = dati_previsione.dropna(axis=0).iloc[-60:]\n",
    "    new_dates = pd.bdate_range(start=dati2.index[-1] + pd.Timedelta(days=1), periods=giorni_previsione)\n",
    "    df_new = pd.DataFrame(index=new_dates.date)\n",
    "\n",
    "    val_prev2 = (pred[-1, i, col_targets[col_analisi]] for i in range(giorni_previsione))\n",
    "    prev2 = pd.DataFrame(val_prev2, columns=[\"Previsione\"], index=df_new.index)\n",
    "    riga_iniziale = pd.DataFrame({\"Previsione\": [dati2[col_analisi].iloc[-1]]}, index=[dati2.index[-1]])\n",
    "    prev2 = pd.concat([riga_iniziale, prev2], axis=0)\n",
    "    \n",
    "    target = go.Scatter(\n",
    "        x = dati2.index,\n",
    "        y = dati2[col_analisi].round(2),\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 0, .9)'),\n",
    "        name = col_analisi\n",
    "    )\n",
    "\n",
    "    previsione = go.Scatter(\n",
    "        x = prev2.index,\n",
    "        y = prev2['Previsione'].round(2),\n",
    "        mode = 'lines',\n",
    "        line = dict(color='rgba(0, 0, 250, .9)'),\n",
    "        name = 'Previsione'\n",
    "    )\n",
    "    layout = dict(xaxis = dict(autorange=True),\n",
    "                  yaxis = dict(title = 'Previsioni', autorange=True),\n",
    "                  autosize = True,\n",
    "                  margin = go.layout.Margin(\n",
    "                      l=0,  # Sinistra\n",
    "                      r=0,  # Destra\n",
    "                      b=0,  # Basso\n",
    "                      t=50,  # Alto\n",
    "                      pad=0  # Padding\n",
    "                  ),\n",
    "                  legend = dict(traceorder = 'normal', bordercolor = 'black')\n",
    "    )\n",
    "    fig = sp.make_subplots(rows=1, cols=1, shared_xaxes=True)\n",
    "    fig.update_layout(layout)\n",
    "    fig.add_trace(target, row=1, col=1)\n",
    "    fig.add_trace(previsione, row=1, col=1)\n",
    "    pyo.plot(fig, filename=\"grafico2.html\", auto_open=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8e3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafico1(dati_previsione, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74dc888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafico2(dati_previsione, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb838c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
