{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ebdd33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:22:47.900636Z",
     "iopub.status.busy": "2023-10-13T15:22:47.899643Z",
     "iopub.status.idle": "2023-10-13T15:22:51.363045Z",
     "shell.execute_reply": "2023-10-13T15:22:51.361927Z",
     "shell.execute_reply.started": "2023-10-13T15:22:47.900579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importa librerieUsing TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................=\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"Importa librerie\", end=\"\", flush=True)\n",
    "from keras_tuner import HyperModel, RandomSearch, Hyperband, BayesianOptimization\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, Dropout, TimeDistributed, Dense\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.models import Sequential\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.regularizers import l2\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras import metrics\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras import backend as K\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from tensorflow.keras.models import load_model\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from keras.callbacks import Callback\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from sklearn.metrics import f1_score\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from keras_tuner import Objective\n",
    "#import keras_tuner\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import pandas as pd\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import yfinance as yf\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import numpy as np\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import tensorflow as tf\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import funzioni as fx\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "print(\".\", end=\"\", flush=True)\n",
    "from sklearn.base import clone\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import pickle\n",
    "print(\".\", end=\"\", flush=True)\n",
    "import gc\n",
    "print(\".\", end=\"\", flush=True)\n",
    "gc.enable()\n",
    "import os\n",
    "print(\"=\", end=\"\\n\", flush=True)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07d842bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:22:51.365894Z",
     "iopub.status.busy": "2023-10-13T15:22:51.365245Z",
     "iopub.status.idle": "2023-10-13T15:50:40.741887Z",
     "shell.execute_reply": "2023-10-13T15:50:40.740591Z",
     "shell.execute_reply.started": "2023-10-13T15:22:51.365873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 Complete [00h 00m 01s]\n",
      "\n",
      "Best val_precision So Far: 0.9375\n",
      "Total elapsed time: 02h 06m 23s\n",
      "\n",
      "Search: Running Trial #37\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "500               |400               |lstm_units_1\n",
      "1                 |2                 |num_lstm_layers\n",
      "350               |300               |lstm_units_2\n",
      "0.335             |0.306             |l2_rate_2\n",
      "0                 |0.3               |lstm_dropout_2\n",
      "4                 |2                 |num_dense_layers\n",
      "250               |400               |dense_units_0\n",
      "0.3               |0.2               |dense_dropout_0\n",
      "400               |300               |dense_units_1\n",
      "0.2               |0.2               |dense_dropout_1\n",
      "500               |300               |dense_units_2\n",
      "0.1               |0                 |dense_dropout_2\n",
      "400               |350               |lstm_units_3\n",
      "1.711             |3.039             |l2_rate_3\n",
      "0.4               |0.1               |lstm_dropout_3\n",
      "150               |200               |lstm_units_4\n",
      "1.894             |2.361             |l2_rate_4\n",
      "0                 |0                 |lstm_dropout_4\n",
      "300               |100               |dense_units_3\n",
      "0.3               |0.2               |dense_dropout_3\n",
      "50                |None              |lstm_units_5\n",
      "4.607             |None              |l2_rate_5\n",
      "0.3               |None              |lstm_dropout_5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n",
      "    model = self._try_build(hp)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"/tmp/ipykernel_22488/690547774.py\", line 201, in build\n",
      "    model.add(LSTM(hp.Int('lstm_units_1', 50, 500, step=50),\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/training/tracking/base.py\", line 587, in _method_wrapper\n",
      "    result = method(self, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/keras/initializers/initializers_v2.py\", line 673, in _generate_init_val\n",
      "    q, r = tf.linalg.qr(a, full_matrices=False)\n",
      "tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Qr: stream did not block host until done; was already in an error state [Op:Qr]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/tmp/ipykernel_22488/690547774.py\", line 201, in build\n    model.add(LSTM(hp.Int('lstm_units_1', 50, 500, step=50),\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/training/tracking/base.py\", line 587, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.9/dist-packages/keras/initializers/initializers_v2.py\", line 673, in _generate_init_val\n    q, r = tf.linalg.qr(a, full_matrices=False)\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Qr: stream did not block host until done; was already in an error state [Op:Qr]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 353\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    352\u001b[0m hypermodel \u001b[38;5;241m=\u001b[39m MixedHyperModel(n_timesteps, n_features, n_targets, giorni_previsione)\n\u001b[0;32m--> 353\u001b[0m model, par, bayesian_search \u001b[38;5;241m=\u001b[39m \u001b[43mbay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypermodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m trials \u001b[38;5;241m=\u001b[39m bayesian_search\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mtrials\n\u001b[1;32m    357\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [2], line 255\u001b[0m, in \u001b[0;36mbay\u001b[0;34m(hypermodel, X_train, Y_train, X_val, Y_val)\u001b[0m\n\u001b[1;32m    245\u001b[0m bayesian_tuner \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m    246\u001b[0m     hypermodel,\n\u001b[1;32m    247\u001b[0m     objective\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbayesian_tuning\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m )\n\u001b[1;32m    253\u001b[0m bayesian_tuner\u001b[38;5;241m.\u001b[39msearch_space_summary()    \n\u001b[0;32m--> 255\u001b[0m \u001b[43mbayesian_tuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m best_model_bayesian \u001b[38;5;241m=\u001b[39m bayesian_tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    258\u001b[0m best_hyperparameters_bayesian \u001b[38;5;241m=\u001b[39m bayesian_tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_trial_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py:338\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moracle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/oracle.py:577\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_consecutive_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/oracle.py:534\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    538\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n    model = self._try_build(hp)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n    model = self._build_hypermodel(hp)\n  File \"/usr/local/lib/python3.9/dist-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n    model = self.hypermodel.build(hp)\n  File \"/tmp/ipykernel_22488/690547774.py\", line 201, in build\n    model.add(LSTM(hp.Int('lstm_units_1', 50, 500, step=50),\n  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/training/tracking/base.py\", line 587, in _method_wrapper\n    result = method(self, *args, **kwargs)\n  File \"/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.9/dist-packages/keras/initializers/initializers_v2.py\", line 673, in _generate_init_val\n    q, r = tf.linalg.qr(a, full_matrices=False)\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Qr: stream did not block host until done; was already in an error state [Op:Qr]\n"
     ]
    }
   ],
   "source": [
    "simbolo_test = \"BTG\"\n",
    "simbolo_validazione = \"DHT\"\n",
    "n_simboli_addestramento = \"Tutti\"\n",
    "epochs=50\n",
    "batch_size=10000\n",
    "n_timesteps = 60 # n. barre del periodo passato per la ricerca di pattern, inclusa ultima data disponibile\n",
    "giorni_previsione = 1 # n. barre nel futuro di cui si desidera prevedere il prezzo\n",
    "set_file_x_y = f\"_{n_simboli_addestramento}\"\n",
    "initial_lr = 0.001\n",
    "\n",
    "features_prezzo = [\n",
    "    \"Close\",\n",
    "#    \"EMA_5\", \n",
    "    \"EMA_20\", \n",
    "    \"EMA_50\",\n",
    "    \"Open\",  \n",
    "    \"High\",\n",
    "    \"Low\",\n",
    "    \"PSAR\",\n",
    "    \"SUPERT\", \n",
    "]\n",
    "\n",
    "features_da_scalare_singolarmente = [\n",
    "    \"Volume\",\n",
    "    \"ATR\",\n",
    "    \"PSARaf\",\n",
    "    \"ADX\",\n",
    "]\n",
    "\n",
    "features_meno_piu = [\n",
    "    \"MACDh\",    \n",
    "    \"AROONOSC\",\n",
    "    \"TRIX\",\n",
    "    \"TRIXs\",\n",
    "    \"DM_OSC\",\n",
    "]\n",
    "\n",
    "features_no_scala = [\n",
    "    \"SUPERTd\",  \n",
    "    \"PSARr\"\n",
    "]\n",
    "\n",
    "features_candele = [\n",
    "#    \"CDL_2CROWS\", \"CDL_3BLACKCROWS\", \"CDL_3INSIDE\", \"CDL_3LINESTRIKE\", \"CDL_3OUTSIDE\", \"CDL_3STARSINSOUTH\", \"CDL_3WHITESOLDIERS\", \"CDL_ABANDONEDBABY\", \"CDL_ADVANCEBLOCK\", \"CDL_BELTHOLD\", \"CDL_BREAKAWAY\", \"CDL_CLOSINGMARUBOZU\", \"CDL_CONCEALBABYSWALL\", \"CDL_COUNTERATTACK\", \"CDL_DARKCLOUDCOVER\", \"CDL_DOJI_10_0.1\", \"CDL_DOJISTAR\", \"CDL_DRAGONFLYDOJI\", \"CDL_ENGULFING\", \"CDL_EVENINGDOJISTAR\", \"CDL_EVENINGSTAR\", \"CDL_GAPSIDESIDEWHITE\", \"CDL_GRAVESTONEDOJI\", \"CDL_HAMMER\", \"CDL_HANGINGMAN\", \"CDL_HARAMI\", \"CDL_HARAMICROSS\", \"CDL_HIGHWAVE\", \"CDL_HIKKAKE\", \"CDL_HIKKAKEMOD\", \"CDL_HOMINGPIGEON\", \"CDL_IDENTICAL3CROWS\", \"CDL_INNECK\", \"CDL_INSIDE\", \"CDL_INVERTEDHAMMER\", \"CDL_KICKING\", \"CDL_KICKINGBYLENGTH\", \"CDL_LADDERBOTTOM\", \"CDL_LONGLEGGEDDOJI\", \"CDL_LONGLINE\", \"CDL_MARUBOZU\", \"CDL_MATCHINGLOW\", \"CDL_MATHOLD\", \"CDL_MORNINGDOJISTAR\", \"CDL_MORNINGSTAR\", \"CDL_ONNECK\", \"CDL_PIERCING\", \"CDL_RICKSHAWMAN\", \"CDL_RISEFALL3METHODS\", \"CDL_SEPARATINGLINES\", \"CDL_SHOOTINGSTAR\", \"CDL_SHORTLINE\", \"CDL_SPINNINGTOP\", \"CDL_STALLEDPATTERN\", \"CDL_STICKSANDWICH\", \"CDL_TAKURI\", \"CDL_TASUKIGAP\", \"CDL_THRUSTING\", \"CDL_TRISTAR\", \"CDL_UNIQUE3RIVER\", \"CDL_UPSIDEGAP2CROWS\", \"CDL_XSIDEGAP3METHODS\",\n",
    "]\n",
    "\n",
    "elenco_targets = [\n",
    "#    \"EMA_5\",\n",
    "#    \"EMA_20\", \n",
    "#    \"EMA_50\",\n",
    "    #\"Open\",\n",
    "    #\"High\",\n",
    "    #\"Low\",\n",
    "    \"Target\"\n",
    "]\n",
    "\n",
    "col_features_prezzo = {col: idx for idx, col in enumerate(features_prezzo)}\n",
    "col_features_da_scalare_singolarmente = {col: idx for idx, col in enumerate(features_da_scalare_singolarmente)}\n",
    "col_features_meno_piu = {col: idx for idx, col in enumerate(features_meno_piu)}\n",
    "col_features_no_scala = {col: idx for idx, col in enumerate(features_no_scala)}\n",
    "col_features_candele = {col: idx for idx, col in enumerate(features_candele)}\n",
    "col_targets = {col: idx for idx, col in enumerate(elenco_targets)}\n",
    "n_features = len(col_features_prezzo) + len(col_features_da_scalare_singolarmente) + len(col_features_meno_piu) + len(col_features_no_scala) + len(col_features_candele) \n",
    "n_targets = len(col_targets) \n",
    "\n",
    "def set_di_tuning(lista_ticker):\n",
    "    if os.path.exists(\"dati/indice.npy\"):    \n",
    "        inizio = np.load(\"dati/indice.npy\")\n",
    "        print(inizio)\n",
    "    else:\n",
    "        inizio = 0\n",
    "\n",
    "    if os.path.exists(f\"dati/X{set_file_x_y}.npy\") and os.path.exists(f\"dati/Y{set_file_x_y}.npy\"):\n",
    "        print(\"Caricamento X e Y\")\n",
    "        X = np.load(f'dati/X{set_file_x_y}.npy')\n",
    "        Y = np.load(f'dati/Y{set_file_x_y}.npy')\n",
    "    else:\n",
    "        X = np.zeros((0, n_timesteps, n_features))\n",
    "        #Y = np.zeros((0, giorni_previsione, n_targets)) #togliere in caso di classificazione binaria\n",
    "        Y = np.zeros((0, 1)) #solo per classificazione binaria\n",
    "    if inizio < n_simboli_addestramento:\n",
    "        lista_x, lista_y = [], []\n",
    "        for i_ticker in range (inizio, n_simboli_addestramento):\n",
    "            nome_simbolo = lista_ticker[\"Ticker\"].iloc[i_ticker]\n",
    "            print(f\"\\033[48;5;42m{i_ticker+1} di {n_simboli_addestramento}: Ticker {nome_simbolo}\\033[0m\")\n",
    "            print(\"Download dati ticker\", flush=True)\n",
    "            try:\n",
    "                ticker = yf.download(nome_simbolo, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "                if ticker[\"Close\"].iloc[-1] >= 1:\n",
    "                    ticker.index = ticker.index.date\n",
    "                    print(\"Calcolo indicatori ticker\", flush=True)\n",
    "                    ticker = fx.crea_indicatori(ticker)\n",
    "                    ticker.dropna(axis=0, inplace=True)\n",
    "\n",
    "                    print(\"Definizione features e target\", flush=True)\n",
    "                    _, X_train, Y_train, _ = fx.to_XY(ticker, features_prezzo, features_da_scalare_singolarmente, features_meno_piu, features_candele, features_no_scala, elenco_targets, n_timesteps, giorni_previsione, addestramento=True)\n",
    "\n",
    "                    print(\"Aggiunta dati a liste X e Y\", flush=True)\n",
    "                    lista_x.append(X_train)  \n",
    "                    print(X_train.shape)      \n",
    "                    lista_y.append(Y_train)   \n",
    "                    print(Y_train.shape)\n",
    "                    if ((i_ticker + 1) % 100 == 0):\n",
    "                        print(\"Concatenamento su X, Y\", flush=True)\n",
    "                        X_arr = np.concatenate(lista_x, axis=0)\n",
    "                        Y_arr = np.concatenate(lista_y, axis=0)\n",
    "                        X = np.concatenate((X, X_arr), axis=0)\n",
    "                        Y = np.concatenate((Y, Y_arr), axis=0)\n",
    "                        print(\"Salvataggio X e Y su file\", flush=True)\n",
    "                        np.save(f'dati/X{set_file_x_y}', X)\n",
    "                        np.save(f'dati/Y{set_file_x_y}', Y)\n",
    "                        np.save(\"dati/indice\", i_ticker)\n",
    "                        lista_x, lista_y = [], []\n",
    "                    gc.collect()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                gc.collect()\n",
    "                continue\n",
    "                \n",
    "        print(\"Concatenamento su X, Y\", flush=True)\n",
    "        X_arr = np.vstack(lista_x)\n",
    "        Y_arr = np.vstack(lista_y)\n",
    "        X = np.vstack((X, X_arr))\n",
    "        Y = np.vstack((Y, Y_arr))\n",
    "        print(\"Salvataggio X e Y su file\", flush=True)\n",
    "        np.save(f'dati/X{set_file_x_y}', X)\n",
    "        np.save(f'dati/Y{set_file_x_y}', Y)\n",
    "        np.save(\"dati/indice\", i_ticker)\n",
    "        lista_x, lista_y = [], []\n",
    "\n",
    "        gc.collect()\n",
    "    return X, Y\n",
    "        \n",
    "def addestramento(lista_ticker):\n",
    "    model = load_model(\"tuning.keras\")\n",
    "    \n",
    "    if os.path.exists(\"dati/indice_addestramento.npy\"):    \n",
    "        inizio = np.load(\"dati/indice_addestramento.npy\")\n",
    "    else:\n",
    "        inizio = 0\n",
    "    print(\"ok\")\n",
    "    X = np.zeros((0, n_timesteps, n_features))\n",
    "    #Y = np.zeros((0, giorni_previsione, n_targets)) #togliere in caso di classificazione binaria\n",
    "    Y = np.zeros((0, 1)) #solo per classificazione binaria\n",
    "    if inizio < n_simboli_addestramento:\n",
    "        for i_ticker in range (inizio, n_simboli_addestramento):\n",
    "            nome_simbolo = lista_ticker[\"Ticker\"].iloc[i_ticker]\n",
    "            print(f\"\\033[48;5;42m{i_ticker+1} di {n_simboli_addestramento}: Ticker {nome_simbolo}\\033[0m\")\n",
    "            print(\"Download dati ticker\", flush=True)\n",
    "            try:\n",
    "                ticker = yf.download(nome_simbolo, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "                if ticker[\"Close\"].iloc[-1] >= 1:\n",
    "                    ticker.index = ticker.index.date\n",
    "                    print(\"Calcolo indicatori ticker\", flush=True)\n",
    "                    ticker = fx.crea_indicatori(ticker)\n",
    "                    ticker.dropna(axis=0, inplace=True)\n",
    "\n",
    "                    print(\"Definizione features e target\", flush=True)\n",
    "                    _, X_train, Y_train, _ = fx.to_XY(ticker, features_prezzo, features_da_scalare_singolarmente, features_meno_piu, features_candele, features_no_scala, elenco_targets, n_timesteps, giorni_previsione, addestramento=True)\n",
    "\n",
    "                    print(f\"Addestramento simbolo {nome_simbolo}\", flush=True)\n",
    "                    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val))\n",
    "                    np.save(\"dati/indice_addestramento\", i_ticker)\n",
    "                    model.save('addestramento.keras')\n",
    "\n",
    "                    gc.collect()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "def weighted_binary_crossentropy(pos_weight):\n",
    "    def loss(y_true, y_pred):\n",
    "        logloss = K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "        return logloss * (1 + (pos_weight - 1) * y_true)\n",
    "    return loss\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    true_positives = tf.cast(true_positives, dtype=tf.float64)\n",
    "    predicted_positives = tf.cast(predicted_positives, dtype=tf.float64)\n",
    "    possible_positives = tf.cast(possible_positives, dtype=tf.float64)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "class MixedHyperModel(HyperModel):\n",
    "    def __init__(self, n_timesteps, n_features, n_targets, giorni_previsione):\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "        self.n_targets = n_targets\n",
    "        self.giorni_previsione = giorni_previsione\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Layer LSTM iniziale\n",
    "        model.add(LSTM(hp.Int('lstm_units_1', 50, 500, step=50),\n",
    "                       input_shape=(self.n_timesteps, self.n_features), kernel_initializer='glorot_uniform'))\n",
    "        model.add(RepeatVector(self.giorni_previsione))\n",
    "\n",
    "        # Aggiunta di layer LSTM intermedi\n",
    "        for i in range(hp.Int('num_lstm_layers', 1, 4)):\n",
    "            model.add(LSTM(hp.Int(f'lstm_units_{i+2}', 50, 500, step=50), return_sequences=True,\n",
    "                           kernel_regularizer=l2(hp.Float(f'l2_rate_{i+2}', 0.000001, 5, step=0.001)), kernel_initializer='glorot_uniform'))\n",
    "            model.add(Dropout(hp.Float(f'lstm_dropout_{i+2}', 0, 0.5, step=0.1)))\n",
    "\n",
    "        # Aggiunta di layer Dense\n",
    "        for i in range(hp.Int('num_dense_layers', 1, 4)):\n",
    "            model.add(TimeDistributed(Dense(hp.Int(f'dense_units_{i}', 50, 500, step=50), activation='relu', kernel_initializer='glorot_uniform')))\n",
    "            model.add(Dropout(hp.Float(f'dense_dropout_{i}', 0, 0.5, step=0.1)))\n",
    "\n",
    "        model.add(TimeDistributed(Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform')))\n",
    "\n",
    "        model.compile(optimizer=\"adam\", \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', \n",
    "                       metrics.Precision(name='precision'), \n",
    "                       metrics.Recall(name='recall'), \n",
    "                       metrics.AUC(name='auc')])\n",
    "        \n",
    "        return model\n",
    "\n",
    "input_shape = (n_timesteps, n_features)\n",
    "output_shape = (giorni_previsione, n_targets)\n",
    "hypermodel = MixedHyperModel(n_timesteps, n_features, n_targets, giorni_previsione)\n",
    "\n",
    "class F1Metric(Callback):\n",
    "    def __init__(self, validation_data): \n",
    "        self.validation_data = validation_data\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n",
    "        val_targ = self.validation_data[1]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        logs['val_f1'] = _val_f1\n",
    "\n",
    "def custom_objective(trial):\n",
    "    val_precision = trial.metrics.get('val_precision')\n",
    "    return val_precision\n",
    "\n",
    "def bay(hypermodel, X_train, Y_train, X_val, Y_val):\n",
    "    bayesian_tuner = BayesianOptimization(\n",
    "        hypermodel,\n",
    "        objective=objective,\n",
    "        max_trials=50,\n",
    "        directory='bayesian_search',\n",
    "        project_name='bayesian_tuning'\n",
    "    )\n",
    "    \n",
    "    bayesian_tuner.search_space_summary()    \n",
    "\n",
    "    bayesian_tuner.search(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=callbacks)\n",
    "    \n",
    "    best_model_bayesian = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters_bayesian = bayesian_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Migliori iperparametri per Bayesian Optimization:\", best_hyperparameters_bayesian.values)\n",
    "    \n",
    "    return bayesian_tuner, best_model_bayesian, best_hyperparameters_bayesian\n",
    "\n",
    "def hb(hypermodel, X_train, Y_train, X_val, Y_val):\n",
    "    hyperband_tuner = Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_loss',\n",
    "        max_trials=50,\n",
    "        directory='hyperband_search',\n",
    "        project_name='hyperband_tuning'\n",
    "    )\n",
    "    \n",
    "    hyperband_tuner.search_space_summary()\n",
    "    \n",
    "    hyperband_tuner.search(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    best_model_hyperband = hyperband_tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters_hyperband = hyperband_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Migliori iperparametri per Hyperband:\", best_hyperparameters_hyperband.values)\n",
    "    \n",
    "    return best_model_hyperband, best_hyperparameters_hyperband\n",
    "\n",
    "def rd(hypermodel, X_train, Y_train, X_val, Y_val):\n",
    "    random_tuner = RandomSearch(\n",
    "        hypermodel,\n",
    "        objective='val_loss',\n",
    "        max_trials=50,\n",
    "        executions_per_trial=2,\n",
    "        directory='random_search',\n",
    "        project_name='random_tuning'\n",
    "    )\n",
    "    random_tuner.search_space_summary()\n",
    "    \n",
    "    random_tuner.search(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=[early_stopping, reduce_lr])\n",
    "    # Ottieni il miglior modello e i migliori iperparametri\n",
    "    best_model_random = random_tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters_random = random_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Migliori iperparametri per Random Search:\", best_hyperparameters_random.values)\n",
    "    \n",
    "    return best_model_random, best_hyperparameters_random\n",
    "\n",
    "if not os.path.exists('dati'):\n",
    "    os.makedirs('dati')\n",
    "\n",
    "if os.path.exists(\"lista_ticker.parquet\"):\n",
    "    print(\"Caricamento lista_ticker esistente\", flush=True)\n",
    "    lista_ticker = pd.read_parquet(\"lista_ticker.parquet\")\n",
    "else:\n",
    "    print(\"Download lista ticker\", flush=True)\n",
    "    lista_ticker = pd.read_parquet(\"Tickers_De_Giro.parquet\")\n",
    "    lista_ticker = lista_ticker.sample(frac=1).reset_index(drop=True)\n",
    "    lista_ticker = lista_ticker.loc[(lista_ticker[\"Ticker\"] != simbolo_test) & (lista_ticker[\"Ticker\"] != simbolo_validazione) & (lista_ticker[\"Categoria\"] != \"D\"), :]\n",
    "    lista_ticker.to_parquet(\"lista_ticker.parquet\")\n",
    "\n",
    "if n_simboli_addestramento == 'Tutti': \n",
    "    n_simboli_addestramento = len(lista_ticker)\n",
    "\n",
    "if os.path.exists(\"dati/ticker_val.parquet\"):\n",
    "    print(\"Caricamento dati ticker validazione\", flush=True)\n",
    "    ticker_val = pd.read_parquet(\"dati/ticker_val.parquet\")\n",
    "else:\n",
    "    print(\"Download dati ticker validazione\")\n",
    "    ticker_val = yf.download(simbolo_validazione, start='2010-01-01', end='2023-12-31', progress=False)\n",
    "    ticker_val.index = ticker_val.index.date\n",
    "    print(\"Calcolo indicatori ticker validazione\")\n",
    "    ticker_val = fx.crea_indicatori(ticker_val)\n",
    "    ticker_val.dropna(axis=0, inplace=True)\n",
    "    ticker_val.to_parquet(\"dati/ticker_val.parquet\")\n",
    "\n",
    "if os.path.exists(\"dati/X_val.parquet\") and os.path.exists(\"dati/Y_val.parquet\"):\n",
    "    print(\"Caricamento features e target ticker validazione\", flush=True)\n",
    "    X_val = np.load('dati/X_val.npy')\n",
    "    Y_val = np.load('dati/Y_val.npy')\n",
    "else:\n",
    "    print(\"Definizione features e target ticker validazione\", flush=True)\n",
    "    _, X_val, Y_val, _ = fx.to_XY(ticker_val, features_prezzo, features_da_scalare_singolarmente, features_meno_piu, features_candele, features_no_scala, elenco_targets, n_timesteps, giorni_previsione, addestramento=True)\n",
    "    np.save(f'dati/X_val', X_val)\n",
    "    np.save(f'dati/Y_val', Y_val)\n",
    "\n",
    "objective = Objective(\"val_precision\", direction=\"max\")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "f1_metric = F1Metric(validation_data=(X_val, Y_val))\n",
    "callbacks = [early_stopping, reduce_lr, f1_metric]\n",
    "\n",
    "X, Y = set_di_tuning(lista_ticker)\n",
    "\n",
    "print(\"Tuning\")\n",
    "hypermodel = MixedHyperModel(n_timesteps, n_features, n_targets, giorni_previsione)\n",
    "model, par, bayesian_search = bay(hypermodel, X, Y, X_val, Y_val)\n",
    "\n",
    "trials = bayesian_search.oracle.trials\n",
    "\n",
    "results = []\n",
    "\n",
    "for trial in trials.values():\n",
    "    trial_metrics = trial['metrics']\n",
    "    val_f1 = trial_metrics['val_f1']['observations'][0]['value'] \n",
    "    results.append({\n",
    "        'trial_id': trial['trial_id'],\n",
    "        'val_f1': val_f1,  \n",
    "        'hyperparameters': trial['hyperparameters'].values\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_excel('results_df.xlsx')\n",
    "    \n",
    "model.save(\"tuning\")\n",
    "with open('hyperparameters.pkl', 'wb') as f:\n",
    "    pickle.dump(par, f)\n",
    "\n",
    "#addestramento(lista_ticker)\n",
    "#    \n",
    "print(\"ok\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
