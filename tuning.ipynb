{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedHyperModel(HyperModel):\n",
    "    def __init__(self, n_timesteps, n_features):\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Layer LSTM iniziale\n",
    "        model.add(LSTM(hp.Int('lstm_units_1', 50, 500, step=50), return_sequences=True,\n",
    "                       input_shape=(self.n_timesteps, self.n_features), kernel_initializer='glorot_uniform'), unroll=False)\n",
    "\n",
    "        # Aggiunta di layer LSTM intermedi\n",
    "        for i in range(hp.Int('num_lstm_layers', 1, 4)):\n",
    "            model.add(LSTM(hp.Int(f'lstm_units_{i+2}', 50, 500, step=50), return_sequences=True,\n",
    "                           kernel_regularizer=l2(hp.Float(f'l2_rate_{i+2}', 0.000001, 5, step=0.001)), kernel_initializer='glorot_uniform'), unroll=False)\n",
    "            model.add(Dropout(hp.Float(f'lstm_dropout_{i+2}', 0, 0.5, step=0.1)))\n",
    "\n",
    "        # Ultimo layer LSTM prima del layer Dense\n",
    "        model.add(LSTM(hp.Int(f'lstm_units_{i+2}', 50, 500, step=50), return_sequences=False,\n",
    "                                       kernel_regularizer=l2(hp.Float(f'l2_rate_{i+2}', 0.000001, 5, step=0.001)), kernel_initializer='glorot_uniform'), unroll=False)\n",
    "\n",
    "        # Aggiunta di layer Dense\n",
    "        for i in range(hp.Int('num_dense_layers', 1, 4)):\n",
    "            model.add(Dense(hp.Int(f'dense_units_{i}', 50, 500, step=50), activation='relu', kernel_initializer='glorot_uniform'))\n",
    "            model.add(Dropout(hp.Float(f'dense_dropout_{i}', 0, 0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform'))\n",
    "\n",
    "        model.compile(optimizer=\"adam\", \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy', \n",
    "                       Precision(name='precision'), \n",
    "                       Recall(name='recall'), \n",
    "                       AUC(name='auc'),\n",
    "                       tf.keras.metrics.F1Score(threshold=0.5)])\n",
    "\n",
    "        return model\n",
    "\n",
    "def bay(hypermodel, objective, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):\n",
    "    bayesian_tuner = BayesianOptimization(\n",
    "        hypermodel,\n",
    "        objective=objective,\n",
    "        max_trials=50,\n",
    "        directory='bayesian_search',\n",
    "        project_name='bayesian_tuning'\n",
    "    )\n",
    "    \n",
    "    bayesian_tuner.search_space_summary()    \n",
    "\n",
    "    bayesian_tuner.search(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=callbacks)\n",
    "    \n",
    "    best_model_bayesian = bayesian_tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters_bayesian = bayesian_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Migliori iperparametri per Bayesian Optimization:\", best_hyperparameters_bayesian.values)\n",
    "    \n",
    "    return bayesian_tuner, best_model_bayesian, best_hyperparameters_bayesian\n",
    "\n",
    "def hb(hypermodel, X_train, Y_train, X_val, Y_val):\n",
    "    hyperband_tuner = kt.tuners.Hyperband(\n",
    "        hypermodel,\n",
    "        objective='val_loss',\n",
    "        max_trials=50,\n",
    "        directory='hyperband_search',\n",
    "        project_name='hyperband_tuning'\n",
    "    )\n",
    "    \n",
    "    hyperband_tuner.search_space_summary()\n",
    "    \n",
    "    hyperband_tuner.search(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    best_model_hyperband = hyperband_tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters_hyperband = hyperband_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Migliori iperparametri per Hyperband:\", best_hyperparameters_hyperband.values)\n",
    "    \n",
    "    return best_model_hyperband, best_hyperparameters_hyperband\n",
    "\n",
    "def rd(hypermodel, X_train, Y_train, X_val, Y_val):\n",
    "    random_tuner = kt.tuners.RandomSearch(\n",
    "        hypermodel,\n",
    "        objective='val_loss',\n",
    "        max_trials=50,\n",
    "        executions_per_trial=2,\n",
    "        directory='random_search',\n",
    "        project_name='random_tuning'\n",
    "    )\n",
    "    random_tuner.search_space_summary()\n",
    "    \n",
    "    random_tuner.search(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, Y_val), callbacks=[early_stopping, reduce_lr])\n",
    "    # Ottieni il miglior modello e i migliori iperparametri\n",
    "    best_model_random = random_tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters_random = random_tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Migliori iperparametri per Random Search:\", best_hyperparameters_random.values)\n",
    "    \n",
    "    return best_model_random, best_hyperparameters_random"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
